ğŸ”§ Running lint with auto-fix (Level 2: Auto-fix)...
ğŸ”§ Running ruff check with auto-fix...
uv run ruff check libs commands api tests --fix  --exclude migrations --exclude node_modules --exclude examples
api/middleware/error_handler.py:149:5: PLR0917 Too many positional arguments (8/5)
    |
149 | def create_error_response(
    |     ^^^^^^^^^^^^^^^^^^^^^ PLR0917
150 |     code: str,
151 |     message: str,
    |

api/routers/controllers.py:41:5: PLR1702 Too many nested blocks (7 > 5)
    |
 39 |   def start_controller(session_name: str) -> None:
 40 |       """ì§€ì •ëœ ì„¸ì…˜ì˜ ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
 41 | /     try:
 42 | |         controller = cm.get_controller(session_name)
 43 | |
 44 | |         # First check if session exists
 45 | |
 46 | |         session_manager = SessionManager()
 47 | |         sessions = session_manager.get_all_sessions()
 48 | |
 49 | |         session_exists = any(s.session_name == session_name and s.status == "running" for s in sessions)
 50 | |
 51 | |         if not session_exists:
 52 | |             raise HTTPException(
 53 | |                 status_code=400,
 54 | |                 detail=(f"âŒ Session '{session_name}' is not running. Please start the session first using 'yesman up' or the dashboarâ€¦
 55 | |             )
 56 | |
 57 | |         # Check if controller is already running
 58 | |         if controller.is_running:
 59 | |             raise HTTPException(
 60 | |                 status_code=400,
 61 | |                 detail=(f"âœ… Controller for session '{session_name}' is already running."),
 62 | |             )
 63 | |
 64 | |         # Check if Claude pane exists before attempting to start
 65 | |         if not controller.claude_pane:
 66 | |             # Get detailed session information for debugging
 67 | |             pane_info = []
 68 | |             window_count = 0
 69 | |             total_panes = 0
 70 | |
 71 | |             try:
 72 | |                 if controller.session_manager.session:
 73 | |                     for window in controller.session_manager.session.list_windows():
 74 | |                         window_count += 1
 75 | |                         panes_in_window = window.list_panes()
 76 | |                         total_panes += len(panes_in_window)
 77 | |
 78 | |                         for pane in panes_in_window:
 79 | |                             try:
 80 | |                                 cmd = pane.cmd(
 81 | |                                     "display-message",
 82 | |                                     "-p",
 83 | |                                     "#{pane_current_command}",
 84 | |                                 ).stdout[0]
 85 | |                                 pane_info.append(
 86 | |                                     f"Window '{window.name}' Pane {pane.index}: {cmd}",
 87 | |                                 )
 88 | |                             except (OSError, subprocess.CalledProcessError, RuntimeError) as e:
 89 | |                                 pane_info.append(
 90 | |                                     f"Window '{window.name}' Pane {pane.index}: <command unknown> (error: {e!s})",
 91 | |                                 )
 92 | |                 else:
 93 | |                     pane_info.append("âŒ Could not access session information")
 94 | |             except (AttributeError, RuntimeError, OSError) as e:
 95 | |                 pane_info.append(f"âŒ Error accessing session: {e!s}")
 96 | |
 97 | |             detail_msg = (
 98 | |                 f"âŒ No Claude Code pane found in session '{session_name}'. "
 99 | |                 f"Make sure Claude Code (claude) is running in one of the panes.\n\n"
100 | |                 f"ğŸ“Š Session Info:\n"
101 | |                 f"â€¢ Windows: {window_count}\n"
102 | |                 f"â€¢ Total Panes: {total_panes}\n\n"
103 | |                 f"ğŸ” Current Panes:\n"
104 | |             )
105 | |
106 | |             if pane_info:
107 | |                 detail_msg += "\n".join(f"  â€¢ {info}" for info in pane_info)
108 | |             else:
109 | |                 detail_msg += "  â€¢ No panes found"
110 | |
111 | |             detail_msg += f"\n\nğŸ’¡ To fix this:\n1. Open session: tmux attach -t {session_name}\n2. Start Claude Code in a pane: claudâ€¦
112 | |
113 | |             raise HTTPException(status_code=500, detail=detail_msg)
114 | |
115 | |         # Try to start the controller with detailed error handling
116 | |         try:
117 | |             success = controller.start()
118 | |
119 | |             if not success:
120 | |                 # Get more specific error from the controller
121 | |                 error_details = []
122 | |
123 | |                 # Check session manager state
124 | |                 if not controller.session_manager.initialize_session():
125 | |                     error_details.append("âŒ Session manager failed to initialize")
126 | |
127 | |                 # Check if monitoring can start
128 | |                 if hasattr(controller, "monitor"):
129 | |                     try:
130 | |                         monitor_status = controller.monitor.is_running
131 | |                         error_details.append(f"ğŸ“Š Monitor running: {monitor_status}")
132 | |                     except (AttributeError, RuntimeError) as e:
133 | |                         error_details.append(f"âŒ Monitor error: {e!s}")
134 | |
135 | |                 detail_msg = f"âŒ Controller failed to start for session '{session_name}'. The system encountered an internal error.\nâ€¦
136 | |
137 | |                 if error_details:
138 | |                     detail_msg += "\n".join(f"  â€¢ {info}" for info in error_details)
139 | |                 else:
140 | |                     detail_msg += "  â€¢ No specific error information available"
141 | |
142 | |                 detail_msg += (
143 | |                     f"\n\nğŸ’¡ Troubleshooting Steps:\n"
144 | |                     f"1. Check if Claude Code is responsive in the session\n"
145 | |                     f"2. Restart the session: yesman down {session_name} && "
146 | |                     f"yesman up {session_name}\n"
147 | |                     f"3. Check logs: yesman logs {session_name}\n"
148 | |                     f"4. Try restarting the API server"
149 | |                 )
150 | |
151 | |                 raise HTTPException(status_code=500, detail=detail_msg)
152 | |
153 | |         except (RuntimeError, OSError, subprocess.CalledProcessError, AttributeError) as start_error:
154 | |             # Detailed error information for start failures
155 | |             detail_msg = (
156 | |                 f"âŒ Controller start failed for session '{session_name}': "
157 | |                 f"{start_error!s}\n\n"
158 | |                 f"ğŸ” Error Type: {type(start_error).__name__}\n"
159 | |                 f"ğŸ“Š Session Status: {'Running' if session_exists else 'Not Running'}\n"
160 | |                 f"ğŸ”§ Claude Pane: {'Found' if controller.claude_pane else 'Not Found'}\n\n"
161 | |                 f"ğŸ’¡ Common Solutions:\n"
162 | |                 f"1. Ensure Claude Code is running: tmux attach -t {session_name}\n"
163 | |                 f"2. Check if claude command is available in the pane\n"
164 | |                 f"3. Try restarting the session\n"
165 | |                 f"4. Check system logs for more details"
166 | |             )
167 | |
168 | |             raise HTTPException(status_code=500, detail=detail_msg)
169 | |
170 | |         return
171 | |
172 | |     except HTTPException:
173 | |         raise
174 | |     except (ImportError, ModuleNotFoundError, AttributeError, ValueError, TypeError) as e:
175 | |         # Catch-all for unexpected errors
176 | |         detail_msg = (
177 | |             f"âŒ Unexpected error starting controller for session '{session_name}': "
178 | |             f"{e!s}\n\n"
179 | |             f"ğŸ” Error Type: {type(e).__name__}\n"
180 | |             f"ğŸ“ This is likely a system-level issue.\n\n"
181 | |             f"ğŸ’¡ Please try:\n"
182 | |             f"1. Restart the API server\n"
183 | |             f"2. Check system logs\n"
184 | |             f"3. Verify tmux is running properly\n"
185 | |             f"4. Report this error if it persists"
186 | |         )
187 | |
188 | |         raise HTTPException(
189 | |             status_code=500,
190 | |             detail=detail_msg,
191 | |         )
    | |_________^ PLR1702
    |

api/routers/controllers.py:52:13: TRY301 Abstract `raise` to an inner function
   |
51 |           if not session_exists:
52 | /             raise HTTPException(
53 | |                 status_code=400,
54 | |                 detail=(f"âŒ Session '{session_name}' is not running. Please start the session first using 'yesman up' or the dashboardâ€¦
55 | |             )
   | |_____________^ TRY301
56 |
57 |           # Check if controller is already running
   |

api/routers/controllers.py:59:13: TRY301 Abstract `raise` to an inner function
   |
57 |           # Check if controller is already running
58 |           if controller.is_running:
59 | /             raise HTTPException(
60 | |                 status_code=400,
61 | |                 detail=(f"âœ… Controller for session '{session_name}' is already running."),
62 | |             )
   | |_____________^ TRY301
63 |
64 |           # Check if Claude pane exists before attempting to start
   |

api/routers/controllers.py:113:13: TRY301 Abstract `raise` to an inner function
    |
111 |             detail_msg += f"\n\nğŸ’¡ To fix this:\n1. Open session: tmux attach -t {session_name}\n2. Start Claude Code in a pane: claudâ€¦
112 |
113 |             raise HTTPException(status_code=500, detail=detail_msg)
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY301
114 |
115 |         # Try to start the controller with detailed error handling
    |

api/routers/controllers.py:170:9: TRY300 Consider moving this statement to an `else` block
    |
168 |             raise HTTPException(status_code=500, detail=detail_msg)
169 |
170 |         return
    |         ^^^^^^ TRY300
171 |
172 |     except HTTPException:
    |

api/routers/controllers.py:221:13: TRY301 Abstract `raise` to an inner function
    |
219 |               )
220 |
221 | /             raise HTTPException(
222 | |                 status_code=500,
223 | |                 detail=detail_msg,
224 | |             )
    | |_____________^ TRY301
225 |           return
226 |       except HTTPException:
    |

api/routers/controllers.py:225:9: TRY300 Consider moving this statement to an `else` block
    |
223 |                 detail=detail_msg,
224 |             )
225 |         return
    |         ^^^^^^ TRY300
226 |     except HTTPException:
227 |         raise
    |

api/routers/controllers.py:263:13: TRY301 Abstract `raise` to an inner function
    |
261 |               )
262 |
263 | /             raise HTTPException(
264 | |                 status_code=400,
265 | |                 detail=detail_msg,
266 | |             )
    | |_____________^ TRY301
267 |
268 |           success = controller.restart_claude_pane()
    |

api/routers/controllers.py:280:13: TRY301 Abstract `raise` to an inner function
    |
278 |               )
279 |
280 | /             raise HTTPException(
281 | |                 status_code=500,
282 | |                 detail=detail_msg,
283 | |             )
    | |_____________^ TRY301
284 |           return
285 |       except HTTPException:
    |

api/routers/controllers.py:284:9: TRY300 Consider moving this statement to an `else` block
    |
282 |                 detail=detail_msg,
283 |             )
284 |         return
    |         ^^^^^^ TRY300
285 |     except HTTPException:
286 |         raise
    |

api/routers/controllers.py:346:13: TRY301 Abstract `raise` to an inner function
    |
344 |           if errors and started_count == 0:
345 |               # ëª¨ë“  ìš”ì²­ì´ ì‹¤íŒ¨í•œ ê²½ìš°
346 | /             raise HTTPException(
347 | |                 status_code=500,
348 | |                 detail=(f"Failed to start any controllers. Errors: {'; '.join(errors)}"),
349 | |             )
    | |_____________^ TRY301
350 |
351 |           return {
    |

api/routers/controllers.py:408:13: TRY301 Abstract `raise` to an inner function
    |
406 |           if errors and stopped_count == 0:
407 |               # ëª¨ë“  ìš”ì²­ì´ ì‹¤íŒ¨í•œ ê²½ìš°
408 | /             raise HTTPException(
409 | |                 status_code=500,
410 | |                 detail=(f"Failed to stop any controllers. Errors: {'; '.join(errors)}"),
411 | |             )
    | |_____________^ TRY301
412 |
413 |           return {
    |

api/routers/dashboard.py:96:9: TRY300 Consider moving this statement to an `else` block
   |
94 |             )
95 |
96 |         return web_sessions
   |         ^^^^^^^^^^^^^^^^^^^ TRY300
97 |     except Exception as e:
98 |         logger.exception("Failed to get sessions")  # noqa: G004
   |

api/routers/logs.py:50:17: TRY301 Abstract `raise` to an inner function
   |
48 |               log_file = Path(log_path_str).expanduser() / "yesman.log"
49 |               if not log_file.exists():
50 | /                 raise HTTPException(
51 | |                     status_code=404,
52 | |                     detail=f"Log file for session '{session_name}' not found.",
53 | |                 )
   | |_________________^ TRY301
54 |
55 |           with open(log_file, encoding="utf-8") as f:
   |

api/routers/logs.py:127:5: PLR1702 Too many nested blocks (7 > 5)
    |
125 |           object: Description of return value.
126 |       """
127 | /     try:
128 | |         config = get_config()
129 | |         log_path_str = config.get("log_path", "~/.scripton/yesman/logs/")
130 | |         log_files = [
131 | |             Path(log_path_str).expanduser() / "yesman.log",
132 | |             Path(log_path_str).expanduser() / "claude_manager.log",
133 | |             Path(log_path_str).expanduser() / "dashboard.log",
134 | |         ]
135 | |
136 | |         all_logs = []
137 | |
138 | |         for log_file in log_files:
139 | |             if log_file.exists():
140 | |                 try:
141 | |                     with open(log_file, encoding="utf-8") as f:
142 | |                         lines = f.readlines()
143 | |                         for line in lines[-limit:]:  # Take last N lines from each file
144 | |                             log_entry = parse_log_line(line)
145 | |                             if log_entry:
146 | |                                 all_logs.append(log_entry)
147 | |                 except Exception:
148 | |                     continue
149 | |
150 | |         # Sort by timestamp (newest first)
151 | |         all_logs.sort(key=lambda x: x.timestamp, reverse=True)
152 | |
153 | |         # Apply filters
154 | |         filtered_logs = all_logs
155 | |
156 | |         if level:
157 | |             filtered_logs = [log for log in filtered_logs if log.level == level.lower()]
158 | |
159 | |         if source:
160 | |             filtered_logs = [log for log in filtered_logs if log.source == source]
161 | |
162 | |         if search:
163 | |             search_lower = search.lower()
164 | |             filtered_logs = [log for log in filtered_logs if search_lower in log.message.lower()]
165 | |
166 | |         return filtered_logs[:limit]
167 | |
168 | |     except Exception as e:
169 | |         raise HTTPException(status_code=500, detail=f"Failed to read logs: {e!s}")
    | |__________________________________________________________________________________^ PLR1702
    |

api/routers/logs.py:147:17: S112 `try`-`except`-`continue` detected, consider logging the exception
    |
145 |                               if log_entry:
146 |                                   all_logs.append(log_entry)
147 | /                 except Exception:
148 | |                     continue
    | |____________________________^ S112
149 |
150 |           # Sort by timestamp (newest first)
    |

api/routers/logs.py:179:5: PLR1702 Too many nested blocks (7 > 5)
    |
177 |           object: Description of return value.
178 |       """
179 | /     try:
180 | |         config = get_config()
181 | |         log_path_str = config.get("log_path", "~/.scripton/yesman/logs/")
182 | |         log_files = [
183 | |             Path(log_path_str).expanduser() / "yesman.log",
184 | |             Path(log_path_str).expanduser() / "claude_manager.log",
185 | |             Path(log_path_str).expanduser() / "dashboard.log",
186 | |         ]
187 | |
188 | |         sources = set()
189 | |
190 | |         for log_file in log_files:
191 | |             if log_file.exists():
192 | |                 try:
193 | |                     with open(log_file, encoding="utf-8") as f:
194 | |                         # Sample first 50 lines to get sources
195 | |                         for i, line in enumerate(f):
196 | |                             if i >= 50:
197 | |                                 break
198 | |                             log_entry = parse_log_line(line)
199 | |                             if log_entry:
200 | |                                 sources.add(log_entry.source)
201 | |                 except Exception as e:
202 | |                     logger.warning(f"Failed to parse log file {log_file}: {e}")  # noqa: G004
203 | |                     continue
204 | |
205 | |         return {"sources": sorted(sources)}
206 | |
207 | |     except Exception as e:
208 | |         raise HTTPException(status_code=500, detail=f"Failed to get log sources: {e!s}")
    | |________________________________________________________________________________________^ PLR1702
    |

api/routers/logs.py:179:5: PLR1702 Too many nested blocks (7 > 5)
    |
177 |           object: Description of return value.
178 |       """
179 | /     try:
180 | |         config = get_config()
181 | |         log_path_str = config.get("log_path", "~/.scripton/yesman/logs/")
182 | |         log_files = [
183 | |             Path(log_path_str).expanduser() / "yesman.log",
184 | |             Path(log_path_str).expanduser() / "claude_manager.log",
185 | |             Path(log_path_str).expanduser() / "dashboard.log",
186 | |         ]
187 | |
188 | |         sources = set()
189 | |
190 | |         for log_file in log_files:
191 | |             if log_file.exists():
192 | |                 try:
193 | |                     with open(log_file, encoding="utf-8") as f:
194 | |                         # Sample first 50 lines to get sources
195 | |                         for i, line in enumerate(f):
196 | |                             if i >= 50:
197 | |                                 break
198 | |                             log_entry = parse_log_line(line)
199 | |                             if log_entry:
200 | |                                 sources.add(log_entry.source)
201 | |                 except Exception as e:
202 | |                     logger.warning(f"Failed to parse log file {log_file}: {e}")  # noqa: G004
203 | |                     continue
204 | |
205 | |         return {"sources": sorted(sources)}
206 | |
207 | |     except Exception as e:
208 | |         raise HTTPException(status_code=500, detail=f"Failed to get log sources: {e!s}")
    | |________________________________________________________________________________________^ PLR1702
    |

api/routers/sessions.py:81:13: TRY300 Consider moving this statement to an `else` block
   |
79 |             if session_data:
80 |                 return self._convert_session_to_api_data(session_data)
81 |             return None
   |             ^^^^^^^^^^^ TRY300
82 |         except Exception as e:
83 |             self.logger.exception("Failed to get session {session_name}: {e}")
   |

api/routers/sessions.py:101:17: TRY301 Abstract `raise` to an inner function
    |
 99 |               if self._session_exists(session_name):
100 |                   msg = f"Session '{session_name}' already exists"
101 | /                 raise YesmanError(
102 | |                     msg,
103 | |                     category=ErrorCategory.VALIDATION,
104 | |                 )
    | |_________________^ TRY301
105 |
106 |               # Load projects configuration
    |

api/routers/sessions.py:110:17: TRY301 Abstract `raise` to an inner function
    |
108 |               if session_name not in projects:
109 |                   msg = f"Session '{session_name}' not found in projects configuration"
110 | /                 raise YesmanError(
111 | |                     msg,
112 | |                     category=ErrorCategory.CONFIGURATION,
113 | |                 )
    | |_________________^ TRY301
114 |
115 |               # Set up session (this would integrate with the improved setup logic)
    |

api/routers/sessions.py:118:13: TRY300 Consider moving this statement to an `else` block
    |
116 |               result = self._setup_session_internal(session_name, projects[session_name])
117 |
118 | /             return {
119 | |                 "session_name": session_name,
120 | |                 "status": "created",
121 | |                 "details": result,
122 | |             }
    | |_____________^ TRY300
123 |
124 |           except YesmanError:
    |

api/routers/sessions.py:144:17: TRY301 Abstract `raise` to an inner function
    |
142 |               if not self._session_exists(session_name):
143 |                   msg = f"Session '{session_name}' not found"
144 | /                 raise YesmanError(
145 | |                     msg,
146 | |                     category=ErrorCategory.VALIDATION,
147 | |                 )
    | |_________________^ TRY301
148 |
149 |               # Teardown session
    |

api/routers/sessions.py:152:13: TRY300 Consider moving this statement to an `else` block
    |
150 |               self._teardown_session_internal(session_name)
151 |
152 | /             return {
153 | |                 "session_name": session_name,
154 | |                 "status": "removed",
155 | |             }
    | |_____________^ TRY300
156 |
157 |           except YesmanError:
    |

api/routers/sessions.py:304:17: TRY301 Abstract `raise` to an inner function
    |
302 |               if self._session_exists(session_name):
303 |                   msg = f"Session '{session_name}' is already running"
304 | /                 raise YesmanError(
305 | |                     msg,
306 | |                     category=ErrorCategory.VALIDATION,
307 | |                 )
    | |_________________^ TRY301
308 |
309 |               # Attach to the session
    |

api/routers/sessions.py:313:13: TRY300 Consider moving this statement to an `else` block
    |
311 |               subprocess.run(["tmux", "attach-session", "-t", session_name], check=False)
312 |
313 | /             return {
314 | |                 "session_name": session_name,
315 | |                 "status": "started",
316 | |             }
    | |_____________^ TRY300
317 |
318 |           except YesmanError:
    |

api/routers/sessions.py:338:17: TRY301 Abstract `raise` to an inner function
    |
336 |               if not self._session_exists(session_name):
337 |                   msg = f"Session '{session_name}' not found"
338 | /                 raise YesmanError(
339 | |                     msg,
340 | |                     category=ErrorCategory.VALIDATION,
341 | |                 )
    | |_________________^ TRY301
342 |
343 |               # Kill the session
    |

api/routers/sessions.py:346:13: TRY300 Consider moving this statement to an `else` block
    |
344 |               self._teardown_session_internal(session_name)
345 |
346 | /             return {
347 | |                 "session_name": session_name,
348 | |                 "status": "stopped",
349 | |             }
    | |_____________^ TRY300
350 |
351 |           except YesmanError:
    |

api/routers/sessions.py:412:33: ARG004 Unused static method argument: `session_name`
    |
411 |     @staticmethod
412 |     def _setup_session_internal(session_name: str, session_config: dict[str, object]) -> dict[str, object]:  # noqa: ARG002  # noqa: â€¦
    |                                 ^^^^^^^^^^^^ ARG004
413 |         """Internal session setup logic.
    |

api/routers/sessions.py:412:52: ARG004 Unused static method argument: `session_config`
    |
411 |     @staticmethod
412 |     def _setup_session_internal(session_name: str, session_config: dict[str, object]) -> dict[str, object]:  # noqa: ARG002  # noqa: â€¦
    |                                                    ^^^^^^^^^^^^^^ ARG004
413 |         """Internal session setup logic.
    |

api/routers/sessions.py:517:13: TRY301 Abstract `raise` to an inner function
    |
516 |           if not session_data:
517 | /             raise HTTPException(
518 | |                 status_code=status.HTTP_404_NOT_FOUND,
519 | |                 detail=f"Session '{session_name}' not found",
520 | |             )
    | |_____________^ TRY301
521 |
522 |           # Convert to Pydantic model
    |

api/tests/test_background_tasks.py:43:5: PLR1702 Too many nested blocks (7 > 5)
   |
41 |       start_time = time.time()
42 |
43 | /     try:
44 | |         async with websockets.connect(uri) as websocket:
45 | |             while time.time() - start_time < duration:
46 | |                 try:
47 | |                     # Set timeout to avoid blocking forever
48 | |                     message = await asyncio.wait_for(websocket.recv(), timeout=1.0)
49 | |                     data = json.loads(message)
50 | |
51 | |                     msg_type = data.get("type", "unknown")
52 | |                     update_counts[msg_type] = update_counts.get(msg_type, 0) + 1
53 | |
54 | |                     if msg_type == "ping":
55 | |                         # Respond to ping
56 | |                         await websocket.send(
57 | |                             json.dumps(
58 | |                                 {
59 | |                                     "type": "pong",
60 | |                                     "timestamp": datetime.now(UTC).isoformat(),
61 | |                                 }
62 | |                             )
63 | |                         )
64 | |
65 | |                     elif msg_type in {
66 | |                         "session_update",
67 | |                         "health_update",
68 | |                         "activity_update",
69 | |                     }:
70 | |                         if msg_type == "session_update":
71 | |                             sessions = data.get("data", [])
72 | |                             for session in sessions[:3]:  # Show first 3
73 | |                                 pass
74 | |
75 | |                         elif msg_type in {"health_update", "activity_update"}:
76 | |                             data.get("data", {})
77 | |
78 | |                 except TimeoutError:
79 | |                     # No message received in 1 second, continue
80 | |                     pass
81 | |
82 | |             for count in update_counts.values():
83 | |                 if count > 0:
84 | |                     pass
85 | |
86 | |     except Exception:
87 | |         pass
   | |____________^ PLR1702
   |

api/tests/test_background_tasks.py:86:5: S110 `try`-`except`-`pass` detected, consider logging the exception
   |
84 |                       pass
85 |
86 | /     except Exception:
87 | |         pass
   | |____________^ S110
   |

api/tests/test_websocket.py:93:5: S110 `try`-`except`-`pass` detected, consider logging the exception
   |
91 |       except websockets.exceptions.WebSocketException:
92 |           pass
93 | /     except Exception:
94 | |         pass
   | |____________^ S110
   |

api/tests/test_websocket.py:136:5: S110 `try`-`except`-`pass` detected, consider logging the exception
    |
134 |               await ws.close()
135 |
136 | /     except Exception:
137 | |         pass
    | |____________^ S110
138 |       finally:
139 |           # Ensure all connections are closed
    |

api/utils/batch_processor.py:386:39: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
384 |         }
385 |
386 |     def update_config(self, **kwargs: Any) -> None:
    |                                       ^^^ ANN401
387 |         """Update batch processing configuration."""
388 |         for key, value in kwargs.items():
    |

api/utils/batch_processor_refactored.py:338:39: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
336 |         }
337 |
338 |     def update_config(self, **kwargs: Any) -> None:
    |                                       ^^^ ANN401
339 |         """Update batch processing configuration."""
340 |         for key, value in kwargs.items():
    |

commands/ai.py:41:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
39 |             raise CommandError(msg) from e
40 |
41 |     def execute(self, **kwargs: Any) -> dict:  # noqa: ARG002
   |                                 ^^^ ANN401
42 |         """Execute the status command.
   |

commands/ai.py:107:13: TRY300 Consider moving this statement to an `else` block
    |
105 |                 self.console.print(projects_table)
106 |
107 |             return stats
    |             ^^^^^^^^^^^^ TRY300
108 |
109 |         except Exception as e:
    |

commands/ai.py:131:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
129 |             raise CommandError(msg) from e
130 |
131 |     def execute(self, **kwargs: Any) -> dict:
    |                                 ^^^ ANN401
132 |         """Execute the command.
    |

commands/ai.py:192:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
190 |             raise CommandError(msg) from e
191 |
192 |     def execute(self, **kwargs: Any) -> dict:
    |                                 ^^^ ANN401
193 |         """Execute the command.
    |

commands/ai.py:262:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
260 |             raise CommandError(msg) from e
261 |
262 |     def execute(self, **kwargs: Any) -> dict:
    |                                 ^^^ ANN401
263 |         """Execute the command.
    |

commands/ai.py:302:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
300 |             raise CommandError(msg) from e
301 |
302 |     def execute(self, **kwargs: Any) -> dict:
    |                                 ^^^ ANN401
303 |         """Execute the command.
    |

commands/ai.py:340:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
338 |             raise CommandError(msg) from e
339 |
340 |     def execute(self, **kwargs: Any) -> dict:
    |                                 ^^^ ANN401
341 |         """Execute the command.
    |

commands/automate.py:32:58: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
30 |         self.console = Console()
31 |
32 |     def execute(self, project_path: str = ".", **kwargs: Any) -> dict[str, Any]:  # noqa: ARG002
   |                                                          ^^^ ANN401
33 |         """Execute the status command.
   |

commands/automate.py:121:13: TRY300 Consider moving this statement to an `else` block
    |
119 |                 self.console.print("\nğŸ“ No workflow chains configured")
120 |
121 |             return stats
    |             ^^^^^^^^^^^^ TRY300
122 |
123 |         except Exception as e:
    |

commands/automate.py:150:78: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
148 |         self.console = Console()
149 |
150 |     def execute(self, project_path: str = ".", interval: int = 10, **kwargs: Any) -> dict[str, Any]:  # noqa: ARG002
    |                                                                              ^^^ ANN401
151 |         """Execute the monitor command.
    |

commands/automate.py:189:13: TRY300 Consider moving this statement to an `else` block
    |
188 |             asyncio.run(run_monitoring())
189 |             return {"monitoring_started": True}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
190 |
191 |         except KeyboardInterrupt:
    |

commands/automate.py:211:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
209 |         context_type: str | None = None,
210 |         description: str = "Manual trigger",
211 |         **kwargs: Any,  # noqa: ARG002
    |                   ^^^ ANN401
212 |     ) -> dict[str, Any]:
213 |         """Execute the trigger command.
    |

commands/automate.py:233:23: ANN202 Missing return type annotation for private function `trigger_automation`
    |
232 | â€¦     # Simulate context detection
233 | â€¦     async def trigger_automation():
    |                 ^^^^^^^^^^^^^^^^^^ ANN202
234 | â€¦         # Create a context info object
235 | â€¦         context_info = ContextInfo(context_type=context_enum, confidence=1.0, details={"description": description, "manual": True},â€¦
    |
    = help: Add return type annotation

commands/automate.py:266:92: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
264 |         self.console = Console()
265 |
266 |     def execute(self, workflow_name: str | None = None, project_path: str = ".", **kwargs: Any) -> dict[str, Any]:
    |                                                                                            ^^^ ANN401
267 |         """Execute the workflow command.
    |

commands/automate.py:286:23: ANN202 Missing return type annotation for private function `run_workflow`
    |
284 |             self.console.print(f"âš¡ Executing workflow: {workflow_name}")
285 |
286 |             async def run_workflow():
    |                       ^^^^^^^^^^^^ ANN202
287 |                 # Create a dummy context for manual execution
288 |                 context_info = ContextInfo(context_type=ContextType.UNKNOWN, confidence=1.0, details={"manual_execution": True, "workâ€¦
    |
    = help: Add return type annotation

commands/automate.py:309:13: TRY300 Consider moving this statement to an `else` block
    |
307 |                     self.console.print(f"  {step}: {result}")
308 |
309 |             return {"success": success, "results": results}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
310 |
311 |         except Exception as e:
    |

commands/automate.py:323:58: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
321 |         self.console = Console()
322 |
323 |     def execute(self, project_path: str = ".", **kwargs: Any) -> dict[str, Any]:  # noqa: ARG002
    |                                                          ^^^ ANN401
324 |         """Execute the detect command.
    |

commands/automate.py:345:27: ANN202 Missing return type annotation for private function `run_detection`
    |
343 |                 )
344 |
345 |                 async def run_detection():
    |                           ^^^^^^^^^^^^^ ANN202
346 |                     return await automation_manager._detect_all_contexts()
    |
    = help: Add return type annotation

commands/automate.py:387:13: TRY300 Consider moving this statement to an `else` block
    |
385 |                     self.console.print(f"  {context.context_type.value}: No workflows configured")
386 |
387 |             return {"contexts": contexts}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
388 |
389 |         except Exception as e:
    |

commands/automate.py:401:85: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
399 |         self.console = Console()
400 |
401 |     def execute(self, project_path: str = ".", output: str | None = None, **kwargs: Any) -> dict[str, Any]:  # noqa: ARG002
    |                                                                                     ^^^ ANN401
402 |         """Execute the config command.
    |

commands/automate.py:476:13: TRY300 Consider moving this statement to an `else` block
    |
474 |                 self.console.print(config_json)
475 |
476 |             return {"config": sample_config, "output": output}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
477 |
478 |         except Exception as e:
    |

commands/browse.py:28:24: ANN001 Missing type annotation for function argument `tmux_manager`
   |
26 |     """Interactive session browser with live updates."""
27 |
28 |     def __init__(self, tmux_manager, config, update_interval: float = 2.0) -> None:
   |                        ^^^^^^^^^^^^ ANN001
29 |         self.console = Console()
30 |         self.config = config
   |

commands/browse.py:28:38: ANN001 Missing type annotation for function argument `config`
   |
26 |     """Interactive session browser with live updates."""
27 |
28 |     def __init__(self, tmux_manager, config, update_interval: float = 2.0) -> None:
   |                                      ^^^^^^ ANN001
29 |         self.console = Console()
30 |         self.config = config
   |

commands/browse.py:235:63: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
233 |             raise CommandError(msg)
234 |
235 |     def execute(self, update_interval: float = 2.0, **kwargs: Any) -> dict:  # noqa: ARG002
    |                                                               ^^^ ANN401
236 |         """Execute the browse command.
    |

commands/browse.py:250:13: TRY300 Consider moving this statement to an `else` block
    |
248 |             browser.start()
249 |
250 |             return {"success": True}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
251 |
252 |         except KeyboardInterrupt:
    |

commands/browse_async.py:73:15: ANN202 Missing return type annotation for private function `_get_sessions_async`
   |
71 |             self.console.print(f"[red]Error updating session data: {e}[/]")
72 |
73 |     async def _get_sessions_async(self):
   |               ^^^^^^^^^^^^^^^^^^^ ANN202
74 |         """Async wrapper for getting sessions list."""
75 |         # Run blocking operation in thread pool
   |
   = help: Add return type annotation

commands/browse_async.py:79:15: ANN202 Missing return type annotation for private function `_get_session_info_async`
   |
77 |         return await loop.run_in_executor(None, self.tmux_manager.get_cached_sessions_list)
78 |
79 |     async def _get_session_info_async(self, session_name: str):
   |               ^^^^^^^^^^^^^^^^^^^^^^^ ANN202
80 |         """Async wrapper for getting session info."""
81 |         loop = asyncio.get_event_loop()
   |
   = help: Add return type annotation

commands/browse_async.py:84:15: ANN202 Missing return type annotation for private function `_get_progress_overview_async`
   |
82 |         return await loop.run_in_executor(None, self.tmux_manager.get_session_info, session_name)
83 |
84 |     async def _get_progress_overview_async(self):
   |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ANN202
85 |         """Async wrapper for getting progress overview."""
86 |         loop = asyncio.get_event_loop()
   |
   = help: Add return type annotation

commands/browse_async.py:264:75: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
262 |             raise CommandError(msg)
263 |
264 |     async def execute_async(self, update_interval: float = 2.0, **kwargs: Any) -> dict:  # noqa: ARG002
    |                                                                           ^^^ ANN401
265 |         """Execute the async browse command."""
266 |         try:
    |

commands/browse_async.py:275:13: TRY300 Consider moving this statement to an `else` block
    |
273 |             await browser.start()
274 |
275 |             return {"success": True, "mode": "async"}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
276 |
277 |         except KeyboardInterrupt:
    |

commands/cleanup.py:32:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
30 |         force: bool = False,  # noqa: FBT001
31 |         cleanup_all: bool = False,  # noqa: FBT001
32 |         **kwargs: Any,  # noqa: ARG002
   |                   ^^^ ANN401
33 |     ) -> dict:
34 |         """Execute the cleanup command.
   |

commands/dashboard.py:114:9: PLR0917 Too many positional arguments (6/5)
    |
112 |         self.env = DashboardEnvironment()
113 |
114 |     def execute(
    |         ^^^^^^^ PLR0917
115 |         self,
116 |         interface: str = "auto",
    |

commands/dashboard.py:122:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
120 |         dev: bool = False,  # noqa: FBT001
121 |         detach: bool = False,  # noqa: FBT001
122 |         **kwargs: Any,  # noqa: ARG002
    |                   ^^^ ANN401
123 |     ) -> dict:
124 |         """Execute the dashboard run command.
    |

commands/dashboard.py:167:17: TRY301 Abstract `raise` to an inner function
    |
165 |             else:
166 |                 msg = f"Unknown interface: {interface}"
167 |                 raise CommandError(msg)
    |                 ^^^^^^^^^^^^^^^^^^^^^^^ TRY301
168 |
169 |             return {"interface": interface, "success": True}
    |

commands/dashboard.py:169:13: TRY300 Consider moving this statement to an `else` block
    |
167 |                 raise CommandError(msg)
168 |
169 |             return {"interface": interface, "success": True}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
170 |
171 |         except KeyboardInterrupt:
    |

commands/dashboard.py:248:9: PLR1702 Too many nested blocks (6 > 5)
    |
246 |           self.print_info(f"ğŸŒ Starting Web Dashboard on http://{host}:{port}...")
247 |
248 | /         try:
249 | |             # Check if port is available
250 | |             sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
251 | |             result = sock.connect_ex((host, port))
252 | |             sock.close()
253 | |
254 | |             if result == 0:
255 | |                 msg = f"Port {port} is already in use. Try a different port with -p option."
256 | |                 raise CommandError(msg)
257 | |
258 | |             # Start FastAPI server
259 | |             api_path = Path(__file__).parent.parent / "api"
260 | |             if not api_path.exists():
261 | |                 self.print_warning("Web dashboard API not found. Creating simple HTML interface...")
262 | |
263 | |                 # Create simple HTML dashboard
264 | |                 template_path = os.path.join(os.path.dirname(__file__), "..", "templates", "web_dashboard.html")
265 | |                 with open(template_path, encoding="utf-8") as f:
266 | |                     html_template = f.read()
267 | |                 html_content = html_template.format(
268 | |                     host=host,
269 | |                     port=port,
270 | |                     theme=theme or "default",
271 | |                     dev="Development" if dev else "Production",
272 | |                 )
273 | |
274 | |                 # Start simple HTTP server
275 | |                 class DashboardHandler(http.server.SimpleHTTPRequestHandler):
276 | |                     def do_GET(self) -> None:
277 | |                         self.send_response(200)
278 | |                         self.send_header("Content-type", "text/html")
279 | |                         self.end_headers()
280 | |                         self.wfile.write(html_content.encode())
281 | |
282 | |                 def run_server() -> None:
283 | |                     with socketserver.TCPServer((host, port), DashboardHandler) as httpd:
284 | |                         httpd.serve_forever()
285 | |
286 | |                 if detach:
287 | |                     # Run in background
288 | |                     server_thread = threading.Thread(target=run_server, daemon=True)
289 | |                     server_thread.start()
290 | |                     self.print_info(f"Web dashboard started in background at http://{host}:{port}")
291 | |
292 | |                     # Open browser
293 | |                     webbrowser.open(f"http://{host}:{port}")
294 | |
295 | |                     # Keep main thread alive
296 | |                     try:
297 | |                         while True:
298 | |                             time_module.sleep(1)
299 | |                     except KeyboardInterrupt:
300 | |                         self.print_info("Shutting down web dashboard...")
301 | |                 else:
302 | |                     self.print_info(f"Web dashboard running at http://{host}:{port}")
303 | |                     self.print_info("Press Ctrl+C to stop")
304 | |
305 | |                     # Open browser
306 | |                     webbrowser.open(f"http://{host}:{port}")
307 | |
308 | |                     try:
309 | |                         run_server()
310 | |                     except KeyboardInterrupt:
311 | |                         self.print_info("\nShutting down web dashboard...")
312 | |             else:
313 | |                 # Use FastAPI server
314 | |                 try:
315 | |                     if uvicorn is None:
316 | |                         msg = "FastAPI/uvicorn not available. Install with: uv add fastapi uvicorn"
317 | |                         raise CommandError(msg)
318 | |
319 | |                     # Show the URL before starting
320 | |                     dashboard_url = f"http://{host}:{port}"
321 | |                     self.print_info(f"ğŸŒ FastAPI server starting at {dashboard_url}")
322 | |                     self.print_info("ğŸ“Š Dashboard will be available at the URL above")
323 | |
324 | |                     if dev:
325 | |                         # ê°œë°œ ëª¨ë“œì—ì„œëŠ” Vite ê°œë°œ ì„œë²„ë„ í•¨ê»˜ ì‹¤í–‰
326 | |                         self.print_info("ğŸš€ Starting Vite dev server for hot module replacement...")
327 | |                         vite_process = subprocess.Popen(
328 | |                             ["npm", "run", "dev"],
329 | |                             cwd=Path(__file__).parent.parent / "tauri-dashboard",
330 | |                             stdout=subprocess.PIPE,
331 | |                             stderr=subprocess.PIPE,
332 | |                         )
333 | |
334 | |                         # Vite ì„œë²„ê°€ ì‹œì‘ë  ë•Œê¹Œì§€ ëŒ€ê¸°
335 | |                         def wait_for_vite_server() -> bool:
336 | |                             for i in range(20):  # ìµœëŒ€ 20ì´ˆ ëŒ€ê¸°
337 | |                                 try:
338 | |                                     sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
339 | |                                     result = sock.connect_ex(("localhost", 5173))
340 | |                                     sock.close()
341 | |                                     if result == 0:  # í¬íŠ¸ê°€ ì—´ë ¤ìˆìŒ
342 | |                                         return True
343 | |                                     time.sleep(1)
344 | |                                 except Exception:
345 | |                                     time.sleep(1)
346 | |                             return False
347 | |
348 | |                         vite_ready = wait_for_vite_server()
349 | |                         if vite_ready:
350 | |                             self.print_success("Vite server is ready!")
351 | |                         else:
352 | |                             self.print_warning("Vite server may not be ready yet...")
353 | |
354 | |                         self.print_info("ğŸ“ Development mode:")
355 | |                         self.print_info(f"  - API Server: http://{host}:{port}")
356 | |                         self.print_info("  - Frontend (Vite): http://localhost:5173")
357 | |                         self.print_info("  - Changes will be reflected automatically!")
358 | |
359 | |                         if not detach:
360 | |                             # ê°œë°œ ëª¨ë“œì—ì„œëŠ” Vite ì„œë²„ë¡œ ì—°ê²° (ì„œë²„ê°€ ì¤€ë¹„ëœ í›„)
361 | |                             if vite_ready:
362 | |                                 self.print_info("ğŸ”— Opening browser to Vite dev server...")
363 | |                                 webbrowser.open("http://localhost:5173")
364 | |                             else:
365 | |                                 self.print_info("")
366 | |                                 self.print_info("=" * 60)
367 | |                                 self.print_warning("VITE SERVER NOT READY - MANUAL ACTION REQUIRED")
368 | |                                 self.print_info("=" * 60)
369 | |                                 self.print_info("ğŸ”— Please manually open: http://localhost:5173")
370 | |                                 self.print_info("   (Wait a few seconds if the page doesn't load)")
371 | |                                 self.print_info("=" * 60)
372 | |                                 self.print_info("")
373 | |
374 | |                     if not detach and not dev:
375 | |                         # Open browser after a short delay in a separate thread
376 | |                         def open_browser() -> None:
377 | |                             time.sleep(2)  # Wait for server to start
378 | |                             try:
379 | |                                 webbrowser.open(dashboard_url)
380 | |                                 self.print_info(f"ğŸ”— Opening browser to {dashboard_url}")
381 | |                             except Exception as e:
382 | |                                 self.print_warning(f"Could not open browser automatically: {e}")
383 | |                                 self.print_info(f"Please open {dashboard_url} manually in your browser")
384 | |
385 | |                         threading.Thread(target=open_browser, daemon=True).start()
386 | |
387 | |                     try:
388 | |                         uvicorn.run("api.main:app", host=host, port=port, reload=dev)
389 | |                     finally:
390 | |                         if dev and "vite_process" in locals():
391 | |                             vite_process.terminate()
392 | |
393 | |                 except Exception as uvicorn_error:
394 | |                     msg = f"FastAPI server error: {uvicorn_error}"
395 | |                     raise CommandError(msg) from uvicorn_error
396 | |
397 | |         except Exception as e:
398 | |             msg = f"Web dashboard error: {e}"
399 | |             raise CommandError(msg) from e
    | |__________________________________________^ PLR1702
400 |
401 |       def _launch_tauri_dashboard(
    |

commands/dashboard.py:248:9: PLR1702 Too many nested blocks (7 > 5)
    |
246 |           self.print_info(f"ğŸŒ Starting Web Dashboard on http://{host}:{port}...")
247 |
248 | /         try:
249 | |             # Check if port is available
250 | |             sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
251 | |             result = sock.connect_ex((host, port))
252 | |             sock.close()
253 | |
254 | |             if result == 0:
255 | |                 msg = f"Port {port} is already in use. Try a different port with -p option."
256 | |                 raise CommandError(msg)
257 | |
258 | |             # Start FastAPI server
259 | |             api_path = Path(__file__).parent.parent / "api"
260 | |             if not api_path.exists():
261 | |                 self.print_warning("Web dashboard API not found. Creating simple HTML interface...")
262 | |
263 | |                 # Create simple HTML dashboard
264 | |                 template_path = os.path.join(os.path.dirname(__file__), "..", "templates", "web_dashboard.html")
265 | |                 with open(template_path, encoding="utf-8") as f:
266 | |                     html_template = f.read()
267 | |                 html_content = html_template.format(
268 | |                     host=host,
269 | |                     port=port,
270 | |                     theme=theme or "default",
271 | |                     dev="Development" if dev else "Production",
272 | |                 )
273 | |
274 | |                 # Start simple HTTP server
275 | |                 class DashboardHandler(http.server.SimpleHTTPRequestHandler):
276 | |                     def do_GET(self) -> None:
277 | |                         self.send_response(200)
278 | |                         self.send_header("Content-type", "text/html")
279 | |                         self.end_headers()
280 | |                         self.wfile.write(html_content.encode())
281 | |
282 | |                 def run_server() -> None:
283 | |                     with socketserver.TCPServer((host, port), DashboardHandler) as httpd:
284 | |                         httpd.serve_forever()
285 | |
286 | |                 if detach:
287 | |                     # Run in background
288 | |                     server_thread = threading.Thread(target=run_server, daemon=True)
289 | |                     server_thread.start()
290 | |                     self.print_info(f"Web dashboard started in background at http://{host}:{port}")
291 | |
292 | |                     # Open browser
293 | |                     webbrowser.open(f"http://{host}:{port}")
294 | |
295 | |                     # Keep main thread alive
296 | |                     try:
297 | |                         while True:
298 | |                             time_module.sleep(1)
299 | |                     except KeyboardInterrupt:
300 | |                         self.print_info("Shutting down web dashboard...")
301 | |                 else:
302 | |                     self.print_info(f"Web dashboard running at http://{host}:{port}")
303 | |                     self.print_info("Press Ctrl+C to stop")
304 | |
305 | |                     # Open browser
306 | |                     webbrowser.open(f"http://{host}:{port}")
307 | |
308 | |                     try:
309 | |                         run_server()
310 | |                     except KeyboardInterrupt:
311 | |                         self.print_info("\nShutting down web dashboard...")
312 | |             else:
313 | |                 # Use FastAPI server
314 | |                 try:
315 | |                     if uvicorn is None:
316 | |                         msg = "FastAPI/uvicorn not available. Install with: uv add fastapi uvicorn"
317 | |                         raise CommandError(msg)
318 | |
319 | |                     # Show the URL before starting
320 | |                     dashboard_url = f"http://{host}:{port}"
321 | |                     self.print_info(f"ğŸŒ FastAPI server starting at {dashboard_url}")
322 | |                     self.print_info("ğŸ“Š Dashboard will be available at the URL above")
323 | |
324 | |                     if dev:
325 | |                         # ê°œë°œ ëª¨ë“œì—ì„œëŠ” Vite ê°œë°œ ì„œë²„ë„ í•¨ê»˜ ì‹¤í–‰
326 | |                         self.print_info("ğŸš€ Starting Vite dev server for hot module replacement...")
327 | |                         vite_process = subprocess.Popen(
328 | |                             ["npm", "run", "dev"],
329 | |                             cwd=Path(__file__).parent.parent / "tauri-dashboard",
330 | |                             stdout=subprocess.PIPE,
331 | |                             stderr=subprocess.PIPE,
332 | |                         )
333 | |
334 | |                         # Vite ì„œë²„ê°€ ì‹œì‘ë  ë•Œê¹Œì§€ ëŒ€ê¸°
335 | |                         def wait_for_vite_server() -> bool:
336 | |                             for i in range(20):  # ìµœëŒ€ 20ì´ˆ ëŒ€ê¸°
337 | |                                 try:
338 | |                                     sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
339 | |                                     result = sock.connect_ex(("localhost", 5173))
340 | |                                     sock.close()
341 | |                                     if result == 0:  # í¬íŠ¸ê°€ ì—´ë ¤ìˆìŒ
342 | |                                         return True
343 | |                                     time.sleep(1)
344 | |                                 except Exception:
345 | |                                     time.sleep(1)
346 | |                             return False
347 | |
348 | |                         vite_ready = wait_for_vite_server()
349 | |                         if vite_ready:
350 | |                             self.print_success("Vite server is ready!")
351 | |                         else:
352 | |                             self.print_warning("Vite server may not be ready yet...")
353 | |
354 | |                         self.print_info("ğŸ“ Development mode:")
355 | |                         self.print_info(f"  - API Server: http://{host}:{port}")
356 | |                         self.print_info("  - Frontend (Vite): http://localhost:5173")
357 | |                         self.print_info("  - Changes will be reflected automatically!")
358 | |
359 | |                         if not detach:
360 | |                             # ê°œë°œ ëª¨ë“œì—ì„œëŠ” Vite ì„œë²„ë¡œ ì—°ê²° (ì„œë²„ê°€ ì¤€ë¹„ëœ í›„)
361 | |                             if vite_ready:
362 | |                                 self.print_info("ğŸ”— Opening browser to Vite dev server...")
363 | |                                 webbrowser.open("http://localhost:5173")
364 | |                             else:
365 | |                                 self.print_info("")
366 | |                                 self.print_info("=" * 60)
367 | |                                 self.print_warning("VITE SERVER NOT READY - MANUAL ACTION REQUIRED")
368 | |                                 self.print_info("=" * 60)
369 | |                                 self.print_info("ğŸ”— Please manually open: http://localhost:5173")
370 | |                                 self.print_info("   (Wait a few seconds if the page doesn't load)")
371 | |                                 self.print_info("=" * 60)
372 | |                                 self.print_info("")
373 | |
374 | |                     if not detach and not dev:
375 | |                         # Open browser after a short delay in a separate thread
376 | |                         def open_browser() -> None:
377 | |                             time.sleep(2)  # Wait for server to start
378 | |                             try:
379 | |                                 webbrowser.open(dashboard_url)
380 | |                                 self.print_info(f"ğŸ”— Opening browser to {dashboard_url}")
381 | |                             except Exception as e:
382 | |                                 self.print_warning(f"Could not open browser automatically: {e}")
383 | |                                 self.print_info(f"Please open {dashboard_url} manually in your browser")
384 | |
385 | |                         threading.Thread(target=open_browser, daemon=True).start()
386 | |
387 | |                     try:
388 | |                         uvicorn.run("api.main:app", host=host, port=port, reload=dev)
389 | |                     finally:
390 | |                         if dev and "vite_process" in locals():
391 | |                             vite_process.terminate()
392 | |
393 | |                 except Exception as uvicorn_error:
394 | |                     msg = f"FastAPI server error: {uvicorn_error}"
395 | |                     raise CommandError(msg) from uvicorn_error
396 | |
397 | |         except Exception as e:
398 | |             msg = f"Web dashboard error: {e}"
399 | |             raise CommandError(msg) from e
    | |__________________________________________^ PLR1702
400 |
401 |       def _launch_tauri_dashboard(
    |

commands/dashboard.py:256:17: TRY301 Abstract `raise` to an inner function
    |
254 |             if result == 0:
255 |                 msg = f"Port {port} is already in use. Try a different port with -p option."
256 |                 raise CommandError(msg)
    |                 ^^^^^^^^^^^^^^^^^^^^^^^ TRY301
257 |
258 |             # Start FastAPI server
    |

commands/dashboard.py:317:25: TRY301 Abstract `raise` to an inner function
    |
315 |                     if uvicorn is None:
316 |                         msg = "FastAPI/uvicorn not available. Install with: uv add fastapi uvicorn"
317 |                         raise CommandError(msg)
    |                         ^^^^^^^^^^^^^^^^^^^^^^^ TRY301
318 |
319 |                     # Show the URL before starting
    |

commands/dashboard.py:490:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
488 |         self.env = DashboardEnvironment()
489 |
490 |     def execute(self, **kwargs: Any) -> dict:  # noqa: ARG002
    |                                 ^^^ ANN401
491 |         """Execute the list command.
    |

commands/dashboard.py:537:59: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
535 |     """Build dashboard for production deployment."""
536 |
537 |     def execute(self, interface: str = "tauri", **kwargs: Any) -> dict:  # noqa: ARG002
    |                                                           ^^^ ANN401
538 |         """Execute the build command.
    |

commands/dashboard.py:607:5: PLR0917 Too many positional arguments (6/5)
    |
605 | @click.option("--dev", is_flag=True, default=False, help="Run in development mode")
606 | @click.option("--detach", "-d", is_flag=True, default=False, help="Run in background")
607 | def run(interface: str, host: str, port: int, theme: str | None, dev: bool, detach: bool) -> None:  # noqa: FBT001
    |     ^^^ PLR0917
608 |     """Run the dashboard with specified interface."""
609 |     command = DashboardRunCommand()
    |

commands/enter.py:37:95: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
35 |             raise CommandError(msg)
36 |
37 |     def execute(self, session_name: str | None = None, list_sessions: bool = False, **kwargs: Any) -> dict:  # noqa: FBT001, ARG002
   |                                                                                               ^^^ ANN401
38 |         """Execute the enter command.
   |

commands/enter.py:102:13: TRY300 Consider moving this statement to an `else` block
    |
100 |                 return selected
101 |             self.print_info("Selection cancelled")
102 |             return None
    |             ^^^^^^^^^^^ TRY300
103 |         except Exception:
104 |             # Fallback to text-based selection
    |

commands/enter.py:117:17: TRY300 Consider moving this statement to an `else` block
    |
115 |                     return str(session_data["session"])
116 |                 self.print_error("Invalid selection")
117 |                 return None
    |                 ^^^^^^^^^^^ TRY300
118 |             except click.Abort:
119 |                 self.print_info("Selection cancelled")
    |

commands/logs.py:37:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
35 |         compression: bool = False,  # noqa: FBT001
36 |         buffer_size: int = 1000,
37 |         **kwargs: Any,  # noqa: ARG002
   |                   ^^^ ANN401
38 |     ) -> dict:
39 |         """Execute the configure command.
   |

commands/logs.py:104:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
102 |         last_hours: int = 24,
103 |         level: str | None = None,
104 |         **kwargs: Any,  # noqa: ARG002
    |                   ^^^ ANN401
105 |     ) -> dict:
106 |         """Execute the analyze command.
    |

commands/logs.py:118:17: TRY301 Abstract `raise` to an inner function
    |
116 |             if not log_path.exists():
117 |                 msg = f"Log directory not found: {log_path}"
118 |                 raise CommandError(msg)
    |                 ^^^^^^^^^^^^^^^^^^^^^^^ TRY301
119 |
120 |             # Find log files
    |

commands/logs.py:155:9: PLR1702 Too many nested blocks (6 > 5)
    |
153 |           cutoff_time = time.time() - (last_hours * 3600)
154 |
155 | /         for log_file in log_files:
156 | |             try:
157 | |                 # Handle compressed files
158 | |                 opener = gzip.open if log_file.suffix == ".gz" else open
159 | |
160 | |                 with opener(log_file, "rt", encoding="utf-8") as f:
161 | |                     for raw_line in f:
162 | |                         line = raw_line.strip()
163 | |                         if not line:
164 | |                             continue
165 | |
166 | |                         try:
167 | |                             # Try to parse as JSON
168 | |                             entry = json.loads(line)
169 | |                             timestamp = entry.get("timestamp", 0)
170 | |
171 | |                             # Filter by time
172 | |                             if timestamp < cutoff_time:
173 | |                                 continue
174 | |
175 | |                             level = entry.get("level", "UNKNOWN")
176 | |
177 | |                             # Filter by level
178 | |                             if level_filter and level != level_filter:
179 | |                                 continue
180 | |
181 | |                             stats["total_entries"] += 1
182 | |                             stats["level_counts"][level] += 1
183 | |
184 | |                             # Hourly distribution
185 | |                             hour = int(timestamp // 3600)
186 | |                             stats["hourly_counts"][hour] += 1
187 | |
188 | |                             # Collect errors
189 | |                             if level in {"ERROR", "CRITICAL"} and len(stats["error_messages"]) < 10:
190 | |                                 stats["error_messages"].append(entry.get("message", ""))
191 | |
192 | |                         except json.JSONDecodeError:
193 | |                             # Handle text format
194 | |                             stats["total_entries"] += 1
195 | |
196 | |             except Exception as e:
197 | |                 self.logger.warning(f"Failed to process log file {log_file}: {e}")  # noqa: G004
198 | |                 continue  # Skip problematic files
    | |________________________^ PLR1702
199 |
200 |           return stats
    |

commands/logs.py:155:9: PLR1702 Too many nested blocks (6 > 5)
    |
153 |           cutoff_time = time.time() - (last_hours * 3600)
154 |
155 | /         for log_file in log_files:
156 | |             try:
157 | |                 # Handle compressed files
158 | |                 opener = gzip.open if log_file.suffix == ".gz" else open
159 | |
160 | |                 with opener(log_file, "rt", encoding="utf-8") as f:
161 | |                     for raw_line in f:
162 | |                         line = raw_line.strip()
163 | |                         if not line:
164 | |                             continue
165 | |
166 | |                         try:
167 | |                             # Try to parse as JSON
168 | |                             entry = json.loads(line)
169 | |                             timestamp = entry.get("timestamp", 0)
170 | |
171 | |                             # Filter by time
172 | |                             if timestamp < cutoff_time:
173 | |                                 continue
174 | |
175 | |                             level = entry.get("level", "UNKNOWN")
176 | |
177 | |                             # Filter by level
178 | |                             if level_filter and level != level_filter:
179 | |                                 continue
180 | |
181 | |                             stats["total_entries"] += 1
182 | |                             stats["level_counts"][level] += 1
183 | |
184 | |                             # Hourly distribution
185 | |                             hour = int(timestamp // 3600)
186 | |                             stats["hourly_counts"][hour] += 1
187 | |
188 | |                             # Collect errors
189 | |                             if level in {"ERROR", "CRITICAL"} and len(stats["error_messages"]) < 10:
190 | |                                 stats["error_messages"].append(entry.get("message", ""))
191 | |
192 | |                         except json.JSONDecodeError:
193 | |                             # Handle text format
194 | |                             stats["total_entries"] += 1
195 | |
196 | |             except Exception as e:
197 | |                 self.logger.warning(f"Failed to process log file {log_file}: {e}")  # noqa: G004
198 | |                 continue  # Skip problematic files
    | |________________________^ PLR1702
199 |
200 |           return stats
    |

commands/logs.py:155:9: PLR1702 Too many nested blocks (6 > 5)
    |
153 |           cutoff_time = time.time() - (last_hours * 3600)
154 |
155 | /         for log_file in log_files:
156 | |             try:
157 | |                 # Handle compressed files
158 | |                 opener = gzip.open if log_file.suffix == ".gz" else open
159 | |
160 | |                 with opener(log_file, "rt", encoding="utf-8") as f:
161 | |                     for raw_line in f:
162 | |                         line = raw_line.strip()
163 | |                         if not line:
164 | |                             continue
165 | |
166 | |                         try:
167 | |                             # Try to parse as JSON
168 | |                             entry = json.loads(line)
169 | |                             timestamp = entry.get("timestamp", 0)
170 | |
171 | |                             # Filter by time
172 | |                             if timestamp < cutoff_time:
173 | |                                 continue
174 | |
175 | |                             level = entry.get("level", "UNKNOWN")
176 | |
177 | |                             # Filter by level
178 | |                             if level_filter and level != level_filter:
179 | |                                 continue
180 | |
181 | |                             stats["total_entries"] += 1
182 | |                             stats["level_counts"][level] += 1
183 | |
184 | |                             # Hourly distribution
185 | |                             hour = int(timestamp // 3600)
186 | |                             stats["hourly_counts"][hour] += 1
187 | |
188 | |                             # Collect errors
189 | |                             if level in {"ERROR", "CRITICAL"} and len(stats["error_messages"]) < 10:
190 | |                                 stats["error_messages"].append(entry.get("message", ""))
191 | |
192 | |                         except json.JSONDecodeError:
193 | |                             # Handle text format
194 | |                             stats["total_entries"] += 1
195 | |
196 | |             except Exception as e:
197 | |                 self.logger.warning(f"Failed to process log file {log_file}: {e}")  # noqa: G004
198 | |                 continue  # Skip problematic files
    | |________________________^ PLR1702
199 |
200 |           return stats
    |

commands/logs.py:250:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
248 |         follow: bool = False,  # noqa: FBT001
249 |         last_lines: int = 50,
250 |         **kwargs: Any,  # noqa: ARG002
    |                   ^^^ ANN401
251 |     ) -> dict:
252 |         """Execute the tail command."""
    |

commands/logs.py:258:17: TRY301 Abstract `raise` to an inner function
    |
256 |             if not log_path.exists():
257 |                 msg = f"Log directory not found: {log_path}"
258 |                 raise CommandError(msg)
    |                 ^^^^^^^^^^^^^^^^^^^^^^^ TRY301
259 |
260 |             # Find most recent log file
    |

commands/logs.py:384:90: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
382 |         self.console = Console()
383 |
384 |     def execute(self, log_dir: str = "~/.scripton/yesman/logs", days: int = 7, **kwargs: Any) -> dict:  # noqa: ARG002
    |                                                                                          ^^^ ANN401
385 |         """Execute the cleanup command.
    |

commands/logs.py:426:13: TRY300 Consider moving this statement to an `else` block
    |
424 |                 }
425 |             self.print_info("Cleanup cancelled")
426 |             return {"success": False, "cancelled": True}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
427 |
428 |         except Exception as e:
    |

commands/ls.py:20:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
18 |     """List all available projects and templates."""
19 |
20 |     def execute(self, **kwargs: Any) -> dict[str]:
   |                                 ^^^ ANN401
21 |         """Execute the command.
   |

commands/ls.py:91:13: TRY300 Consider moving this statement to an `else` block
   |
89 |                 )
90 |
91 |             return projects
   |             ^^^^^^^^^^^^^^^ TRY300
92 |
93 |         except Exception as e:
   |

commands/ls.py:106:13: TRY300 Consider moving this statement to an `else` block
    |
104 |             if self.session_exists(session_name):
105 |                 return "running"
106 |             return "stopped"
    |             ^^^^^^^^^^^^^^^^ TRY300
107 |         except Exception:
108 |             return "unknown"
    |

commands/multi_agent/agent_pool.py:33:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
31 |         work_dir: str | None = None,
32 |         monitor: bool = False,  # noqa: FBT001
33 |         **kwargs: Any,  # noqa: ARG002
   |                   ^^^ ANN401
34 |     ) -> dict:
35 |         """Execute the start agents command.
   |

commands/multi_agent/agent_pool.py:79:13: TRY300 Consider moving this statement to an `else` block
   |
78 |                 asyncio.run(run_pool())
79 |             return {"success": True, "max_agents": max_agents, "work_dir": work_dir}
   |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
80 |
81 |         except Exception as e:
   |

commands/multi_agent/agent_pool.py:94:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
92 |         duration: float | None = None,
93 |         refresh: float = 1.0,
94 |         **kwargs: Any,  # noqa: ARG002
   |                   ^^^ ANN401
95 |     ) -> dict:
96 |         """Execute the monitor agents command.
   |

commands/multi_agent/agent_pool.py:134:61: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `frame`
    |
132 |                     self.print_info(f"â±ï¸  Monitoring for {duration} seconds...")
133 |
134 |                     def timeout_handler(signum: int, frame: Any) -> Never:  # noqa: ARG001
    |                                                             ^^^ ANN401
135 |                         raise KeyboardInterrupt
    |

commands/multi_agent/agent_pool.py:135:25: TRY301 Abstract `raise` to an inner function
    |
134 |                     def timeout_handler(signum: int, frame: Any) -> Never:  # noqa: ARG001
135 |                         raise KeyboardInterrupt
    |                         ^^^^^^^^^^^^^^^^^^^^^^^ TRY301
136 |
137 |                     signal.signal(signal.SIGALRM, timeout_handler)
    |

commands/multi_agent/agent_pool.py:146:13: TRY300 Consider moving this statement to an `else` block
    |
145 |             asyncio.run(run_monitor())
146 |             return {"success": True, "work_dir": work_dir, "duration": duration}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
147 |
148 |         except Exception as e:
    |

commands/multi_agent/agent_pool.py:156:62: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
154 |     """Show current agent pool status."""
155 |
156 |     def execute(self, work_dir: str | None = None, **kwargs: Any) -> dict:  # noqa: ARG002
    |                                                              ^^^ ANN401
157 |         """Execute the status command.
    |

commands/multi_agent/agent_pool.py:197:13: TRY300 Consider moving this statement to an `else` block
    |
195 |                       self.print_info(f"  {status_icon} {agent['agent_id']} - Completed: {agent.get('completed_tasks', 0)}, Failed: {agâ€¦
196 |
197 | /             return {
198 | |                 "success": True,
199 | |                 "work_dir": work_dir,
200 | |                 "statistics": stats,
201 | |                 "agents": agents,
202 | |                 "tasks": tasks,
203 | |             }
    | |_____________^ TRY300
204 |
205 |           except Exception as e:
    |

commands/multi_agent/agent_pool.py:213:62: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
211 |     """Stop the multi-agent pool."""
212 |
213 |     def execute(self, work_dir: str | None = None, **kwargs: Any) -> dict:  # noqa: ARG002
    |                                                              ^^^ ANN401
214 |         """Execute the stop agents command.
    |

commands/multi_agent/agent_pool.py:230:13: TRY300 Consider moving this statement to an `else` block
    |
228 |               asyncio.run(stop_pool())
229 |
230 | /             return {
231 | |                 "success": True,
232 | |                 "work_dir": work_dir,
233 | |                 "message": "Agent pool stopped successfully",
234 | |             }
    | |_____________^ TRY300
235 |
236 |           except Exception as e:
    |

commands/multi_agent/agent_pool.py:244:9: PLR0917 Too many positional arguments (8/5)
    |
242 |     """Add a task to the agent pool queue."""
243 |
244 |     def execute(
    |         ^^^^^^^ PLR0917
245 |         self,
246 |         title: str | None = None,
    |

commands/multi_agent/agent_pool.py:254:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
252 |         timeout: int = 300,
253 |         description: str | None = None,
254 |         **kwargs: Any,  # noqa: ARG002
    |                   ^^^ ANN401
255 |     ) -> dict:
256 |         """Execute the add task command.
    |

commands/multi_agent/agent_pool.py:265:17: TRY301 Abstract `raise` to an inner function
    |
263 |             if not title:
264 |                 msg = "Task title is required"
265 |                 raise CommandError(msg)
    |                 ^^^^^^^^^^^^^^^^^^^^^^^ TRY301
266 |             if not command:
267 |                 msg = "Task command is required"
    |

commands/multi_agent/agent_pool.py:268:17: TRY301 Abstract `raise` to an inner function
    |
266 |             if not command:
267 |                 msg = "Task command is required"
268 |                 raise CommandError(msg)
    |                 ^^^^^^^^^^^^^^^^^^^^^^^ TRY301
269 |
270 |             pool = AgentPool(work_dir=work_dir)
    |

commands/multi_agent/agent_pool.py:287:13: TRY300 Consider moving this statement to an `else` block
    |
285 |               self.print_info(f"   Priority: {task.priority}")
286 |
287 | /             return {
288 | |                 "success": True,
289 | |                 "work_dir": work_dir,
290 | |                 "task_id": task.task_id,
291 | |                 "title": task.title,
292 | |                 "command": task.command,
293 | |                 "priority": task.priority,
294 | |             }
    | |_____________^ TRY300
295 |
296 |           except Exception as e:
    |

commands/multi_agent/agent_pool.py:304:89: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
302 |     """List tasks in the agent pool."""
303 |
304 |     def execute(self, work_dir: str | None = None, status: str | None = None, **kwargs: Any) -> dict:  # noqa: ARG002
    |                                                                                         ^^^ ANN401
305 |         """Execute the list tasks command.
    |

commands/multi_agent/batch_operations.py:19:19: ARG004 Unused static method argument: `kwargs`
   |
18 |     @staticmethod
19 |     def execute(**kwargs: dict[str, object]) -> dict:  # noqa: ARG002  # noqa: ARG004
   |                   ^^^^^^ ARG004
20 |         """Execute the batch merge command.
   |

commands/multi_agent/batch_operations.py:35:19: ARG004 Unused static method argument: `kwargs`
   |
34 |     @staticmethod
35 |     def execute(**kwargs: dict[str, object]) -> dict:  # noqa: ARG002  # noqa: ARG004
   |                   ^^^^^^ ARG004
36 |         """Execute the auto-resolve command.
   |

commands/multi_agent/batch_operations.py:51:19: ARG004 Unused static method argument: `kwargs`
   |
50 |     @staticmethod
51 |     def execute(**kwargs: dict[str, object]) -> dict:  # noqa: ARG002  # noqa: ARG004
   |                   ^^^^^^ ARG004
52 |         """Execute the prevent conflicts command.
   |

commands/multi_agent/cli.py:72:5: PLR0917 Too many positional arguments (8/5)
   |
70 | @click.option("--timeout", "-t", default=300, help="Task timeout in seconds")
71 | @click.option("--description", help="Task description")
72 | def add_task(title: str, command: tuple[str, ...], work_dir: str | None, directory: str, priority: int, complexity: int, timeout: int,â€¦
   |     ^^^^^^^^ PLR0917
73 |     """Add a task to the agent pool queue."""
74 |     command_obj = AddTaskCommand()
   |

commands/multi_agent/code_review.py:19:19: ARG004 Unused static method argument: `kwargs`
   |
18 |     @staticmethod
19 |     def execute(**kwargs: dict[str, object]) -> dict:  # noqa: ARG002  # noqa: ARG004
   |                   ^^^^^^ ARG004
20 |         """Execute the review initiate command.
   |

commands/multi_agent/code_review.py:35:19: ARG004 Unused static method argument: `kwargs`
   |
34 |     @staticmethod
35 |     def execute(**kwargs: dict[str, object]) -> dict:  # noqa: ARG002  # noqa: ARG004
   |                   ^^^^^^ ARG004
36 |         """Execute the review approve command.
   |

commands/multi_agent/code_review.py:51:19: ARG004 Unused static method argument: `kwargs`
   |
50 |     @staticmethod
51 |     def execute(**kwargs: dict[str, object]) -> dict:  # noqa: ARG002  # noqa: ARG004
   |                   ^^^^^^ ARG004
52 |         """Execute the review reject command.
   |

commands/multi_agent/code_review.py:67:19: ARG004 Unused static method argument: `kwargs`
   |
66 |     @staticmethod
67 |     def execute(**kwargs: dict[str, object]) -> dict:  # noqa: ARG002  # noqa: ARG004
   |                   ^^^^^^ ARG004
68 |         """Execute the review status command.
   |

commands/multi_agent/code_review.py:83:19: ARG004 Unused static method argument: `kwargs`
   |
82 |     @staticmethod
83 |     def execute(**kwargs: dict[str, object]) -> dict:  # noqa: ARG002  # noqa: ARG004
   |                   ^^^^^^ ARG004
84 |         """Execute the quality check command.
   |

commands/multi_agent/code_review.py:99:19: ARG004 Unused static method argument: `kwargs`
    |
 98 |     @staticmethod
 99 |     def execute(**kwargs: dict[str, object]) -> dict:  # noqa: ARG002  # noqa: ARG004
    |                   ^^^^^^ ARG004
100 |         """Execute the review summary command.
    |

commands/multi_agent/collaboration.py:19:19: ARG004 Unused static method argument: `kwargs`
   |
18 |     @staticmethod
19 |     def execute(**kwargs: dict[str, object]) -> dict:  # noqa: ARG002  # noqa: ARG004
   |                   ^^^^^^ ARG004
20 |         """Execute the collaborate command.
   |

commands/multi_agent/collaboration.py:35:19: ARG004 Unused static method argument: `kwargs`
   |
34 |     @staticmethod
35 |     def execute(**kwargs: dict[str, object]) -> dict:  # noqa: ARG002  # noqa: ARG004
   |                   ^^^^^^ ARG004
36 |         """Execute the send message command.
   |

commands/multi_agent/collaboration.py:51:19: ARG004 Unused static method argument: `kwargs`
   |
50 |     @staticmethod
51 |     def execute(**kwargs: dict[str, object]) -> dict:  # noqa: ARG002  # noqa: ARG004
   |                   ^^^^^^ ARG004
52 |         """Execute the share knowledge command.
   |

commands/multi_agent/collaboration.py:67:19: ARG004 Unused static method argument: `kwargs`
   |
66 |     @staticmethod
67 |     def execute(**kwargs: dict[str, object]) -> dict:  # noqa: ARG002  # noqa: ARG004
   |                   ^^^^^^ ARG004
68 |         """Execute the branch info command.
   |

commands/multi_agent/conflict_prediction.py:31:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
29 |         min_confidence: float = 0.3,
30 |         limit: int = 10,
31 |         **kwargs: Any,  # noqa: ARG002
   |                   ^^^ ANN401
32 |     ) -> dict:
33 |         """Execute the predict conflicts command.
   |

commands/multi_agent/conflict_prediction.py:42:17: TRY301 Abstract `raise` to an inner function
   |
40 |             if not branches:
41 |                 msg = "Branches list is required for conflict prediction"
42 |                 raise CommandError(msg)
   |                 ^^^^^^^^^^^^^^^^^^^^^^^ TRY301
43 |
44 |             self.print_info(f"ğŸ”® Predicting conflicts for branches: {', '.join(branches)}")
   |

commands/multi_agent/conflict_prediction.py:57:23: ANN202 Missing return type annotation for private function `run_prediction`
   |
55 |             predictor.max_predictions_per_run = limit * 2  # Get more, filter later
56 |
57 |             async def run_prediction():
   |                       ^^^^^^^^^^^^^^ ANN202
58 |                 horizon = timedelta(days=time_horizon)
59 |                 predictions = await predictor.predict_conflicts(branches, horizon)
   |
   = help: Add return type annotation

commands/multi_agent/conflict_prediction.py:124:63: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
122 |     """Show prediction summary and statistics."""
123 |
124 |     def execute(self, repo_path: str | None = None, **kwargs: Any) -> dict:  # noqa: ARG002
    |                                                               ^^^ ANN401
125 |         """Execute the prediction summary command.
    |

commands/multi_agent/conflict_prediction.py:156:13: TRY300 Consider moving this statement to an `else` block
    |
154 |                         self.print_info(f"  {confidence.capitalize()}: {count}")
155 |
156 |             return {"success": True, "repo_path": repo_path, "summary": summary}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
157 |
158 |         except Exception as e:
    |

commands/multi_agent/conflict_prediction.py:166:63: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
164 |     """Analyze detailed conflict patterns and trends."""
165 |
166 |     def execute(self, repo_path: str | None = None, **kwargs: Any) -> dict:  # noqa: ARG002
    |                                                               ^^^ ANN401
167 |         """Execute the analyze conflict patterns command.
    |

commands/multi_agent/conflict_prediction.py:196:13: TRY300 Consider moving this statement to an `else` block
    |
194 |                     self.print_info(f"  ğŸ“ {hotspot['location']}: {hotspot['severity']}")
195 |
196 |             return {"success": True, "repo_path": repo_path, "analysis": analysis}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
197 |
198 |         except Exception as e:
    |

commands/multi_agent/conflict_resolution.py:24:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
22 |     """Detect conflicts between branches."""
23 |
24 |     def execute(self, **kwargs: Any) -> dict:
   |                                 ^^^ ANN401
25 |         """Execute the command.
   |

commands/multi_agent/conflict_resolution.py:55:27: ANN202 Missing return type annotation for private function `run_detection`
   |
53 |                 engine = ConflictResolutionEngine(branch_manager, repo_path)
54 |
55 |                 async def run_detection():
   |                           ^^^^^^^^^^^^^ ANN202
56 |                     conflicts = await engine.detect_potential_conflicts(branches)
57 |                     progress.update(detection_task, description="âœ… Conflict detection completed")
   |
   = help: Add return type annotation

commands/multi_agent/conflict_resolution.py:126:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
124 |     """Resolve a specific conflict."""
125 |
126 |     def execute(self, **kwargs: Any) -> dict:
    |                                 ^^^ ANN401
127 |         """Execute the command.
    |

commands/multi_agent/conflict_resolution.py:159:23: ANN202 Missing return type annotation for private function `run_resolution`
    |
157 |                     ) from e
158 |
159 |             async def run_resolution():
    |                       ^^^^^^^^^^^^^^ ANN202
160 |                 result = await engine.resolve_conflict(conflict_id, resolution_strategy)
    |
    = help: Add return type annotation

commands/multi_agent/conflict_resolution.py:198:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
196 |     """Show conflict resolution summary and statistics."""
197 |
198 |     def execute(self, **kwargs: Any) -> dict:
    |                                 ^^^ ANN401
199 |         """Execute the command.
    |

commands/multi_agent/conflict_resolution.py:261:13: TRY300 Consider moving this statement to an `else` block
    |
259 |                 self.print_info(f"  Average time: {stats['average_resolution_time']:.2f}s")
260 |
261 |             return {"success": True, "repo_path": repo_path, "summary": summary}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
262 |
263 |         except Exception as e:
    |

commands/multi_agent/dependency_tracking.py:19:19: ARG004 Unused static method argument: `kwargs`
   |
18 |     @staticmethod
19 |     def execute(**kwargs: dict[str, object]) -> dict:  # noqa: ARG002  # noqa: ARG004
   |                   ^^^^^^ ARG004
20 |         """Execute the dependency track command.
   |

commands/multi_agent/dependency_tracking.py:35:19: ARG004 Unused static method argument: `kwargs`
   |
34 |     @staticmethod
35 |     def execute(**kwargs: dict[str, object]) -> dict:  # noqa: ARG002  # noqa: ARG004
   |                   ^^^^^^ ARG004
36 |         """Execute the dependency status command.
   |

commands/multi_agent/dependency_tracking.py:51:19: ARG004 Unused static method argument: `kwargs`
   |
50 |     @staticmethod
51 |     def execute(**kwargs: dict[str, object]) -> dict:  # noqa: ARG002  # noqa: ARG004
   |                   ^^^^^^ ARG004
52 |         """Execute the dependency impact command.
   |

commands/multi_agent/dependency_tracking.py:67:19: ARG004 Unused static method argument: `kwargs`
   |
66 |     @staticmethod
67 |     def execute(**kwargs: dict[str, object]) -> dict:  # noqa: ARG002  # noqa: ARG004
   |                   ^^^^^^ ARG004
68 |         """Execute the dependency propagate command.
   |

commands/multi_agent/semantic_analysis.py:33:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
31 |         language: str = "python",
32 |         repo_path: str | None = None,
33 |         **kwargs: Any,
   |                   ^^^ ANN401
34 |     ) -> dict[str, Any]:
35 |         """Execute the analyze semantic conflicts command.
   |

commands/multi_agent/semantic_analysis.py:47:17: TRY301 Abstract `raise` to an inner function
   |
45 |             if not files:
46 |                 msg = "Files parameter is required for conflict analysis"
47 |                 raise CommandError(msg)
   |                 ^^^^^^^^^^^^^^^^^^^^^^^ TRY301
48 |
49 |             self.print_info(f"ğŸ§  Analyzing semantic conflicts in {len(files)} files...")
   |

commands/multi_agent/semantic_analysis.py:55:23: ANN202 Missing return type annotation for private function `run_analysis`
   |
53 |             analyzer = SemanticAnalyzer(branch_manager=branch_manager, repo_path=repo_path)
54 |
55 |             async def run_analysis():
   |                       ^^^^^^^^^^^^ ANN202
56 |                 # For semantic conflict analysis, we need branches
57 |                 if len(files) < MIN_BRANCHES_FOR_ANALYSIS:
   |
   = help: Add return type annotation

commands/multi_agent/semantic_analysis.py:59:21: TRY301 Abstract `raise` to an inner function
   |
57 |                 if len(files) < MIN_BRANCHES_FOR_ANALYSIS:
58 |                     msg = "At least 2 branches are required for conflict analysis"
59 |                     raise CommandError(msg)
   |                     ^^^^^^^^^^^^^^^^^^^^^^^ TRY301
60 |
61 |                 branch1, branch2 = files[0], files[1]
   |

commands/multi_agent/semantic_analysis.py:110:63: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
108 |     """Show semantic analysis summary."""
109 |
110 |     def execute(self, repo_path: str | None = None, **kwargs: Any) -> dict[str]:  # noqa: ARG002
    |                                                               ^^^ ANN401
111 |         """Execute the semantic summary command.
    |

commands/multi_agent/semantic_analysis.py:129:13: TRY300 Consider moving this statement to an `else` block
    |
127 |             self.print_info(f"Class Conflicts: {summary['class_conflicts']}")
128 |
129 |             return {"success": True, "repo_path": repo_path, "summary": summary}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
130 |
131 |         except Exception as e:
    |

commands/multi_agent/semantic_analysis.py:139:111: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
137 |     """Show function-level differences."""
138 |
139 |     def execute(self, file1: str | None = None, file2: str | None = None, language: str = "python", **kwargs: Any) -> dict[str]:  # nâ€¦
    |                                                                                                               ^^^ ANN401
140 |         """Execute the function diff command.
    |

commands/multi_agent/semantic_analysis.py:154:17: TRY301 Abstract `raise` to an inner function
    |
152 |             if not file1 or not file2:
153 |                 msg = "Both file1 and file2 parameters are required"
154 |                 raise CommandError(msg)
    |                 ^^^^^^^^^^^^^^^^^^^^^^^ TRY301
155 |
156 |             self.print_info(f"ğŸ” Function-level diff: {file1} vs {file2}")
    |

commands/multi_agent/semantic_analysis.py:180:13: TRY300 Consider moving this statement to an `else` block
    |
178 |                 self.print_info("No function differences available in current implementation")
179 |
180 |             return {"success": True, "file1": file1, "file2": file2, "diff": diff}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
181 |
182 |         except Exception as e:
    |

commands/multi_agent/semantic_analysis.py:196:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
194 |         language: str = "python",  # noqa: ARG002
195 |         strategy: str = "auto",
196 |         **kwargs: Any,
    |                   ^^^ ANN401
197 |     ) -> dict[str, Any]:
198 |         """Execute the semantic merge command.
    |

commands/multi_agent/semantic_analysis.py:212:17: TRY301 Abstract `raise` to an inner function
    |
210 |             if not source_file or not target_file:
211 |                 msg = "Both source_file and target_file parameters are required"
212 |                 raise CommandError(msg)
    |                 ^^^^^^^^^^^^^^^^^^^^^^^ TRY301
213 |
214 |             self.print_info(f"ğŸ”€ Semantic merge: {source_file} â†’ {target_file}")
    |

commands/multi_agent/semantic_analysis.py:228:23: ANN202 Missing return type annotation for private function `run_merge`
    |
226 |                 merge_strategy = MergeStrategy.INTELLIGENT_MERGE  # Default fallback
227 |
228 |             async def run_merge():
    |                       ^^^^^^^^^ ANN202
229 |                 result = await merger.perform_semantic_merge(file_path=target_file, branch1="current", branch2="other", strategy=mergâ€¦
    |
    = help: Add return type annotation

commands/multi_agent_backup.py:67:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
65 |     """Start the multi-agent pool."""
66 |
67 |     def execute(self, **kwargs: Any) -> dict[str, object]:
   |                                 ^^^ ANN401
68 |         """Execute the command."""
69 |         # Extract parameters from kwargs
   |

commands/multi_agent_backup.py:77:9: PLR1702 Too many nested blocks (6 > 5)
    |
 75 |           monitor = kwargs.get("monitor", False)
 76 |           """Execute the start agents command."""
 77 | /         try:
 78 | |             # Create progress indicator for agent startup
 79 | |             with Progress(
 80 | |                 SpinnerColumn(),
 81 | |                 TextColumn("[bold blue]{task.description}"),
 82 | |                 TimeElapsedColumn(),
 83 | |                 transient=True,
 84 | |             ) as progress:
 85 | |                 startup_task = progress.add_task(
 86 | |                     f"ğŸ¤– Starting multi-agent pool with {max_agents} agents...",
 87 | |                     total=None,
 88 | |                 )
 89 | |
 90 | |                 # Create agent pool
 91 | |                 pool = AgentPool(max_agents=max_agents, work_dir=work_dir)
 92 | |
 93 | |                 async def run_pool() -> None:
 94 | |                     await pool.start()
 95 | |                     progress.update(startup_task, description="âœ… Agent pool started successfully")
 96 | |
 97 | |                     if monitor:
 98 | |                         progress.update(
 99 | |                             startup_task,
100 | |                             description="ğŸ“Š Starting monitoring dashboard...",
101 | |                         )
102 | |                         await run_agent_monitor(pool)
103 | |                     else:
104 | |                         self.print_success("âœ… Agent pool started successfully")
105 | |                         self.print_info("Use 'yesman multi-agent monitor' to view status")
106 | |
107 | |                         # Keep running until interrupted
108 | |                         try:
109 | |                             event = asyncio.Event()
110 | |                             while not event.is_set():
111 | |                                 if hasattr(pool, "is_running") and not pool.is_running:
112 | |                                     break
113 | |                                 if hasattr(pool, "_running") and not pool._running:  # noqa: SLF001
114 | |                                     break
115 | |                                 await event.wait()
116 | |                         except KeyboardInterrupt:
117 | |                             self.print_warning("\nğŸ›‘ Stopping agent pool...")
118 | |                             await pool.stop()
119 | |
120 | |                 asyncio.run(run_pool())
121 | |         except Exception as e:
122 | |             msg = f"Error starting agents: {e}"
123 | |             raise CommandError(msg) from e
124 | |         else:
125 | |             return {"success": True, "max_agents": max_agents, "work_dir": work_dir}
    | |____________________________________________________________________________________^ PLR1702
    |

commands/multi_agent_backup.py:77:9: PLR1702 Too many nested blocks (6 > 5)
    |
 75 |           monitor = kwargs.get("monitor", False)
 76 |           """Execute the start agents command."""
 77 | /         try:
 78 | |             # Create progress indicator for agent startup
 79 | |             with Progress(
 80 | |                 SpinnerColumn(),
 81 | |                 TextColumn("[bold blue]{task.description}"),
 82 | |                 TimeElapsedColumn(),
 83 | |                 transient=True,
 84 | |             ) as progress:
 85 | |                 startup_task = progress.add_task(
 86 | |                     f"ğŸ¤– Starting multi-agent pool with {max_agents} agents...",
 87 | |                     total=None,
 88 | |                 )
 89 | |
 90 | |                 # Create agent pool
 91 | |                 pool = AgentPool(max_agents=max_agents, work_dir=work_dir)
 92 | |
 93 | |                 async def run_pool() -> None:
 94 | |                     await pool.start()
 95 | |                     progress.update(startup_task, description="âœ… Agent pool started successfully")
 96 | |
 97 | |                     if monitor:
 98 | |                         progress.update(
 99 | |                             startup_task,
100 | |                             description="ğŸ“Š Starting monitoring dashboard...",
101 | |                         )
102 | |                         await run_agent_monitor(pool)
103 | |                     else:
104 | |                         self.print_success("âœ… Agent pool started successfully")
105 | |                         self.print_info("Use 'yesman multi-agent monitor' to view status")
106 | |
107 | |                         # Keep running until interrupted
108 | |                         try:
109 | |                             event = asyncio.Event()
110 | |                             while not event.is_set():
111 | |                                 if hasattr(pool, "is_running") and not pool.is_running:
112 | |                                     break
113 | |                                 if hasattr(pool, "_running") and not pool._running:  # noqa: SLF001
114 | |                                     break
115 | |                                 await event.wait()
116 | |                         except KeyboardInterrupt:
117 | |                             self.print_warning("\nğŸ›‘ Stopping agent pool...")
118 | |                             await pool.stop()
119 | |
120 | |                 asyncio.run(run_pool())
121 | |         except Exception as e:
122 | |             msg = f"Error starting agents: {e}"
123 | |             raise CommandError(msg) from e
124 | |         else:
125 | |             return {"success": True, "max_agents": max_agents, "work_dir": work_dir}
    | |____________________________________________________________________________________^ PLR1702
    |

commands/multi_agent_backup.py:131:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
129 |     """Start real-time agent monitoring dashboard."""
130 |
131 |     def execute(self, **kwargs: Any) -> dict[str, object]:
    |                                 ^^^ ANN401
132 |         """Execute the command."""
133 |         # Extract parameters from kwargs
    |

commands/multi_agent_backup.py:174:25: TRY301 Abstract `raise` to an inner function
    |
173 |                     def timeout_handler(_signum: int, _frame: types.FrameType | None) -> Never:
174 |                         raise KeyboardInterrupt
    |                         ^^^^^^^^^^^^^^^^^^^^^^^ TRY301
175 |
176 |                     signal.signal(signal.SIGALRM, timeout_handler)
    |

commands/multi_agent_backup.py:185:13: TRY300 Consider moving this statement to an `else` block
    |
184 |             asyncio.run(run_monitor())
185 |             return {"success": True, "work_dir": work_dir, "duration": duration}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
186 |
187 |         except Exception as e:
    |

commands/multi_agent_backup.py:195:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
193 |     """Show current agent pool status."""
194 |
195 |     def execute(self, **kwargs: Any) -> dict[str, object]:
    |                                 ^^^ ANN401
196 |         """Execute the command."""
197 |         # Extract parameters from kwargs
    |

commands/multi_agent_backup.py:236:13: TRY300 Consider moving this statement to an `else` block
    |
234 |                       self.print_info(f"  {status_icon} {agent['agent_id']} - Completed: {agent.get('completed_tasks', 0)}, Failed: {agâ€¦
235 |
236 | /             return {
237 | |                 "success": True,
238 | |                 "work_dir": work_dir,
239 | |                 "statistics": stats,
240 | |                 "agents": agents,
241 | |                 "tasks": tasks,
242 | |             }
    | |_____________^ TRY300
243 |
244 |           except Exception as e:
    |

commands/multi_agent_backup.py:252:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
250 |     """Stop the multi-agent pool."""
251 |
252 |     def execute(self, **kwargs: Any) -> dict[str, object]:
    |                                 ^^^ ANN401
253 |         """Execute the command."""
254 |         # Extract parameters from kwargs
    |

commands/multi_agent_backup.py:269:13: TRY300 Consider moving this statement to an `else` block
    |
267 |               asyncio.run(stop_pool())
268 |
269 | /             return {
270 | |                 "success": True,
271 | |                 "work_dir": work_dir,
272 | |                 "message": "Agent pool stopped successfully",
273 | |             }
    | |_____________^ TRY300
274 |
275 |           except Exception as e:
    |

commands/multi_agent_backup.py:283:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
281 |     """Add a task to the agent pool queue."""
282 |
283 |     def execute(self, **kwargs: Any) -> dict[str, object]:
    |                                 ^^^ ANN401
284 |         """Execute the add task command."""
285 |         # Extract parameters from kwargs
    |

commands/multi_agent_backup.py:313:13: TRY300 Consider moving this statement to an `else` block
    |
311 |               self.print_info(f"   Priority: {task.priority}")
312 |
313 | /             return {
314 | |                 "success": True,
315 | |                 "work_dir": work_dir,
316 | |                 "task_id": task.task_id,
317 | |                 "title": task.title,
318 | |                 "command": task.command,
319 | |                 "priority": task.priority,
320 | |             }
    | |_____________^ TRY300
321 |
322 |           except Exception as e:
    |

commands/multi_agent_backup.py:330:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
328 |     """List tasks in the agent pool."""
329 |
330 |     def execute(self, **kwargs: Any) -> dict[str, object]:
    |                                 ^^^ ANN401
331 |         """Execute the command."""
332 |         # Extract parameters from kwargs
    |

commands/multi_agent_backup.py:394:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
392 |     """Detect conflicts between branches."""
393 |
394 |     def execute(self, **kwargs: Any) -> dict[str, object]:
    |                                 ^^^ ANN401
395 |         """Execute the command."""
396 |         # Extract parameters from kwargs
    |

commands/multi_agent_backup.py:421:27: ANN202 Missing return type annotation for private function `run_detection`
    |
419 |                 engine = ConflictResolutionEngine(branch_manager, repo_path)
420 |
421 |                 async def run_detection():
    |                           ^^^^^^^^^^^^^ ANN202
422 |                     return await engine.detect_potential_conflicts(branches)
    |
    = help: Add return type annotation

commands/multi_agent_backup.py:463:31: ANN202 Missing return type annotation for private function `run_auto_resolve`
    |
461 |                     self.print_info("ğŸ”§ Attempting automatic resolution...")
462 |
463 |                     async def run_auto_resolve():
    |                               ^^^^^^^^^^^^^^^^ ANN202
464 |                         return await engine.auto_resolve_all()
    |
    = help: Add return type annotation

commands/multi_agent_backup.py:499:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
497 |     """Resolve a specific conflict."""
498 |
499 |     def execute(self, **kwargs: Any) -> dict[str, object]:
    |                                 ^^^ ANN401
500 |         """Execute the command."""
501 |         # Extract parameters from kwargs
    |

commands/multi_agent_backup.py:528:23: ANN202 Missing return type annotation for private function `run_resolution`
    |
526 |                     ) from e
527 |
528 |             async def run_resolution():
    |                       ^^^^^^^^^^^^^^ ANN202
529 |                 result = await engine.resolve_conflict(conflict_id, resolution_strategy)
    |
    = help: Add return type annotation

commands/multi_agent_backup.py:567:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
565 |     """Show conflict resolution summary and statistics."""
566 |
567 |     def execute(self, **kwargs: Any) -> dict[str, object]:
    |                                 ^^^ ANN401
568 |         """Execute the command."""
569 |         # Extract parameters from kwargs
    |

commands/multi_agent_backup.py:626:13: TRY300 Consider moving this statement to an `else` block
    |
624 |                 self.print_info(f"  Average time: {stats['average_resolution_time']:.2f}s")
625 |
626 |             return {"success": True, "repo_path": repo_path, "summary": summary}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
627 |
628 |         except Exception as e:
    |

commands/multi_agent_backup.py:636:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
634 |     """Predict potential conflicts between branches."""
635 |
636 |     def execute(self, **kwargs: Any) -> dict[str, object]:
    |                                 ^^^ ANN401
637 |         """Execute the command."""
638 |         # Extract parameters from kwargs
    |

commands/multi_agent_backup.py:664:23: ANN202 Missing return type annotation for private function `run_prediction`
    |
662 |             predictor.max_predictions_per_run = limit * 2  # Get more, filter later
663 |
664 |             async def run_prediction():
    |                       ^^^^^^^^^^^^^^ ANN202
665 |                 horizon = timedelta(days=time_horizon)
666 |                 predictions = await predictor.predict_conflicts(branches, horizon)
    |
    = help: Add return type annotation

commands/multi_agent_backup.py:755:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
753 |     """Show conflict prediction summary and statistics."""
754 |
755 |     def execute(self, **kwargs: Any) -> dict[str, object]:
    |                                 ^^^ ANN401
756 |         """Execute the command."""
757 |         # Extract parameters from kwargs
    |

commands/multi_agent_backup.py:820:13: TRY300 Consider moving this statement to an `else` block
    |
818 |                     self.print_info(f"  â€¢ {conflict['description']} ({conflict['likelihood']:.1%})")
819 |
820 |             return {"success": True, "repo_path": repo_path, "summary": summary}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
821 |
822 |         except Exception as e:
    |

commands/multi_agent_backup.py:830:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
828 |     """Analyze detailed conflict patterns between branches."""
829 |
830 |     def execute(self, **kwargs: Any) -> dict[str, object]:
    |                                 ^^^ ANN401
831 |         """Execute the command."""
832 |         # Extract parameters from kwargs
    |

commands/multi_agent_backup.py:842:9: PLR1702 Too many nested blocks (7 > 5)
    |
840 |           export = kwargs.get("export")
841 |           """Execute the analyze conflict patterns command."""
842 | /         try:
843 | |             self.print_info(f"ğŸ” Analyzing conflict patterns for: {', '.join(branches)}")
844 | |
845 | |             # Create prediction system
846 | |             branch_manager = BranchManager(repo_path=repo_path)
847 | |             conflict_engine = ConflictResolutionEngine(branch_manager, repo_path)
848 | |             predictor = ConflictPredictor(conflict_engine, branch_manager, repo_path)
849 | |
850 | |             async def run_analysis():
851 | |                 analysis_results = {}
852 | |
853 | |                 # Calculate conflict vectors for all pairs
854 | |                 self.print_info("\nğŸ“Š Conflict Vector Analysis:")
855 | |                 self.print_info("-" * 40)
856 | |
857 | |                 for i, branch1 in enumerate(branches):
858 | |                     for branch2 in branches[i + 1 :]:
859 | |                         self.print_info(f"\nğŸ”— {branch1} â†” {branch2}")
860 | |
861 | |                         vector = await predictor._calculate_conflict_vector(branch1, branch2)
862 | |                         analysis_results[f"{branch1}:{branch2}"] = {
863 | |                             "vector": vector._asdict(),
864 | |                             "patterns": {},
865 | |                             "details": [],
866 | |                         }
867 | |
868 | |                         # Display vector components
869 | |                         self.print_info(f"   File Overlap: {vector.file_overlap_score:.2f}")
870 | |                         self.print_info(f"   Change Frequency: {vector.change_frequency_score:.2f}")
871 | |                         self.print_info(f"   Complexity: {vector.complexity_score:.2f}")
872 | |                         self.print_info(f"   Dependency Coupling: {vector.dependency_coupling_score:.2f}")
873 | |                         self.print_info(f"   Semantic Distance: {vector.semantic_distance_score:.2f}")
874 | |                         self.print_info(f"   Temporal Proximity: {vector.temporal_proximity_score:.2f}")
875 | |
876 | |                         # Overall risk score
877 | |                         risk_score = sum(vector) / len(vector)
878 | |                         risk_level = "ğŸ”´ HIGH" if risk_score > RISK_THRESHOLD_HIGH else "ğŸŸ¡ MEDIUM" if risk_score > RISK_THRESHOLD_MED â€¦
879 | |                         self.print_info(f"   Overall Risk: {risk_level} ({risk_score:.2f})")
880 | |
881 | |                         # Pattern-specific analysis
882 | |                         if pattern:
883 | |                             try:
884 | |                                 target_pattern = ConflictPattern(pattern)
885 | |                                 detector = predictor.pattern_detectors.get(target_pattern)
886 | |                                 if detector:
887 | |                                     result = await detector(branch1, branch2, vector)
888 | |                                     if result:
889 | |                                         analysis_results[f"{branch1}:{branch2}"]["patterns"][pattern] = {  # type: ignore[index]
890 | |                                             "likelihood": result.likelihood_score,
891 | |                                             "confidence": result.confidence.value,
892 | |                                             "description": result.description,
893 | |                                         }
894 | |                                         self.print_info(f"   {pattern.replace('_', ' ').title()}: {result.likelihood_score:.1%} confiâ€¦
895 | |                             except ValueError:
896 | |                                 self.print_warning(f"   âŒ Unknown pattern: {pattern}")
897 | |
898 | |                 # Export results if requested
899 | |                 if export:
900 | |                     export_path = Path(export)
901 | |                     with export_path.open("w") as f:
902 | |                         json.dump(analysis_results, f, indent=2, default=str)
903 | |                     self.print_success(f"\nğŸ’¾ Analysis exported to: {export}")
904 | |
905 | |                 return {
906 | |                     "success": True,
907 | |                     "branches": branches,
908 | |                     "repo_path": repo_path,
909 | |                     "pattern": pattern,
910 | |                     "export": export,
911 | |                     "analysis_results": analysis_results,
912 | |                 }
913 | |
914 | |             return asyncio.run(run_analysis())
915 | |
916 | |         except Exception as e:
917 | |             msg = f"Error analyzing patterns: {e}"
918 | |             raise CommandError(msg) from e
    | |__________________________________________^ PLR1702
    |

commands/multi_agent_backup.py:850:23: ANN202 Missing return type annotation for private function `run_analysis`
    |
848 |             predictor = ConflictPredictor(conflict_engine, branch_manager, repo_path)
849 |
850 |             async def run_analysis():
    |                       ^^^^^^^^^^^^ ANN202
851 |                 analysis_results = {}
    |
    = help: Add return type annotation

commands/multi_agent_backup.py:924:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
922 |     """Analyze AST-based semantic conflicts between branches."""
923 |
924 |     def execute(self, **kwargs: Any) -> dict[str, object]:
    |                                 ^^^ ANN401
925 |         """Execute the command."""
926 |         # Extract parameters from kwargs
    |

commands/multi_agent_backup.py:951:23: ANN202 Missing return type annotation for private function `run_semantic_analysis`
    |
949 |             file_list = files.split(",") if files else None
950 |
951 |             async def run_semantic_analysis():
    |                       ^^^^^^^^^^^^^^^^^^^^^ ANN202
952 |                 all_conflicts = []
953 |                 if len(branches) >= MIN_BRANCHES_FOR_COMPARISON:
    |
    = help: Add return type annotation

commands/multi_agent_backup.py:1014:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
     |
1012 |     """Show semantic structure summary of code."""
1013 |
1014 |     def execute(self, **kwargs: Any) -> dict[str, object]:
     |                                 ^^^ ANN401
1015 |         """Execute the command."""
1016 |         # Extract parameters from kwargs
     |

commands/multi_agent_backup.py:1035:23: ANN202 Missing return type annotation for private function `run_summary`
     |
1033 |             file_list = files.split(",") if files else None
1034 |
1035 |             async def run_summary():
     |                       ^^^^^^^^^^^ ANN202
1036 |                 current_branch = branch or "HEAD"
1037 |                 # Get semantic context for files in the current branch
     |
     = help: Add return type annotation

commands/multi_agent_backup.py:1081:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
     |
1079 |     """Compare function signatures between branches."""
1080 |
1081 |     def execute(self, **kwargs: Any) -> dict[str, object]:
     |                                 ^^^ ANN401
1082 |         """Execute the command."""
1083 |         # Extract parameters from kwargs
     |

commands/multi_agent_backup.py:1103:23: ANN202 Missing return type annotation for private function `run_diff`
     |
1101 |             file_list = files.split(",") if files else None
1102 |
1103 |             async def run_diff():
     |                       ^^^^^^^^ ANN202
1104 |                 diff_results = []
1105 |                 if len(branches) >= MIN_BRANCHES_FOR_COMPARISON:
     |
     = help: Add return type annotation

commands/multi_agent_backup.py:1148:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
     |
1146 |     """Perform intelligent semantic merge."""
1147 |
1148 |     def execute(self, **kwargs: Any) -> dict[str, object]:
     |                                 ^^^ ANN401
1149 |         """Execute the command."""
1150 |         # Extract parameters from kwargs
     |

commands/multi_agent_backup.py:1171:23: ANN202 Missing return type annotation for private function `run_merge`
     |
1169 |             SemanticMerger(semantic_analyzer, conflict_engine, branch_manager, repo_path)
1170 |
1171 |             async def run_merge():
     |                       ^^^^^^^^^ ANN202
1172 |                 if dry_run:
1173 |                     self.print_info("ğŸ” Dry run mode - no changes will be made")
     |
     = help: Add return type annotation

commands/multi_agent_backup.py:1260:5: PLR0917 Too many positional arguments (8/5)
     |
1258 | @click.option("--timeout", "-t", default=300, help="Task timeout in seconds")
1259 | @click.option("--description", help="Task description")
1260 | def add_task(
     |     ^^^^^^^^ PLR0917
1261 |     title: str,
1262 |     command: tuple,
     |

commands/multi_agent_backup.py:1406:5: PLR0917 Too many positional arguments (6/5)
     |
1404 | @click.option("--export", "-e", help="Export results to JSON file")
1405 | @click.option("--detailed", "-d", is_flag=True, help="Show detailed analysis")
1406 | def analyze_semantic_conflicts(
     |     ^^^^^^^^^^^^^^^^^^^^^^^^^^ PLR0917
1407 |     branches: tuple,
1408 |     repo_path: str | None,
     |

commands/multi_agent_backup.py:1415:5: PLR1702 Too many nested blocks (6 > 5)
     |
1413 |   ) -> None:
1414 |       """Analyze AST-based semantic conflicts between branches."""
1415 | /     try:
1416 | |         click.echo(f"ğŸ§  Analyzing semantic conflicts between: {', '.join(branches)}")
1417 | |
1418 | |         if len(branches) < MIN_BRANCHES_FOR_COMPARISON:
1419 | |             click.echo("âŒ Need at least 2 branches for comparison")
1420 | |             return
1421 | |
1422 | |         # Create semantic analyzer
1423 | |         branch_manager = BranchManager(repo_path=repo_path)
1424 | |         analyzer = SemanticAnalyzer(branch_manager, repo_path)
1425 | |
1426 | |         # Configure analysis options
1427 | |         analyzer.check_private_members = include_private
1428 | |
1429 | |         # Parse file list
1430 | |         file_list = None
1431 | |         if files:
1432 | |             file_list = [f.strip() for f in files.split(",")]
1433 | |
1434 | |         async def run_semantic_analysis() -> None:
1435 | |             all_conflicts = []
1436 | |             analysis_results = {}
1437 | |
1438 | |             # Analyze all pairs of branches
1439 | |             for i, branch1 in enumerate(branches):
1440 | |                 for branch2 in branches[i + 1 :]:
1441 | |                     click.echo(f"\nğŸ”¬ Analyzing {branch1} â†” {branch2}")
1442 | |
1443 | |                     conflicts = await analyzer.analyze_semantic_conflicts(
1444 | |                         branch1,
1445 | |                         branch2,
1446 | |                         file_list,
1447 | |                     )
1448 | |
1449 | |                     pair_key = f"{branch1}:{branch2}"
1450 | |                     analysis_results[pair_key] = {
1451 | |                         "conflicts": len(conflicts),
1452 | |                         "details": [],
1453 | |                     }
1454 | |
1455 | |                     if not conflicts:
1456 | |                         click.echo("âœ… No semantic conflicts detected")
1457 | |                         continue
1458 | |
1459 | |                     click.echo(f"âš ï¸  Found {len(conflicts)} semantic conflicts:")
1460 | |                     click.echo("-" * 60)
1461 | |
1462 | |                     for j, conflict in enumerate(conflicts[:10], 1):  # Show top 10
1463 | |                         severity_icon = {
1464 | |                             "low": "ğŸŸ¢",
1465 | |                             "medium": "ğŸŸ¡",
1466 | |                             "high": "ğŸ”´",
1467 | |                             "critical": "ğŸ’€",
1468 | |                         }.get(conflict.severity.value, "â“")
1469 | |
1470 | |                         conflict_type_icon = {
1471 | |                             "function_signature_change": "ğŸ”§",
1472 | |                             "class_interface_change": "ğŸ—ï¸",
1473 | |                             "api_breaking_change": "ğŸ’¥",
1474 | |                             "inheritance_conflict": "ğŸ§¬",
1475 | |                             "import_semantic_conflict": "ğŸ“¦",
1476 | |                             "variable_type_conflict": "ğŸ”¤",
1477 | |                             "decorator_conflict": "âœ¨",
1478 | |                             "data_structure_change": "ğŸ“Š",
1479 | |                         }.get(conflict.conflict_type.value, "â“")
1480 | |
1481 | |                         click.echo(
1482 | |                             f"{j}. {severity_icon} {conflict_type_icon} {conflict.symbol_name}",
1483 | |                         )
1484 | |                         click.echo(
1485 | |                             f"   Type: {conflict.conflict_type.value.replace('_', ' ').title()}",
1486 | |                         )
1487 | |                         click.echo(f"   Severity: {conflict.severity.value.upper()}")
1488 | |                         click.echo(f"   File: {conflict.file_path}")
1489 | |                         click.echo(f"   Description: {conflict.description}")
1490 | |
1491 | |                         if detailed:
1492 | |                             if conflict.old_definition:
1493 | |                                 click.echo(f"   Old: {conflict.old_definition}")
1494 | |                             if conflict.new_definition:
1495 | |                                 click.echo(f"   New: {conflict.new_definition}")
1496 | |                             click.echo(
1497 | |                                 f"   Suggested Resolution: {conflict.suggested_resolution.value}",
1498 | |                             )
1499 | |
1500 | |                         click.echo()
1501 | |
1502 | |                         # Store for export
1503 | |                         analysis_results[pair_key]["details"].append(  # type: ignore[attr-defined]
1504 | |                             {
1505 | |                                 "conflict_id": conflict.conflict_id,
1506 | |                                 "type": conflict.conflict_type.value,
1507 | |                                 "severity": conflict.severity.value,
1508 | |                                 "symbol": conflict.symbol_name,
1509 | |                                 "file": conflict.file_path,
1510 | |                                 "description": conflict.description,
1511 | |                                 "old_definition": conflict.old_definition,
1512 | |                                 "new_definition": conflict.new_definition,
1513 | |                                 "suggested_resolution": conflict.suggested_resolution.value,
1514 | |                                 "metadata": conflict.metadata,
1515 | |                             },
1516 | |                         )
1517 | |
1518 | |                     if len(conflicts) > DEFAULT_DISPLAY_LIMIT_LARGE:
1519 | |                         click.echo(f"   ... and {len(conflicts) - 10} more conflicts")
1520 | |
1521 | |                     all_conflicts.extend(conflicts)
1522 | |
1523 | |             # Show summary
1524 | |             if all_conflicts:
1525 | |                 click.echo("\nğŸ“Š Analysis Summary:")
1526 | |                 click.echo(f"Total conflicts: {len(all_conflicts)}")
1527 | |
1528 | |                 # Group by type
1529 | |
1530 | |                 type_counts = Counter(c.conflict_type.value for c in all_conflicts)
1531 | |                 severity_counts = Counter(c.severity.value for c in all_conflicts)
1532 | |
1533 | |                 click.echo("\nğŸ·ï¸  By Type:")
1534 | |                 for conflict_type, count in type_counts.most_common():
1535 | |                     click.echo(
1536 | |                         f"  â€¢ {conflict_type.replace('_', ' ').title()}: {count}",
1537 | |                     )
1538 | |
1539 | |                 click.echo("\nâš¡ By Severity:")
1540 | |                 for severity, count in severity_counts.most_common():
1541 | |                     icon = {
1542 | |                         "critical": "ğŸ’€",
1543 | |                         "high": "ğŸ”´",
1544 | |                         "medium": "ğŸŸ¡",
1545 | |                         "low": "ğŸŸ¢",
1546 | |                     }.get(severity, "â“")
1547 | |                     click.echo(f"  {icon} {severity.capitalize()}: {count}")
1548 | |
1549 | |                 # Analysis performance
1550 | |                 summary = analyzer.get_analysis_summary()
1551 | |                 click.echo("\nğŸ“ˆ Performance:")
1552 | |                 click.echo(f"  Files analyzed: {summary['files_analyzed']}")
1553 | |                 click.echo(f"  Analysis time: {summary['analysis_time']:.2f}s")
1554 | |                 click.echo(f"  Cache hits: {summary['cache_hits']}")
1555 | |
1556 | |             # Export results
1557 | |             if export:
1558 | |                 export_data = {
1559 | |                     "branches": list(branches),
1560 | |                     "analysis_timestamp": datetime.now(UTC).isoformat(),
1561 | |                     "total_conflicts": len(all_conflicts),
1562 | |                     "branch_pairs": analysis_results,
1563 | |                     "performance_stats": analyzer.get_analysis_summary(),
1564 | |                 }
1565 | |
1566 | |                 export_path = Path(export)
1567 | |                 with export_path.open("w") as f:
1568 | |                     json.dump(export_data, f, indent=2, default=str)
1569 | |                 click.echo(f"\nğŸ’¾ Results exported to: {export}")
1570 | |
1571 | |         asyncio.run(run_semantic_analysis())
1572 | |
1573 | |     except (OSError, json.JSONEncodeError, RuntimeError, ValueError) as e:
1574 | |         click.echo(f"âŒ Error analyzing semantic conflicts: {e}", err=True)
     | |___________________________________________________________________________^ PLR1702
     |

commands/multi_agent_backup.py:1415:5: PLR1702 Too many nested blocks (6 > 5)
     |
1413 |   ) -> None:
1414 |       """Analyze AST-based semantic conflicts between branches."""
1415 | /     try:
1416 | |         click.echo(f"ğŸ§  Analyzing semantic conflicts between: {', '.join(branches)}")
1417 | |
1418 | |         if len(branches) < MIN_BRANCHES_FOR_COMPARISON:
1419 | |             click.echo("âŒ Need at least 2 branches for comparison")
1420 | |             return
1421 | |
1422 | |         # Create semantic analyzer
1423 | |         branch_manager = BranchManager(repo_path=repo_path)
1424 | |         analyzer = SemanticAnalyzer(branch_manager, repo_path)
1425 | |
1426 | |         # Configure analysis options
1427 | |         analyzer.check_private_members = include_private
1428 | |
1429 | |         # Parse file list
1430 | |         file_list = None
1431 | |         if files:
1432 | |             file_list = [f.strip() for f in files.split(",")]
1433 | |
1434 | |         async def run_semantic_analysis() -> None:
1435 | |             all_conflicts = []
1436 | |             analysis_results = {}
1437 | |
1438 | |             # Analyze all pairs of branches
1439 | |             for i, branch1 in enumerate(branches):
1440 | |                 for branch2 in branches[i + 1 :]:
1441 | |                     click.echo(f"\nğŸ”¬ Analyzing {branch1} â†” {branch2}")
1442 | |
1443 | |                     conflicts = await analyzer.analyze_semantic_conflicts(
1444 | |                         branch1,
1445 | |                         branch2,
1446 | |                         file_list,
1447 | |                     )
1448 | |
1449 | |                     pair_key = f"{branch1}:{branch2}"
1450 | |                     analysis_results[pair_key] = {
1451 | |                         "conflicts": len(conflicts),
1452 | |                         "details": [],
1453 | |                     }
1454 | |
1455 | |                     if not conflicts:
1456 | |                         click.echo("âœ… No semantic conflicts detected")
1457 | |                         continue
1458 | |
1459 | |                     click.echo(f"âš ï¸  Found {len(conflicts)} semantic conflicts:")
1460 | |                     click.echo("-" * 60)
1461 | |
1462 | |                     for j, conflict in enumerate(conflicts[:10], 1):  # Show top 10
1463 | |                         severity_icon = {
1464 | |                             "low": "ğŸŸ¢",
1465 | |                             "medium": "ğŸŸ¡",
1466 | |                             "high": "ğŸ”´",
1467 | |                             "critical": "ğŸ’€",
1468 | |                         }.get(conflict.severity.value, "â“")
1469 | |
1470 | |                         conflict_type_icon = {
1471 | |                             "function_signature_change": "ğŸ”§",
1472 | |                             "class_interface_change": "ğŸ—ï¸",
1473 | |                             "api_breaking_change": "ğŸ’¥",
1474 | |                             "inheritance_conflict": "ğŸ§¬",
1475 | |                             "import_semantic_conflict": "ğŸ“¦",
1476 | |                             "variable_type_conflict": "ğŸ”¤",
1477 | |                             "decorator_conflict": "âœ¨",
1478 | |                             "data_structure_change": "ğŸ“Š",
1479 | |                         }.get(conflict.conflict_type.value, "â“")
1480 | |
1481 | |                         click.echo(
1482 | |                             f"{j}. {severity_icon} {conflict_type_icon} {conflict.symbol_name}",
1483 | |                         )
1484 | |                         click.echo(
1485 | |                             f"   Type: {conflict.conflict_type.value.replace('_', ' ').title()}",
1486 | |                         )
1487 | |                         click.echo(f"   Severity: {conflict.severity.value.upper()}")
1488 | |                         click.echo(f"   File: {conflict.file_path}")
1489 | |                         click.echo(f"   Description: {conflict.description}")
1490 | |
1491 | |                         if detailed:
1492 | |                             if conflict.old_definition:
1493 | |                                 click.echo(f"   Old: {conflict.old_definition}")
1494 | |                             if conflict.new_definition:
1495 | |                                 click.echo(f"   New: {conflict.new_definition}")
1496 | |                             click.echo(
1497 | |                                 f"   Suggested Resolution: {conflict.suggested_resolution.value}",
1498 | |                             )
1499 | |
1500 | |                         click.echo()
1501 | |
1502 | |                         # Store for export
1503 | |                         analysis_results[pair_key]["details"].append(  # type: ignore[attr-defined]
1504 | |                             {
1505 | |                                 "conflict_id": conflict.conflict_id,
1506 | |                                 "type": conflict.conflict_type.value,
1507 | |                                 "severity": conflict.severity.value,
1508 | |                                 "symbol": conflict.symbol_name,
1509 | |                                 "file": conflict.file_path,
1510 | |                                 "description": conflict.description,
1511 | |                                 "old_definition": conflict.old_definition,
1512 | |                                 "new_definition": conflict.new_definition,
1513 | |                                 "suggested_resolution": conflict.suggested_resolution.value,
1514 | |                                 "metadata": conflict.metadata,
1515 | |                             },
1516 | |                         )
1517 | |
1518 | |                     if len(conflicts) > DEFAULT_DISPLAY_LIMIT_LARGE:
1519 | |                         click.echo(f"   ... and {len(conflicts) - 10} more conflicts")
1520 | |
1521 | |                     all_conflicts.extend(conflicts)
1522 | |
1523 | |             # Show summary
1524 | |             if all_conflicts:
1525 | |                 click.echo("\nğŸ“Š Analysis Summary:")
1526 | |                 click.echo(f"Total conflicts: {len(all_conflicts)}")
1527 | |
1528 | |                 # Group by type
1529 | |
1530 | |                 type_counts = Counter(c.conflict_type.value for c in all_conflicts)
1531 | |                 severity_counts = Counter(c.severity.value for c in all_conflicts)
1532 | |
1533 | |                 click.echo("\nğŸ·ï¸  By Type:")
1534 | |                 for conflict_type, count in type_counts.most_common():
1535 | |                     click.echo(
1536 | |                         f"  â€¢ {conflict_type.replace('_', ' ').title()}: {count}",
1537 | |                     )
1538 | |
1539 | |                 click.echo("\nâš¡ By Severity:")
1540 | |                 for severity, count in severity_counts.most_common():
1541 | |                     icon = {
1542 | |                         "critical": "ğŸ’€",
1543 | |                         "high": "ğŸ”´",
1544 | |                         "medium": "ğŸŸ¡",
1545 | |                         "low": "ğŸŸ¢",
1546 | |                     }.get(severity, "â“")
1547 | |                     click.echo(f"  {icon} {severity.capitalize()}: {count}")
1548 | |
1549 | |                 # Analysis performance
1550 | |                 summary = analyzer.get_analysis_summary()
1551 | |                 click.echo("\nğŸ“ˆ Performance:")
1552 | |                 click.echo(f"  Files analyzed: {summary['files_analyzed']}")
1553 | |                 click.echo(f"  Analysis time: {summary['analysis_time']:.2f}s")
1554 | |                 click.echo(f"  Cache hits: {summary['cache_hits']}")
1555 | |
1556 | |             # Export results
1557 | |             if export:
1558 | |                 export_data = {
1559 | |                     "branches": list(branches),
1560 | |                     "analysis_timestamp": datetime.now(UTC).isoformat(),
1561 | |                     "total_conflicts": len(all_conflicts),
1562 | |                     "branch_pairs": analysis_results,
1563 | |                     "performance_stats": analyzer.get_analysis_summary(),
1564 | |                 }
1565 | |
1566 | |                 export_path = Path(export)
1567 | |                 with export_path.open("w") as f:
1568 | |                     json.dump(export_data, f, indent=2, default=str)
1569 | |                 click.echo(f"\nğŸ’¾ Results exported to: {export}")
1570 | |
1571 | |         asyncio.run(run_semantic_analysis())
1572 | |
1573 | |     except (OSError, json.JSONEncodeError, RuntimeError, ValueError) as e:
1574 | |         click.echo(f"âŒ Error analyzing semantic conflicts: {e}", err=True)
     | |___________________________________________________________________________^ PLR1702
     |

commands/multi_agent_backup.py:1627:5: PLR0917 Too many positional arguments (8/5)
     |
1625 | @click.option("--apply", "-a", is_flag=True, help="Apply merge result to target branch")
1626 | @click.option("--export", "-e", help="Export merge result to file")
1627 | def semantic_merge(
     |     ^^^^^^^^^^^^^^ PLR0917
1628 |     file_path: str,  # noqa: ARG001
1629 |     branch1: str,
     |

commands/multi_agent_backup.py:1667:5: PLR0917 Too many positional arguments (9/5)
     |
1665 | @click.option("--apply", "-a", is_flag=True, help="Apply successful merge results")
1666 | @click.option("--export-summary", "-e", help="Export batch summary to JSON file")
1667 | def batch_merge(
     |     ^^^^^^^^^^^ PLR0917
1668 |     branch1: str,
1669 |     branch2: str,
     |

commands/multi_agent_backup.py:1854:5: PLR0917 Too many positional arguments (9/5)
     |
1852 |     help="Preview mode - show what would be resolved",
1853 | )
1854 | def auto_resolve(
     |     ^^^^^^^^^^^^ PLR0917
1855 |     branch1: str,
1856 |     branch2: str,
     |

commands/multi_agent_backup.py:2265:5: PLR0917 Too many positional arguments (6/5)
     |
2263 |     help="Enable auto-sync between agents",
2264 | )
2265 | def collaborate(
     |     ^^^^^^^^^^^ PLR0917
2266 |     agents: tuple,
2267 |     repo_path: str | None,
     |

commands/multi_agent_backup.py:2430:5: PLR0917 Too many positional arguments (7/5)
     |
2428 | )
2429 | @click.option("--repo-path", "-r", help="Path to git repository")
2430 | def send_message(
     |     ^^^^^^^^^^^^ PLR0917
2431 |     sender: str,
2432 |     recipient: str | None,
     |

commands/multi_agent_backup.py:2504:5: PLR0917 Too many positional arguments (6/5)
     |
2502 | )
2503 | @click.option("--repo-path", help="Path to git repository")
2504 | def share_knowledge(
     |     ^^^^^^^^^^^^^^^ PLR0917
2505 |     agent: str,
2506 |     type: str,
     |

commands/multi_agent_backup.py:2579:5: PLR0917 Too many positional arguments (7/5)
     |
2577 |     help="Synchronization strategy",
2578 | )
2579 | def branch_info(
     |     ^^^^^^^^^^^ PLR0917
2580 |     action: str,
2581 |     branch: str | None,
     |

commands/multi_agent_backup.py:2730:5: PLR0917 Too many positional arguments (7/5)
     |
2728 | )
2729 | @click.option("--repo-path", "-r", help="Path to git repository")
2730 | def dependency_track(
     |     ^^^^^^^^^^^^^^^^ PLR0917
2731 |     file: str,
2732 |     agent: str,
     |

commands/multi_agent_backup.py:3079:5: PLR0917 Too many positional arguments (7/5)
     |
3077 | )
3078 | @click.option("--repo-path", help="Path to git repository")
3079 | def review_initiate(
     |     ^^^^^^^^^^^^^^^ PLR0917
3080 |     branch_name: str,
3081 |     agent: str,
     |

commands/multi_agent_backup.py:3351:5: PLR1702 Too many nested blocks (6 > 5)
     |
3349 |   def review_status(review_id: str | None, repo_path: str | None, detailed: bool) -> None:  # noqa: FBT001
3350 |       """Get status of a specific review or all reviews."""
3351 | /     try:
3352 | |         if review_id:
3353 | |             click.echo(f"ğŸ“Š Code Review Status: {review_id}")
3354 | |         else:
3355 | |             click.echo("ğŸ“Š All Code Reviews Status")
3356 | |
3357 | |         async def get_status() -> None:
3358 | |             try:
3359 | |                 # Create mock components
3360 | |                 branch_manager = BranchManager(repo_path=repo_path)
3361 | |                 collab_engine = Mock()
3362 | |                 semantic_analyzer = Mock()
3363 | |
3364 | |                 review_engine = CodeReviewEngine(
3365 | |                     collaboration_engine=collab_engine,
3366 | |                     semantic_analyzer=semantic_analyzer,
3367 | |                     branch_manager=branch_manager,
3368 | |                     repo_path=repo_path,
3369 | |                 )
3370 | |
3371 | |                 await review_engine.start()
3372 | |
3373 | |                 if review_id:
3374 | |                     # Get specific review status
3375 | |                     review = await review_engine.get_review_status(review_id)
3376 | |                     if review:
3377 | |                         click.echo("\nğŸ“‹ Review Details:")
3378 | |                         click.echo(f"   Branch: {review.branch_name}")
3379 | |                         click.echo(f"   Requester: {review.agent_id}")
3380 | |                         click.echo(f"   Status: {review.status.value}")
3381 | |                         click.echo(f"   Overall score: {review.overall_score:.1f}/10")
3382 | |                         click.echo(f"   Auto-approved: {review.auto_approved}")
3383 | |
3384 | |                         if review.reviewer_ids:
3385 | |                             click.echo(
3386 | |                                 f"   Reviewers: {', '.join(review.reviewer_ids)}",
3387 | |                             )
3388 | |
3389 | |                         click.echo(f"   Files: {len(review.files_changed)}")
3390 | |                         if detailed:
3391 | |                             for file in review.files_changed:
3392 | |                                 click.echo(f"     â€¢ {file}")
3393 | |
3394 | |                         click.echo(f"   Findings: {len(review.findings)}")
3395 | |                         if detailed and review.findings:
3396 | |                             # Group findings by severity
3397 | |
3398 | |                             severity_counts = Counter(f.severity.value for f in review.findings)
3399 | |                             for severity, count in severity_counts.items():
3400 | |                                 severity_icon = {
3401 | |                                     "critical": "ğŸ’€",
3402 | |                                     "high": "ğŸ”´",
3403 | |                                     "medium": "ğŸŸ¡",
3404 | |                                     "low": "ğŸŸ¢",
3405 | |                                     "info": "â„¹ï¸",
3406 | |                                 }.get(severity, "â“")
3407 | |                                 click.echo(f"     {severity_icon} {severity}: {count}")
3408 | |
3409 | |                         if review.quality_metrics:
3410 | |                             click.echo(
3411 | |                                 f"   Quality metrics: {len(review.quality_metrics)}",
3412 | |                             )
3413 | |                             if detailed:
3414 | |                                 for metric in review.quality_metrics:
3415 | |                                     click.echo(f"     â€¢ {metric.file_path}")
3416 | |                                     if metric.violations:
3417 | |                                         for violation in metric.violations:
3418 | |                                             click.echo(f"       âš ï¸ {violation.value}")
3419 | |
3420 | |                     else:
3421 | |                         click.echo(f"\nâŒ Review {review_id} not found")
3422 | |
3423 | |                 else:
3424 | |                     # Get review summary
3425 | |                     summary = review_engine.get_review_summary()
3426 | |                     click.echo("\nğŸ“Š Review Summary:")
3427 | |                     click.echo(f"   Total reviews: {summary.total_reviews}")
3428 | |                     click.echo(f"   Approved: {summary.approved_reviews}")
3429 | |                     click.echo(f"   Rejected: {summary.rejected_reviews}")
3430 | |                     click.echo(f"   Pending: {summary.pending_reviews}")
3431 | |
3432 | |                     if summary.total_reviews > 0:
3433 | |                         click.echo(f"   Average score: {summary.average_score:.1f}/10")
3434 | |
3435 | |                     click.echo(f"   Total findings: {summary.total_findings}")
3436 | |                     click.echo(f"   Critical findings: {summary.critical_findings}")
3437 | |
3438 | |                     if summary.most_common_issues:
3439 | |                         click.echo("\nğŸ” Most Common Issues:")
3440 | |                         for issue, count in summary.most_common_issues[:5]:
3441 | |                             click.echo(f"   â€¢ {issue}: {count}")
3442 | |
3443 | |                     if summary.review_time_stats and "average_time" in summary.review_time_stats:
3444 | |                         avg_time = summary.review_time_stats["average_time"]
3445 | |                         click.echo(f"   Average review time: {avg_time:.1f}s")
3446 | |
3447 | |                 await review_engine.stop()
3448 | |
3449 | |             except Exception as e:
3450 | |                 click.echo(f"   âŒ Error getting status: {e}")
3451 | |
3452 | |         try:
3453 | |             asyncio.run(get_status())
3454 | |         except (TimeoutError, RuntimeError, OSError, AttributeError) as e:
3455 | |             # Fallback to demo output
3456 | |             if review_id:
3457 | |                 click.echo("\nğŸ“‹ Review Details:")
3458 | |                 click.echo("   Status: pending")
3459 | |                 click.echo("   Overall score: 0.0/10")
3460 | |                 click.echo("   (Demo mode - actual status not available)")
3461 | |             else:
3462 | |                 click.echo("\nğŸ“Š Review Summary:")
3463 | |                 click.echo("   Total reviews: 0")
3464 | |                 click.echo("   (Demo mode - actual summary not available)")
3465 | |
3466 | |     except Exception as e:
3467 | |         click.echo(f"âŒ Error getting review status: {e}", err=True)
     | |____________________________________________________________________^ PLR1702
     |

commands/multi_agent_backup.py:3351:5: PLR1702 Too many nested blocks (6 > 5)
     |
3349 |   def review_status(review_id: str | None, repo_path: str | None, detailed: bool) -> None:  # noqa: FBT001
3350 |       """Get status of a specific review or all reviews."""
3351 | /     try:
3352 | |         if review_id:
3353 | |             click.echo(f"ğŸ“Š Code Review Status: {review_id}")
3354 | |         else:
3355 | |             click.echo("ğŸ“Š All Code Reviews Status")
3356 | |
3357 | |         async def get_status() -> None:
3358 | |             try:
3359 | |                 # Create mock components
3360 | |                 branch_manager = BranchManager(repo_path=repo_path)
3361 | |                 collab_engine = Mock()
3362 | |                 semantic_analyzer = Mock()
3363 | |
3364 | |                 review_engine = CodeReviewEngine(
3365 | |                     collaboration_engine=collab_engine,
3366 | |                     semantic_analyzer=semantic_analyzer,
3367 | |                     branch_manager=branch_manager,
3368 | |                     repo_path=repo_path,
3369 | |                 )
3370 | |
3371 | |                 await review_engine.start()
3372 | |
3373 | |                 if review_id:
3374 | |                     # Get specific review status
3375 | |                     review = await review_engine.get_review_status(review_id)
3376 | |                     if review:
3377 | |                         click.echo("\nğŸ“‹ Review Details:")
3378 | |                         click.echo(f"   Branch: {review.branch_name}")
3379 | |                         click.echo(f"   Requester: {review.agent_id}")
3380 | |                         click.echo(f"   Status: {review.status.value}")
3381 | |                         click.echo(f"   Overall score: {review.overall_score:.1f}/10")
3382 | |                         click.echo(f"   Auto-approved: {review.auto_approved}")
3383 | |
3384 | |                         if review.reviewer_ids:
3385 | |                             click.echo(
3386 | |                                 f"   Reviewers: {', '.join(review.reviewer_ids)}",
3387 | |                             )
3388 | |
3389 | |                         click.echo(f"   Files: {len(review.files_changed)}")
3390 | |                         if detailed:
3391 | |                             for file in review.files_changed:
3392 | |                                 click.echo(f"     â€¢ {file}")
3393 | |
3394 | |                         click.echo(f"   Findings: {len(review.findings)}")
3395 | |                         if detailed and review.findings:
3396 | |                             # Group findings by severity
3397 | |
3398 | |                             severity_counts = Counter(f.severity.value for f in review.findings)
3399 | |                             for severity, count in severity_counts.items():
3400 | |                                 severity_icon = {
3401 | |                                     "critical": "ğŸ’€",
3402 | |                                     "high": "ğŸ”´",
3403 | |                                     "medium": "ğŸŸ¡",
3404 | |                                     "low": "ğŸŸ¢",
3405 | |                                     "info": "â„¹ï¸",
3406 | |                                 }.get(severity, "â“")
3407 | |                                 click.echo(f"     {severity_icon} {severity}: {count}")
3408 | |
3409 | |                         if review.quality_metrics:
3410 | |                             click.echo(
3411 | |                                 f"   Quality metrics: {len(review.quality_metrics)}",
3412 | |                             )
3413 | |                             if detailed:
3414 | |                                 for metric in review.quality_metrics:
3415 | |                                     click.echo(f"     â€¢ {metric.file_path}")
3416 | |                                     if metric.violations:
3417 | |                                         for violation in metric.violations:
3418 | |                                             click.echo(f"       âš ï¸ {violation.value}")
3419 | |
3420 | |                     else:
3421 | |                         click.echo(f"\nâŒ Review {review_id} not found")
3422 | |
3423 | |                 else:
3424 | |                     # Get review summary
3425 | |                     summary = review_engine.get_review_summary()
3426 | |                     click.echo("\nğŸ“Š Review Summary:")
3427 | |                     click.echo(f"   Total reviews: {summary.total_reviews}")
3428 | |                     click.echo(f"   Approved: {summary.approved_reviews}")
3429 | |                     click.echo(f"   Rejected: {summary.rejected_reviews}")
3430 | |                     click.echo(f"   Pending: {summary.pending_reviews}")
3431 | |
3432 | |                     if summary.total_reviews > 0:
3433 | |                         click.echo(f"   Average score: {summary.average_score:.1f}/10")
3434 | |
3435 | |                     click.echo(f"   Total findings: {summary.total_findings}")
3436 | |                     click.echo(f"   Critical findings: {summary.critical_findings}")
3437 | |
3438 | |                     if summary.most_common_issues:
3439 | |                         click.echo("\nğŸ” Most Common Issues:")
3440 | |                         for issue, count in summary.most_common_issues[:5]:
3441 | |                             click.echo(f"   â€¢ {issue}: {count}")
3442 | |
3443 | |                     if summary.review_time_stats and "average_time" in summary.review_time_stats:
3444 | |                         avg_time = summary.review_time_stats["average_time"]
3445 | |                         click.echo(f"   Average review time: {avg_time:.1f}s")
3446 | |
3447 | |                 await review_engine.stop()
3448 | |
3449 | |             except Exception as e:
3450 | |                 click.echo(f"   âŒ Error getting status: {e}")
3451 | |
3452 | |         try:
3453 | |             asyncio.run(get_status())
3454 | |         except (TimeoutError, RuntimeError, OSError, AttributeError) as e:
3455 | |             # Fallback to demo output
3456 | |             if review_id:
3457 | |                 click.echo("\nğŸ“‹ Review Details:")
3458 | |                 click.echo("   Status: pending")
3459 | |                 click.echo("   Overall score: 0.0/10")
3460 | |                 click.echo("   (Demo mode - actual status not available)")
3461 | |             else:
3462 | |                 click.echo("\nğŸ“Š Review Summary:")
3463 | |                 click.echo("   Total reviews: 0")
3464 | |                 click.echo("   (Demo mode - actual summary not available)")
3465 | |
3466 | |     except Exception as e:
3467 | |         click.echo(f"âŒ Error getting review status: {e}", err=True)
     | |____________________________________________________________________^ PLR1702
     |

commands/multi_agent_backup.py:3351:5: PLR1702 Too many nested blocks (9 > 5)
     |
3349 |   def review_status(review_id: str | None, repo_path: str | None, detailed: bool) -> None:  # noqa: FBT001
3350 |       """Get status of a specific review or all reviews."""
3351 | /     try:
3352 | |         if review_id:
3353 | |             click.echo(f"ğŸ“Š Code Review Status: {review_id}")
3354 | |         else:
3355 | |             click.echo("ğŸ“Š All Code Reviews Status")
3356 | |
3357 | |         async def get_status() -> None:
3358 | |             try:
3359 | |                 # Create mock components
3360 | |                 branch_manager = BranchManager(repo_path=repo_path)
3361 | |                 collab_engine = Mock()
3362 | |                 semantic_analyzer = Mock()
3363 | |
3364 | |                 review_engine = CodeReviewEngine(
3365 | |                     collaboration_engine=collab_engine,
3366 | |                     semantic_analyzer=semantic_analyzer,
3367 | |                     branch_manager=branch_manager,
3368 | |                     repo_path=repo_path,
3369 | |                 )
3370 | |
3371 | |                 await review_engine.start()
3372 | |
3373 | |                 if review_id:
3374 | |                     # Get specific review status
3375 | |                     review = await review_engine.get_review_status(review_id)
3376 | |                     if review:
3377 | |                         click.echo("\nğŸ“‹ Review Details:")
3378 | |                         click.echo(f"   Branch: {review.branch_name}")
3379 | |                         click.echo(f"   Requester: {review.agent_id}")
3380 | |                         click.echo(f"   Status: {review.status.value}")
3381 | |                         click.echo(f"   Overall score: {review.overall_score:.1f}/10")
3382 | |                         click.echo(f"   Auto-approved: {review.auto_approved}")
3383 | |
3384 | |                         if review.reviewer_ids:
3385 | |                             click.echo(
3386 | |                                 f"   Reviewers: {', '.join(review.reviewer_ids)}",
3387 | |                             )
3388 | |
3389 | |                         click.echo(f"   Files: {len(review.files_changed)}")
3390 | |                         if detailed:
3391 | |                             for file in review.files_changed:
3392 | |                                 click.echo(f"     â€¢ {file}")
3393 | |
3394 | |                         click.echo(f"   Findings: {len(review.findings)}")
3395 | |                         if detailed and review.findings:
3396 | |                             # Group findings by severity
3397 | |
3398 | |                             severity_counts = Counter(f.severity.value for f in review.findings)
3399 | |                             for severity, count in severity_counts.items():
3400 | |                                 severity_icon = {
3401 | |                                     "critical": "ğŸ’€",
3402 | |                                     "high": "ğŸ”´",
3403 | |                                     "medium": "ğŸŸ¡",
3404 | |                                     "low": "ğŸŸ¢",
3405 | |                                     "info": "â„¹ï¸",
3406 | |                                 }.get(severity, "â“")
3407 | |                                 click.echo(f"     {severity_icon} {severity}: {count}")
3408 | |
3409 | |                         if review.quality_metrics:
3410 | |                             click.echo(
3411 | |                                 f"   Quality metrics: {len(review.quality_metrics)}",
3412 | |                             )
3413 | |                             if detailed:
3414 | |                                 for metric in review.quality_metrics:
3415 | |                                     click.echo(f"     â€¢ {metric.file_path}")
3416 | |                                     if metric.violations:
3417 | |                                         for violation in metric.violations:
3418 | |                                             click.echo(f"       âš ï¸ {violation.value}")
3419 | |
3420 | |                     else:
3421 | |                         click.echo(f"\nâŒ Review {review_id} not found")
3422 | |
3423 | |                 else:
3424 | |                     # Get review summary
3425 | |                     summary = review_engine.get_review_summary()
3426 | |                     click.echo("\nğŸ“Š Review Summary:")
3427 | |                     click.echo(f"   Total reviews: {summary.total_reviews}")
3428 | |                     click.echo(f"   Approved: {summary.approved_reviews}")
3429 | |                     click.echo(f"   Rejected: {summary.rejected_reviews}")
3430 | |                     click.echo(f"   Pending: {summary.pending_reviews}")
3431 | |
3432 | |                     if summary.total_reviews > 0:
3433 | |                         click.echo(f"   Average score: {summary.average_score:.1f}/10")
3434 | |
3435 | |                     click.echo(f"   Total findings: {summary.total_findings}")
3436 | |                     click.echo(f"   Critical findings: {summary.critical_findings}")
3437 | |
3438 | |                     if summary.most_common_issues:
3439 | |                         click.echo("\nğŸ” Most Common Issues:")
3440 | |                         for issue, count in summary.most_common_issues[:5]:
3441 | |                             click.echo(f"   â€¢ {issue}: {count}")
3442 | |
3443 | |                     if summary.review_time_stats and "average_time" in summary.review_time_stats:
3444 | |                         avg_time = summary.review_time_stats["average_time"]
3445 | |                         click.echo(f"   Average review time: {avg_time:.1f}s")
3446 | |
3447 | |                 await review_engine.stop()
3448 | |
3449 | |             except Exception as e:
3450 | |                 click.echo(f"   âŒ Error getting status: {e}")
3451 | |
3452 | |         try:
3453 | |             asyncio.run(get_status())
3454 | |         except (TimeoutError, RuntimeError, OSError, AttributeError) as e:
3455 | |             # Fallback to demo output
3456 | |             if review_id:
3457 | |                 click.echo("\nğŸ“‹ Review Details:")
3458 | |                 click.echo("   Status: pending")
3459 | |                 click.echo("   Overall score: 0.0/10")
3460 | |                 click.echo("   (Demo mode - actual status not available)")
3461 | |             else:
3462 | |                 click.echo("\nğŸ“Š Review Summary:")
3463 | |                 click.echo("   Total reviews: 0")
3464 | |                 click.echo("   (Demo mode - actual summary not available)")
3465 | |
3466 | |     except Exception as e:
3467 | |         click.echo(f"âŒ Error getting review status: {e}", err=True)
     | |____________________________________________________________________^ PLR1702
     |

commands/setup.py:25:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
23 |     """Create all tmux sessions defined in projects.yaml."""
24 |
25 |     def execute(self, **kwargs: Any) -> dict:
   |                                 ^^^ ANN401
26 |         """Execute the setup command.
   |

commands/show.py:15:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
13 |     """List all running tmux sessions."""
14 |
15 |     def execute(self, **kwargs: Any) -> dict:  # noqa: ARG002
   |                                 ^^^ ANN401
16 |         """Execute the show command.
   |

commands/status.py:281:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
279 |         update_interval: float = 5.0,
280 |         detailed: bool = False,  # noqa: FBT001
281 |         **kwargs: Any,  # noqa: ARG002
    |                   ^^^ ANN401
282 |     ) -> dict:
283 |         """Execute the status command."""
    |

commands/status.py:327:13: TRY300 Consider moving this statement to an `else` block
    |
326 |             console.print("\nğŸ’¡ Use --interactive for live dashboard or --detailed for full view")
327 |             return {"success": True, "mode": "quick", "project_path": project_path}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
328 |
329 |         except KeyboardInterrupt:
    |

commands/status_async.py:27:1: SyntaxError: Unexpected indentation
   |
27 |     ActivityHeatmapGenerator,
   | ^^^^
28 |     GitActivityWidget,
29 |     ProgressTracker,
   |

commands/status_async.py:27:1: E113 Unexpected indentation
   |
27 |     ActivityHeatmapGenerator,
   | ^^^^ E113
28 |     GitActivityWidget,
29 |     ProgressTracker,
   |

commands/status_async.py:27:5: E303 Too many blank lines (3)
   |
27 |     ActivityHeatmapGenerator,
   |     ^^^^^^^^^^^^^^^^^^^^^^^^ E303
28 |     GitActivityWidget,
29 |     ProgressTracker,
   |
   = help: Remove extraneous blank line(s)

commands/status_async.py:32:1: SyntaxError: Expected a statement
   |
30 |     ProjectHealth,
31 |     SessionBrowser,
32 | )
   | ^
   |

commands/status_async.py:32:2: SyntaxError: Expected a statement
   |
30 |     ProjectHealth,
31 |     SessionBrowser,
32 | )
   |  ^
33 |
34 |
35 | class AsyncStatusDashboard:
   |

commands/status_refactored.py:35:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
33 |         }
34 |
35 |     def execute(self, **kwargs: Any) -> dict[str, object]:
   |                                 ^^^ ANN401
36 |         """Execute the status command.
   |

commands/status_refactored.py:56:13: TRY300 Consider moving this statement to an `else` block
   |
55 |             self.update_status("idle")
56 |             return result
   |             ^^^^^^^^^^^^^ TRY300
57 |
58 |         except Exception as e:
   |

commands/task_runner.py:35:86: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
33 |     """Process the next available task."""
34 |
35 |     def execute(self, directory: str | None = None, verbose: bool = False, **kwargs: Any) -> dict:  # noqa: FBT001, ARG002
   |                                                                                      ^^^ ANN401
36 |         """Execute the next command.
   |

commands/task_runner.py:49:13: TRY300 Consider moving this statement to an `else` block
   |
47 |                 return {"success": True, "task_processed": True}
48 |             self.print_info("â„¹ï¸ No more tasks to process")
49 |             return {"success": True, "task_processed": False}
   |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
50 |
51 |         except Exception as e:
   |

commands/task_runner.py:64:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
62 |         max_iterations: int = 100,
63 |         dry_run: bool = False,  # noqa: FBT001
64 |         **kwargs: Any,  # noqa: ARG002
   |                   ^^^ ANN401
65 |     ) -> dict:
66 |         """Execute the run command.
   |

commands/task_runner.py:108:13: TRY300 Consider moving this statement to an `else` block
    |
106 |                   progress.update(task_id, description="âœ… Task processing completed")
107 |
108 | /             return {
109 | |                 "success": True,
110 | |                 "directory": directory,
111 | |                 "max_iterations": max_iterations,
112 | |             }
    | |_____________^ TRY300
113 |
114 |           except KeyboardInterrupt:
    |

commands/task_runner.py:125:9: PLR0914 Too many local variables (17/15)
    |
123 |     """Show current task status."""
124 |
125 |     def execute(self, directory: str | None = None, detailed: bool = False, **kwargs: Any) -> dict:  # noqa: FBT001, ARG002
    |         ^^^^^^^ PLR0914
126 |         """Execute the status command.
    |

commands/task_runner.py:125:87: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
123 |     """Show current task status."""
124 |
125 |     def execute(self, directory: str | None = None, detailed: bool = False, **kwargs: Any) -> dict:  # noqa: FBT001, ARG002
    |                                                                                       ^^^ ANN401
126 |         """Execute the status command.
    |

commands/task_runner.py:194:13: TRY300 Consider moving this statement to an `else` block
    |
192 |                   self.print_success("\nğŸ‰ All tasks completed!")
193 |
194 | /             return {
195 | |                 "success": True,
196 | |                 "total_files": total_files,
197 | |                 "completed_files": completed_files,
198 | |                 "total_tasks": total_tasks,
199 | |                 "completed_tasks": completed_tasks,
200 | |                 "skipped_tasks": skipped_tasks,
201 | |             }
    | |_____________^ TRY300
202 |
203 |           except Exception as e:
    |

commands/task_runner.py:211:88: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
209 |     """Add a new task to a todo file."""
210 |
211 |     def execute(self, task: str | None = None, file_path: str | None = None, **kwargs: Any) -> dict:  # noqa: ARG002
    |                                                                                        ^^^ ANN401
212 |         """Execute the add command.
    |

commands/task_runner.py:229:17: TRY301 Abstract `raise` to an inner function
    |
227 |             if not resolved_path.exists():
228 |                 msg = f"File not found: {resolved_path}"
229 |                 raise CommandError(msg)
    |                 ^^^^^^^^^^^^^^^^^^^^^^^ TRY301
230 |
231 |             # Add task to file
    |

commands/teardown.py:17:66: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
15 |     """Kill all tmux sessions (ê¸°ë³¸) ë˜ëŠ” ì§€ì •í•œ ì„¸ì…˜ë§Œ ì‚­ì œí•©ë‹ˆë‹¤."""
16 |
17 |     def execute(self, session_name: str | None = None, **kwargs: Any) -> dict:  # noqa: ARG002
   |                                                                  ^^^ ANN401
18 |         """Execute the teardown command.
   |

commands/validate.py:25:9: PLR0914 Too many local variables (19/15)
   |
23 |     """Check if all directories in projects.yaml exist (or only for a specific session)."""
24 |
25 |     def execute(self, session_name: str | None = None, format: str = "table", **kwargs: Any) -> dict:  # noqa: ARG002
   |         ^^^^^^^ PLR0914
26 |         """Execute the validate command.
   |

commands/validate.py:25:89: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
23 |     """Check if all directories in projects.yaml exist (or only for a specific session)."""
24 |
25 |     def execute(self, session_name: str | None = None, format: str = "table", **kwargs: Any) -> dict:  # noqa: ARG002
   |                                                                                         ^^^ ANN401
26 |         """Execute the validate command.
   |

commands/validate.py:31:9: PLR1702 Too many nested blocks (6 > 5)
    |
 29 |           dict: Description of return value.
 30 |           """
 31 | /         try:
 32 | |             console = Console()
 33 | |             sessions = self.tmux_manager.load_projects().get("sessions", {})
 34 | |
 35 | |             if not sessions:
 36 | |                 console.print("[red]âŒ No sessions defined in projects.yaml[/red]")
 37 | |                 return {"success": False, "error": "no_sessions_defined"}
 38 | |
 39 | |             if session_name:
 40 | |                 if session_name not in sessions:
 41 | |                     console.print(f"[red]âŒ Session '{session_name}' not defined in projects.yaml[/red]")
 42 | |                     return {"success": False, "error": "session_not_defined"}
 43 | |                 sessions = {session_name: sessions[session_name]}
 44 | |
 45 | |             # Show progress for multiple sessions
 46 | |             if len(sessions) > 1:
 47 | |                 console.print(f"[blue]ğŸ” Validating {len(sessions)} sessions...[/blue]\n")
 48 | |
 49 | |             missing = []
 50 | |             valid_count = 0
 51 | |
 52 | |             # Process sessions with progress tracking for multiple sessions
 53 | |             sessions_to_process = list(sessions.items())
 54 | |             iterator = track(sessions_to_process, description="Validating sessions...") if len(sessions) > 3 else sessions_to_process
 55 | |
 56 | |             for s_name, sess_conf in iterator:
 57 | |                 try:
 58 | |                     # í…œí”Œë¦¿ì´ ì ìš©ëœ ìµœì¢… ì„¤ì •ì„ ê°€ì ¸ì˜¤ê¸°
 59 | |                     final_config = self.tmux_manager.get_session_config(s_name, sess_conf)
 60 | |                     session_missing = []
 61 | |
 62 | |                     # ì„¸ì…˜ ì‹œì‘ ë””ë ‰í† ë¦¬ ê²€ì‚¬
 63 | |                     start_dir = final_config.get("start_directory", os.getcwd())
 64 | |                     expanded_dir = os.path.expanduser(start_dir)
 65 | |                     if not os.path.exists(expanded_dir):
 66 | |                         session_missing.append(("session", "Session Root", expanded_dir))
 67 | |
 68 | |                     # ìœˆë„ìš°ë³„ start_directory ê²€ì‚¬
 69 | |                     windows = final_config.get("windows", [])
 70 | |                     for i, window in enumerate(windows):
 71 | |                         window_start_dir = window.get("start_directory")
 72 | |                         if window_start_dir:
 73 | |                             if not os.path.isabs(window_start_dir):
 74 | |                                 # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì„¸ì…˜ì˜ ì‹œì‘ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•¨
 75 | |                                 base_dir = expanded_dir
 76 | |                                 window_start_dir = os.path.join(base_dir, window_start_dir)
 77 | |                             expanded_window_dir = os.path.expanduser(window_start_dir)
 78 | |                             if not os.path.exists(expanded_window_dir):
 79 | |                                 window_name = window.get("window_name", f"window_{i}")
 80 | |                                 session_missing.append(("window", window_name, expanded_window_dir))
 81 | |
 82 | |                         # íŒ¬ë³„ start_directory ê²€ì‚¬ (íŒ¬ì´ ìˆëŠ” ê²½ìš°)
 83 | |                         panes = window.get("panes", [])
 84 | |                         for j, pane in enumerate(panes):
 85 | |                             if isinstance(pane, dict) and "start_directory" in pane:
 86 | |                                 pane_start_dir = pane["start_directory"]
 87 | |                                 if not os.path.isabs(pane_start_dir):
 88 | |                                     base_dir = expanded_dir
 89 | |                                     pane_start_dir = os.path.join(base_dir, pane_start_dir)
 90 | |                                 expanded_pane_dir = os.path.expanduser(pane_start_dir)
 91 | |                                 if not os.path.exists(expanded_pane_dir):
 92 | |                                     window_name = window.get("window_name", f"window_{i}")
 93 | |                                     session_missing.append(
 94 | |                                         (
 95 | |                                             "pane",
 96 | |                                             f"{window_name}/pane_{j}",
 97 | |                                             expanded_pane_dir,
 98 | |                                         )
 99 | |                                     )
100 | |
101 | |                     # Store results for this session
102 | |                     if session_missing:
103 | |                         missing.append((s_name, session_missing))
104 | |                     else:
105 | |                         valid_count += 1
106 | |
107 | |                 except Exception as e:
108 | |                     console.print(f"[yellow]âš ï¸  Error processing session '{s_name}': {e}[/yellow]")
109 | |                     continue
110 | |
111 | |             # Display results based on format
112 | |             if not missing:
113 | |                 _display_success(console, valid_count, len(sessions))
114 | |             elif format == "table":
115 | |                 _display_table_format(console, missing, valid_count, len(sessions))
116 | |             elif format == "tree":
117 | |                 _display_tree_format(console, missing, valid_count, len(sessions))
118 | |             else:
119 | |                 _display_simple_format(console, missing, valid_count, len(sessions))
120 | |
121 | |             return {
122 | |                 "success": True,
123 | |                 "total_sessions": len(sessions),
124 | |                 "valid_sessions": valid_count,
125 | |                 "invalid_sessions": len(missing),
126 | |                 "missing_directories": missing,
127 | |             }
128 | |
129 | |         except Exception as e:
130 | |             msg = f"Error validating directories: {e}"
131 | |             raise CommandError(msg) from e
    | |__________________________________________^ PLR1702
    |

commands/validate.py:31:9: PLR1702 Too many nested blocks (6 > 5)
    |
 29 |           dict: Description of return value.
 30 |           """
 31 | /         try:
 32 | |             console = Console()
 33 | |             sessions = self.tmux_manager.load_projects().get("sessions", {})
 34 | |
 35 | |             if not sessions:
 36 | |                 console.print("[red]âŒ No sessions defined in projects.yaml[/red]")
 37 | |                 return {"success": False, "error": "no_sessions_defined"}
 38 | |
 39 | |             if session_name:
 40 | |                 if session_name not in sessions:
 41 | |                     console.print(f"[red]âŒ Session '{session_name}' not defined in projects.yaml[/red]")
 42 | |                     return {"success": False, "error": "session_not_defined"}
 43 | |                 sessions = {session_name: sessions[session_name]}
 44 | |
 45 | |             # Show progress for multiple sessions
 46 | |             if len(sessions) > 1:
 47 | |                 console.print(f"[blue]ğŸ” Validating {len(sessions)} sessions...[/blue]\n")
 48 | |
 49 | |             missing = []
 50 | |             valid_count = 0
 51 | |
 52 | |             # Process sessions with progress tracking for multiple sessions
 53 | |             sessions_to_process = list(sessions.items())
 54 | |             iterator = track(sessions_to_process, description="Validating sessions...") if len(sessions) > 3 else sessions_to_process
 55 | |
 56 | |             for s_name, sess_conf in iterator:
 57 | |                 try:
 58 | |                     # í…œí”Œë¦¿ì´ ì ìš©ëœ ìµœì¢… ì„¤ì •ì„ ê°€ì ¸ì˜¤ê¸°
 59 | |                     final_config = self.tmux_manager.get_session_config(s_name, sess_conf)
 60 | |                     session_missing = []
 61 | |
 62 | |                     # ì„¸ì…˜ ì‹œì‘ ë””ë ‰í† ë¦¬ ê²€ì‚¬
 63 | |                     start_dir = final_config.get("start_directory", os.getcwd())
 64 | |                     expanded_dir = os.path.expanduser(start_dir)
 65 | |                     if not os.path.exists(expanded_dir):
 66 | |                         session_missing.append(("session", "Session Root", expanded_dir))
 67 | |
 68 | |                     # ìœˆë„ìš°ë³„ start_directory ê²€ì‚¬
 69 | |                     windows = final_config.get("windows", [])
 70 | |                     for i, window in enumerate(windows):
 71 | |                         window_start_dir = window.get("start_directory")
 72 | |                         if window_start_dir:
 73 | |                             if not os.path.isabs(window_start_dir):
 74 | |                                 # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì„¸ì…˜ì˜ ì‹œì‘ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•¨
 75 | |                                 base_dir = expanded_dir
 76 | |                                 window_start_dir = os.path.join(base_dir, window_start_dir)
 77 | |                             expanded_window_dir = os.path.expanduser(window_start_dir)
 78 | |                             if not os.path.exists(expanded_window_dir):
 79 | |                                 window_name = window.get("window_name", f"window_{i}")
 80 | |                                 session_missing.append(("window", window_name, expanded_window_dir))
 81 | |
 82 | |                         # íŒ¬ë³„ start_directory ê²€ì‚¬ (íŒ¬ì´ ìˆëŠ” ê²½ìš°)
 83 | |                         panes = window.get("panes", [])
 84 | |                         for j, pane in enumerate(panes):
 85 | |                             if isinstance(pane, dict) and "start_directory" in pane:
 86 | |                                 pane_start_dir = pane["start_directory"]
 87 | |                                 if not os.path.isabs(pane_start_dir):
 88 | |                                     base_dir = expanded_dir
 89 | |                                     pane_start_dir = os.path.join(base_dir, pane_start_dir)
 90 | |                                 expanded_pane_dir = os.path.expanduser(pane_start_dir)
 91 | |                                 if not os.path.exists(expanded_pane_dir):
 92 | |                                     window_name = window.get("window_name", f"window_{i}")
 93 | |                                     session_missing.append(
 94 | |                                         (
 95 | |                                             "pane",
 96 | |                                             f"{window_name}/pane_{j}",
 97 | |                                             expanded_pane_dir,
 98 | |                                         )
 99 | |                                     )
100 | |
101 | |                     # Store results for this session
102 | |                     if session_missing:
103 | |                         missing.append((s_name, session_missing))
104 | |                     else:
105 | |                         valid_count += 1
106 | |
107 | |                 except Exception as e:
108 | |                     console.print(f"[yellow]âš ï¸  Error processing session '{s_name}': {e}[/yellow]")
109 | |                     continue
110 | |
111 | |             # Display results based on format
112 | |             if not missing:
113 | |                 _display_success(console, valid_count, len(sessions))
114 | |             elif format == "table":
115 | |                 _display_table_format(console, missing, valid_count, len(sessions))
116 | |             elif format == "tree":
117 | |                 _display_tree_format(console, missing, valid_count, len(sessions))
118 | |             else:
119 | |                 _display_simple_format(console, missing, valid_count, len(sessions))
120 | |
121 | |             return {
122 | |                 "success": True,
123 | |                 "total_sessions": len(sessions),
124 | |                 "valid_sessions": valid_count,
125 | |                 "invalid_sessions": len(missing),
126 | |                 "missing_directories": missing,
127 | |             }
128 | |
129 | |         except Exception as e:
130 | |             msg = f"Error validating directories: {e}"
131 | |             raise CommandError(msg) from e
    | |__________________________________________^ PLR1702
    |

commands/validate.py:31:9: PLR1702 Too many nested blocks (7 > 5)
    |
 29 |           dict: Description of return value.
 30 |           """
 31 | /         try:
 32 | |             console = Console()
 33 | |             sessions = self.tmux_manager.load_projects().get("sessions", {})
 34 | |
 35 | |             if not sessions:
 36 | |                 console.print("[red]âŒ No sessions defined in projects.yaml[/red]")
 37 | |                 return {"success": False, "error": "no_sessions_defined"}
 38 | |
 39 | |             if session_name:
 40 | |                 if session_name not in sessions:
 41 | |                     console.print(f"[red]âŒ Session '{session_name}' not defined in projects.yaml[/red]")
 42 | |                     return {"success": False, "error": "session_not_defined"}
 43 | |                 sessions = {session_name: sessions[session_name]}
 44 | |
 45 | |             # Show progress for multiple sessions
 46 | |             if len(sessions) > 1:
 47 | |                 console.print(f"[blue]ğŸ” Validating {len(sessions)} sessions...[/blue]\n")
 48 | |
 49 | |             missing = []
 50 | |             valid_count = 0
 51 | |
 52 | |             # Process sessions with progress tracking for multiple sessions
 53 | |             sessions_to_process = list(sessions.items())
 54 | |             iterator = track(sessions_to_process, description="Validating sessions...") if len(sessions) > 3 else sessions_to_process
 55 | |
 56 | |             for s_name, sess_conf in iterator:
 57 | |                 try:
 58 | |                     # í…œí”Œë¦¿ì´ ì ìš©ëœ ìµœì¢… ì„¤ì •ì„ ê°€ì ¸ì˜¤ê¸°
 59 | |                     final_config = self.tmux_manager.get_session_config(s_name, sess_conf)
 60 | |                     session_missing = []
 61 | |
 62 | |                     # ì„¸ì…˜ ì‹œì‘ ë””ë ‰í† ë¦¬ ê²€ì‚¬
 63 | |                     start_dir = final_config.get("start_directory", os.getcwd())
 64 | |                     expanded_dir = os.path.expanduser(start_dir)
 65 | |                     if not os.path.exists(expanded_dir):
 66 | |                         session_missing.append(("session", "Session Root", expanded_dir))
 67 | |
 68 | |                     # ìœˆë„ìš°ë³„ start_directory ê²€ì‚¬
 69 | |                     windows = final_config.get("windows", [])
 70 | |                     for i, window in enumerate(windows):
 71 | |                         window_start_dir = window.get("start_directory")
 72 | |                         if window_start_dir:
 73 | |                             if not os.path.isabs(window_start_dir):
 74 | |                                 # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì„¸ì…˜ì˜ ì‹œì‘ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•¨
 75 | |                                 base_dir = expanded_dir
 76 | |                                 window_start_dir = os.path.join(base_dir, window_start_dir)
 77 | |                             expanded_window_dir = os.path.expanduser(window_start_dir)
 78 | |                             if not os.path.exists(expanded_window_dir):
 79 | |                                 window_name = window.get("window_name", f"window_{i}")
 80 | |                                 session_missing.append(("window", window_name, expanded_window_dir))
 81 | |
 82 | |                         # íŒ¬ë³„ start_directory ê²€ì‚¬ (íŒ¬ì´ ìˆëŠ” ê²½ìš°)
 83 | |                         panes = window.get("panes", [])
 84 | |                         for j, pane in enumerate(panes):
 85 | |                             if isinstance(pane, dict) and "start_directory" in pane:
 86 | |                                 pane_start_dir = pane["start_directory"]
 87 | |                                 if not os.path.isabs(pane_start_dir):
 88 | |                                     base_dir = expanded_dir
 89 | |                                     pane_start_dir = os.path.join(base_dir, pane_start_dir)
 90 | |                                 expanded_pane_dir = os.path.expanduser(pane_start_dir)
 91 | |                                 if not os.path.exists(expanded_pane_dir):
 92 | |                                     window_name = window.get("window_name", f"window_{i}")
 93 | |                                     session_missing.append(
 94 | |                                         (
 95 | |                                             "pane",
 96 | |                                             f"{window_name}/pane_{j}",
 97 | |                                             expanded_pane_dir,
 98 | |                                         )
 99 | |                                     )
100 | |
101 | |                     # Store results for this session
102 | |                     if session_missing:
103 | |                         missing.append((s_name, session_missing))
104 | |                     else:
105 | |                         valid_count += 1
106 | |
107 | |                 except Exception as e:
108 | |                     console.print(f"[yellow]âš ï¸  Error processing session '{s_name}': {e}[/yellow]")
109 | |                     continue
110 | |
111 | |             # Display results based on format
112 | |             if not missing:
113 | |                 _display_success(console, valid_count, len(sessions))
114 | |             elif format == "table":
115 | |                 _display_table_format(console, missing, valid_count, len(sessions))
116 | |             elif format == "tree":
117 | |                 _display_tree_format(console, missing, valid_count, len(sessions))
118 | |             else:
119 | |                 _display_simple_format(console, missing, valid_count, len(sessions))
120 | |
121 | |             return {
122 | |                 "success": True,
123 | |                 "total_sessions": len(sessions),
124 | |                 "valid_sessions": valid_count,
125 | |                 "invalid_sessions": len(missing),
126 | |                 "missing_directories": missing,
127 | |             }
128 | |
129 | |         except Exception as e:
130 | |             msg = f"Error validating directories: {e}"
131 | |             raise CommandError(msg) from e
    | |__________________________________________^ PLR1702
    |

commands/validate.py:31:9: PLR1702 Too many nested blocks (7 > 5)
    |
 29 |           dict: Description of return value.
 30 |           """
 31 | /         try:
 32 | |             console = Console()
 33 | |             sessions = self.tmux_manager.load_projects().get("sessions", {})
 34 | |
 35 | |             if not sessions:
 36 | |                 console.print("[red]âŒ No sessions defined in projects.yaml[/red]")
 37 | |                 return {"success": False, "error": "no_sessions_defined"}
 38 | |
 39 | |             if session_name:
 40 | |                 if session_name not in sessions:
 41 | |                     console.print(f"[red]âŒ Session '{session_name}' not defined in projects.yaml[/red]")
 42 | |                     return {"success": False, "error": "session_not_defined"}
 43 | |                 sessions = {session_name: sessions[session_name]}
 44 | |
 45 | |             # Show progress for multiple sessions
 46 | |             if len(sessions) > 1:
 47 | |                 console.print(f"[blue]ğŸ” Validating {len(sessions)} sessions...[/blue]\n")
 48 | |
 49 | |             missing = []
 50 | |             valid_count = 0
 51 | |
 52 | |             # Process sessions with progress tracking for multiple sessions
 53 | |             sessions_to_process = list(sessions.items())
 54 | |             iterator = track(sessions_to_process, description="Validating sessions...") if len(sessions) > 3 else sessions_to_process
 55 | |
 56 | |             for s_name, sess_conf in iterator:
 57 | |                 try:
 58 | |                     # í…œí”Œë¦¿ì´ ì ìš©ëœ ìµœì¢… ì„¤ì •ì„ ê°€ì ¸ì˜¤ê¸°
 59 | |                     final_config = self.tmux_manager.get_session_config(s_name, sess_conf)
 60 | |                     session_missing = []
 61 | |
 62 | |                     # ì„¸ì…˜ ì‹œì‘ ë””ë ‰í† ë¦¬ ê²€ì‚¬
 63 | |                     start_dir = final_config.get("start_directory", os.getcwd())
 64 | |                     expanded_dir = os.path.expanduser(start_dir)
 65 | |                     if not os.path.exists(expanded_dir):
 66 | |                         session_missing.append(("session", "Session Root", expanded_dir))
 67 | |
 68 | |                     # ìœˆë„ìš°ë³„ start_directory ê²€ì‚¬
 69 | |                     windows = final_config.get("windows", [])
 70 | |                     for i, window in enumerate(windows):
 71 | |                         window_start_dir = window.get("start_directory")
 72 | |                         if window_start_dir:
 73 | |                             if not os.path.isabs(window_start_dir):
 74 | |                                 # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì„¸ì…˜ì˜ ì‹œì‘ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•¨
 75 | |                                 base_dir = expanded_dir
 76 | |                                 window_start_dir = os.path.join(base_dir, window_start_dir)
 77 | |                             expanded_window_dir = os.path.expanduser(window_start_dir)
 78 | |                             if not os.path.exists(expanded_window_dir):
 79 | |                                 window_name = window.get("window_name", f"window_{i}")
 80 | |                                 session_missing.append(("window", window_name, expanded_window_dir))
 81 | |
 82 | |                         # íŒ¬ë³„ start_directory ê²€ì‚¬ (íŒ¬ì´ ìˆëŠ” ê²½ìš°)
 83 | |                         panes = window.get("panes", [])
 84 | |                         for j, pane in enumerate(panes):
 85 | |                             if isinstance(pane, dict) and "start_directory" in pane:
 86 | |                                 pane_start_dir = pane["start_directory"]
 87 | |                                 if not os.path.isabs(pane_start_dir):
 88 | |                                     base_dir = expanded_dir
 89 | |                                     pane_start_dir = os.path.join(base_dir, pane_start_dir)
 90 | |                                 expanded_pane_dir = os.path.expanduser(pane_start_dir)
 91 | |                                 if not os.path.exists(expanded_pane_dir):
 92 | |                                     window_name = window.get("window_name", f"window_{i}")
 93 | |                                     session_missing.append(
 94 | |                                         (
 95 | |                                             "pane",
 96 | |                                             f"{window_name}/pane_{j}",
 97 | |                                             expanded_pane_dir,
 98 | |                                         )
 99 | |                                     )
100 | |
101 | |                     # Store results for this session
102 | |                     if session_missing:
103 | |                         missing.append((s_name, session_missing))
104 | |                     else:
105 | |                         valid_count += 1
106 | |
107 | |                 except Exception as e:
108 | |                     console.print(f"[yellow]âš ï¸  Error processing session '{s_name}': {e}[/yellow]")
109 | |                     continue
110 | |
111 | |             # Display results based on format
112 | |             if not missing:
113 | |                 _display_success(console, valid_count, len(sessions))
114 | |             elif format == "table":
115 | |                 _display_table_format(console, missing, valid_count, len(sessions))
116 | |             elif format == "tree":
117 | |                 _display_tree_format(console, missing, valid_count, len(sessions))
118 | |             else:
119 | |                 _display_simple_format(console, missing, valid_count, len(sessions))
120 | |
121 | |             return {
122 | |                 "success": True,
123 | |                 "total_sessions": len(sessions),
124 | |                 "valid_sessions": valid_count,
125 | |                 "invalid_sessions": len(missing),
126 | |                 "missing_directories": missing,
127 | |             }
128 | |
129 | |         except Exception as e:
130 | |             msg = f"Error validating directories: {e}"
131 | |             raise CommandError(msg) from e
    | |__________________________________________^ PLR1702
    |

libs/ai/adaptive_response.py:88:13: TRY300 Consider moving this statement to an `else` block
   |
86 |             logger.info("Sent adaptive response: '{response}' for prompt type: {self.analyzer._classify_prompt_type(prompt_text)}")
87 |
88 |             return True
   |             ^^^^^^^^^^^ TRY300
89 |
90 |         except Exception:
   |

libs/ai/adaptive_response.py:194:13: TRY300 Consider moving this statement to an `else` block
    |
192 |             }
193 |
194 |             return {**base_stats, **adaptive_stats}
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
195 |
196 |         except Exception:
    |

libs/ai/adaptive_response.py:221:13: TRY300 Consider moving this statement to an `else` block
    |
219 |                     break
220 |
221 |             return suggestions
    |             ^^^^^^^^^^^^^^^^^^ TRY300
222 |
223 |         except Exception:
    |

libs/ai/adaptive_response.py:248:13: TRY300 Consider moving this statement to an `else` block
    |
247 |             logger.info("Learning data exported to: {output_path}")
248 |             return True
    |             ^^^^^^^^^^^ TRY300
249 |
250 |         except Exception:
    |

libs/automation/workflow_engine.py:441:17: TRY300 Consider moving this statement to an `else` block
    |
439 |                     }
440 |                 )
441 |                 return True
    |                 ^^^^^^^^^^^ TRY300
442 |
443 |             except Exception as e:
    |

libs/core/async_base_command.py:19:1: E303 Too many blank lines (4)
   |
19 | class AsyncBaseCommand(BaseCommand, ABC):
   | ^^^^^ E303
20 |     """Async-capable base class for commands with long-running operations."""
   |
   = help: Remove extraneous blank line(s)

libs/core/async_base_command.py:40:55: SyntaxError: Expected ',', found ':'
   |
38 |                 if loop.is_running():
39 |                     # We're already in an async context, use run_coroutine_threadsafe directly
40 |                     coro = self.execute_async(**kwargs: dict[str, object])
   |                                                       ^
41 |                     return asyncio.run_coroutine_threadsafe(coro, loop).result()
42 |                 return loop.run_until_complete(self.execute_async(**kwargs: dict[str, object]))
   |

libs/core/async_base_command.py:40:57: SyntaxError: Positional argument cannot follow keyword argument unpacking
   |
38 |                 if loop.is_running():
39 |                     # We're already in an async context, use run_coroutine_threadsafe directly
40 |                     coro = self.execute_async(**kwargs: dict[str, object])
   |                                                         ^^^^^^^^^^^^^^^^^
41 |                     return asyncio.run_coroutine_threadsafe(coro, loop).result()
42 |                 return loop.run_until_complete(self.execute_async(**kwargs: dict[str, object]))
   |

libs/core/async_base_command.py:42:75: SyntaxError: Expected ',', found ':'
   |
40 |                     coro = self.execute_async(**kwargs: dict[str, object])
41 |                     return asyncio.run_coroutine_threadsafe(coro, loop).result()
42 |                 return loop.run_until_complete(self.execute_async(**kwargs: dict[str, object]))
   |                                                                           ^
43 |             except RuntimeError:
44 |                 # No event loop exists, create a new one
   |

libs/core/async_base_command.py:42:77: SyntaxError: Positional argument cannot follow keyword argument unpacking
   |
40 |                     coro = self.execute_async(**kwargs: dict[str, object])
41 |                     return asyncio.run_coroutine_threadsafe(coro, loop).result()
42 |                 return loop.run_until_complete(self.execute_async(**kwargs: dict[str, object]))
   |                                                                             ^^^^^^^^^^^^^^^^^
43 |             except RuntimeError:
44 |                 # No event loop exists, create a new one
   |

libs/core/async_base_command.py:45:63: SyntaxError: Expected ',', found ':'
   |
43 |             except RuntimeError:
44 |                 # No event loop exists, create a new one
45 |                 return asyncio.run(self.execute_async(**kwargs: dict[str, object]))
   |                                                               ^
46 |         except Exception as e:
47 |             if isinstance(e, CommandError):
   |

libs/core/async_base_command.py:45:65: SyntaxError: Positional argument cannot follow keyword argument unpacking
   |
43 |             except RuntimeError:
44 |                 # No event loop exists, create a new one
45 |                 return asyncio.run(self.execute_async(**kwargs: dict[str, object]))
   |                                                                 ^^^^^^^^^^^^^^^^^
46 |         except Exception as e:
47 |             if isinstance(e, CommandError):
   |

libs/core/base_command.py:44:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
42 |         exit_code: int = 1,
43 |         recovery_hint: str | None = None,
44 |         **kwargs: Any,
   |                   ^^^ ANN401
45 |     ) -> None:
46 |         super().__init__(
   |

libs/core/base_command.py:182:62: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
180 |             error_handler.handle_error(error, exit_on_critical=True)
181 |
182 |     def log_command_start(self, command_name: str, **kwargs: Any) -> None:
    |                                                              ^^^ ANN401
183 |         """Log command start with parameters."""
184 |         params = ", ".join(f"{k}={v}" for k, v in kwargs.items() if v is not None)
    |

libs/core/base_command.py:221:33: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
220 |     @abstractmethod
221 |     def execute(self, **kwargs: Any) -> object:
    |                                 ^^^ ANN401
222 |         """Execute the command (must be implemented by subclasses)."""
    |

libs/core/base_command.py:224:29: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
222 |         """Execute the command (must be implemented by subclasses)."""
223 |
224 |     def run(self, **kwargs: Any) -> object:
    |                             ^^^ ANN401
225 |         """Main execution wrapper with error handling."""
226 |         command_name = self.__class__.__name__.replace("Command", "").lower()
    |

libs/core/base_command.py:236:13: TRY300 Consider moving this statement to an `else` block
    |
234 |             self.log_command_end(command_name, success=True)
235 |             self.handle_success(result)
236 |             return result
    |             ^^^^^^^^^^^^^ TRY300
237 |
238 |         except YesmanError as e:
    |

libs/core/base_command.py:346:13: TRY300 Consider moving this statement to an `else` block
    |
344 |             shutil.copy2(config_path, backup_path)
345 |             self.logger.info("Configuration backed up to: %s", backup_path)
346 |             return backup_path
    |             ^^^^^^^^^^^^^^^^^^ TRY300
347 |         except Exception as e:
348 |             self.logger.warning("Failed to create backup: %s", e)
    |

libs/core/claude_manager.py:17:1: PLR0904 Too many public methods (47 > 20)
    |
 17 | / class DashboardController:
 18 | |     """Main controller that orchestrates Claude session management, process control, and monitoring."""
 19 | |
 20 | |     def __init__(self, session_name: str, pane_id: str | None = None) -> None:
 21 | |         """Initialize the dashboard controller."""
 22 | |         self.session_name = session_name
 23 | |         self.pane_id = pane_id
 24 | |
 25 | |         # Initialize component managers
 26 | |         self.session_manager = ClaudeSessionManager(session_name, pane_id)
 27 | |         self.status_manager = ClaudeStatusManager(session_name)
 28 | |         self.process_controller = ClaudeProcessController(
 29 | |             self.session_manager,
 30 | |             self.status_manager,
 31 | |         )
 32 | |         self.monitor = ClaudeMonitor(
 33 | |             self.session_manager,
 34 | |             self.process_controller,
 35 | |             self.status_manager,
 36 | |         )
 37 | |
 38 | |         self.logger = logging.getLogger(f"yesman.dashboard.controller.{session_name}")
 39 | |
 40 | |         # Try to initialize session, but don't fail if session doesn't exist
 41 | |         if not self.session_manager.initialize_session():
 42 | |             self.status_manager.update_status(
 43 | |                 f"[yellow]Session '{session_name}' not found[/]",
 44 | |             )
 45 | |
 46 | |     @property
 47 | |     def claude_pane(self) -> object:
 48 | |         """Get the Claude pane (for backward compatibility).
 49 | |
 50 | |         Returns:
 51 | |         object: Description of return value.
 52 | |         """
 53 | |         return self.session_manager.get_claude_pane()
 54 | |
 55 | |     @property
 56 | |     def is_running(self) -> bool:
 57 | |         """Check if monitoring is running.
 58 | |
 59 | |         Returns:
 60 | |         bool: Description of return value.
 61 | |         """
 62 | |         return self.monitor.is_running
 63 | |
 64 | |     @property
 65 | |     def is_auto_next_enabled(self) -> bool:
 66 | |         """Check if auto-next is enabled.
 67 | |
 68 | |         Returns:
 69 | |         bool: Description of return value.
 70 | |         """
 71 | |         return self.monitor.is_auto_next_enabled
 72 | |
 73 | |     @property
 74 | |     def selected_model(self) -> str:
 75 | |         """Get selected model.
 76 | |
 77 | |         Returns:
 78 | |         str: Description of return value.
 79 | |         """
 80 | |         return self.process_controller.selected_model
 81 | |
 82 | |     def set_status_callback(self, callback: Callable) -> None:
 83 | |         """Set callback for status updates."""
 84 | |         self.status_manager.set_status_callback(callback)
 85 | |
 86 | |     def set_activity_callback(self, callback: Callable) -> None:
 87 | |         """Set callback for activity updates."""
 88 | |         self.status_manager.set_activity_callback(callback)
 89 | |
 90 | |     def start(self) -> bool:
 91 | |         """Start the controller.
 92 | |
 93 | |         Returns:
 94 | |         bool: Description of return value.
 95 | |         """
 96 | |         # Re-initialize in case session was created after initialization
 97 | |         if not self.claude_pane and not self.session_manager.initialize_session():
 98 | |             self.logger.error(f"Failed to initialize session '{self.session_name}'")  # noqa: G004
 99 | |             return False
100 | |
101 | |         if not self.claude_pane:
102 | |             self.logger.error(
103 | |                 "No Claude pane found in session '%s'. Make sure the session is running and Claude Code is started.",
104 | |                 self.session_name,
105 | |             )
106 | |             return False
107 | |
108 | |         return self.monitor.start_monitoring()
109 | |
110 | |     def stop(self) -> bool:
111 | |         """Stop the controller.
112 | |
113 | |         Returns:
114 | |         bool: Description of return value.
115 | |         """
116 | |         return self.monitor.stop_monitoring()
117 | |
118 | |     def restart_claude_pane(self) -> bool:
119 | |         """Restart Claude pane.
120 | |
121 | |         Returns:
122 | |         bool: Description of return value.
123 | |         """
124 | |         return self.process_controller.restart_claude_pane()
125 | |
126 | |     def set_model(self, model: str) -> None:
127 | |         """Set the selected model."""
128 | |         self.process_controller.set_model(model)
129 | |
130 | |     def set_auto_next(self, enabled: bool) -> None:  # noqa: FBT001
131 | |         """Enable or disable auto-next responses."""
132 | |         self.monitor.set_auto_next(enabled)
133 | |
134 | |     def set_mode_yn(self, mode: str, response: str) -> None:
135 | |         """Set manual override for Y/N prompts."""
136 | |         self.monitor.set_mode_yn(mode, response)
137 | |
138 | |     def set_mode_12(self, mode: str, response: str) -> None:
139 | |         """Set manual override for 1/2 prompts."""
140 | |         self.monitor.set_mode_12(mode, response)
141 | |
142 | |     def set_mode_123(self, mode: str, response: str) -> None:
143 | |         """Set manual override for 1/2/3 prompts."""
144 | |         self.monitor.set_mode_123(mode, response)
145 | |
146 | |     def capture_pane_content(self, lines: int = 50) -> str:
147 | |         """Capture content from Claude pane.
148 | |
149 | |         Returns:
150 | |         str: Description of return value.
151 | |         """
152 | |         content = self.session_manager.capture_pane_content(lines)
153 | |         if content:
154 | |             # Save capture to file
155 | |             self.status_manager.save_capture_to_file(content, self.pane_id)
156 | |         return content
157 | |
158 | |     def send_input(self, text: str) -> None:
159 | |         """Send input to Claude pane."""
160 | |         self.process_controller.send_input(text)
161 | |
162 | |     def is_waiting_for_input(self) -> bool:
163 | |         """Check if Claude is currently waiting for user input.
164 | |
165 | |         Returns:
166 | |         bool: Description of return value.
167 | |         """
168 | |         return self.monitor.is_waiting_for_input()
169 | |
170 | |     def get_current_prompt(self) -> PromptInfo | None:
171 | |         """Get the current prompt information.
172 | |
173 | |         Returns:
174 | |         object: Description of return value.
175 | |         """
176 | |         return self.monitor.get_current_prompt()
177 | |
178 | |     def get_response_history(self) -> list:
179 | |         """Get the response history.
180 | |
181 | |         Returns:
182 | |         list: Description of return value.
183 | |         """
184 | |         return self.status_manager.get_response_history()
185 | |
186 | |     def get_collection_stats(self) -> dict:
187 | |         """Get content collection statistics.
188 | |
189 | |         Returns:
190 | |         dict: Description of return value.
191 | |         """
192 | |         return self.monitor.get_collection_stats()
193 | |
194 | |     def cleanup_old_collections(self, days_to_keep: int = 7) -> int:
195 | |         """Clean up old collection files.
196 | |
197 | |         Returns:
198 | |         int: Description of return value.
199 | |         """
200 | |         return self.monitor.cleanup_old_collections(days_to_keep)
201 | |
202 | |     # Adaptive response system methods
203 | |     def get_adaptive_statistics(self) -> dict:
204 | |         """Get statistics from the adaptive response system.
205 | |
206 | |         Returns:
207 | |         dict: Description of return value.
208 | |         """
209 | |         return self.monitor.get_adaptive_statistics()
210 | |
211 | |     def set_adaptive_confidence_threshold(self, threshold: float) -> None:
212 | |         """Adjust the confidence threshold for adaptive responses."""
213 | |         self.monitor.set_adaptive_confidence_threshold(threshold)
214 | |
215 | |     def enable_adaptive_response(self, enabled: bool = True) -> None:  # noqa: FBT001
216 | |         """Enable or disable adaptive response functionality."""
217 | |         self.monitor.enable_adaptive_response(enabled)
218 | |
219 | |     def enable_adaptive_learning(self, enabled: bool = True) -> None:  # noqa: FBT001
220 | |         """Enable or disable adaptive learning functionality."""
221 | |         self.monitor.enable_adaptive_learning(enabled)
222 | |
223 | |     def export_adaptive_data(self, output_path: str) -> bool:
224 | |         """Export adaptive learning data for analysis.
225 | |
226 | |         Returns:
227 | |         bool: Description of return value.
228 | |         """
229 | |         return self.monitor.export_adaptive_data(output_path)
230 | |
231 | |     def learn_from_user_input(
232 | |         self,
233 | |         prompt_text: str,
234 | |         user_response: str,
235 | |         context: str = "",
236 | |     ) -> None:
237 | |         """Learn from manual user input for future improvements.
238 | |
239 | |         Returns:
240 | |         None: Description of return value.
241 | |         """
242 | |         self.monitor.learn_from_user_input(prompt_text, user_response, context)
243 | |
244 | |     # Context-aware automation methods
245 | |     async def start_automation_monitoring(self, monitor_interval: int = 10) -> bool:
246 | |         """Start context-aware automation monitoring."""
247 | |         return await self.monitor.start_automation_monitoring(monitor_interval)
248 | |
249 | |     async def stop_automation_monitoring(self) -> bool:
250 | |         """Stop context-aware automation monitoring."""
251 | |         return await self.monitor.stop_automation_monitoring()
252 | |
253 | |     def get_automation_status(self) -> dict:
254 | |         """Get automation system status.
255 | |
256 | |         Returns:
257 | |         dict: Description of return value.
258 | |         """
259 | |         return self.monitor.get_automation_status()
260 | |
261 | |     def register_automation_workflow(self, workflow: object) -> None:
262 | |         """Register a custom automation workflow."""
263 | |         self.monitor.register_automation_workflow(workflow)
264 | |
265 | |     async def test_automation(self, context_type_name: str) -> dict:
266 | |         """Test automation with simulated context."""
267 | |         return await self.monitor.test_automation(context_type_name)
268 | |
269 | |     def get_automation_execution_history(self, limit: int = 10) -> list:
270 | |         """Get recent automation execution history.
271 | |
272 | |         Returns:
273 | |         list: Description of return value.
274 | |         """
275 | |         return self.monitor.get_automation_execution_history(limit)
276 | |
277 | |     def save_automation_config(self) -> None:
278 | |         """Save automation configuration."""
279 | |         self.monitor.save_automation_config()
280 | |
281 | |     def load_automation_config(self) -> None:
282 | |         """Load automation configuration."""
283 | |         self.monitor.load_automation_config()
284 | |
285 | |     # Project health monitoring methods
286 | |     async def calculate_project_health(self, force_refresh: bool = False) -> dict:  # noqa: FBT001
287 | |         """Calculate comprehensive project health."""
288 | |         return await self.monitor.calculate_project_health(force_refresh)
289 | |
290 | |     def get_health_summary(self) -> dict:
291 | |         """Get a quick health summary.
292 | |
293 | |         Returns:
294 | |         dict: Description of return value.
295 | |         """
296 | |         return self.monitor.get_health_summary()
297 | |
298 | |     # Asynchronous logging methods
299 | |     async def start_async_monitoring(self) -> bool:
300 | |         """Start monitoring with async logging enabled."""
301 | |         return await self.monitor.start_async_monitoring()
302 | |
303 | |     async def stop_async_monitoring(self) -> bool:
304 | |         """Stop monitoring and async logging."""
305 | |         return await self.monitor.stop_async_monitoring()
306 | |
307 | |     def get_async_logging_stats(self) -> dict:
308 | |         """Get async logging statistics.
309 | |
310 | |         Returns:
311 | |         dict: Description of return value.
312 | |         """
313 | |         return self.monitor.get_async_logging_stats()
314 | |
315 | |     async def flush_async_logs(self) -> None:
316 | |         """Force flush all pending async logs."""
317 | |         await self.monitor.flush_async_logs()
318 | |
319 | |     # Legacy compatibility methods (deprecated but kept for backward compatibility)
320 | |     def detect_trust_prompt(self, content: str) -> bool:
321 | |         """Detect trust prompt in content (deprecated - use monitor methods).
322 | |
323 | |         Returns:
324 | |         bool: Description of return value.
325 | |         """
326 | |         return self.monitor._detect_trust_prompt(content)
327 | |
328 | |     def auto_trust_if_needed(self) -> bool:
329 | |         """Auto-respond to trust prompts if detected (deprecated - use monitor methods).
330 | |
331 | |         Returns:
332 | |         bool: Description of return value.
333 | |         """
334 | |         return self.monitor._auto_trust_if_needed()
335 | |
336 | |     def auto_respond_to_selection(self, prompt_info: PromptInfo) -> bool:
337 | |         """Auto-respond to selection prompts (deprecated - use monitor methods).
338 | |
339 | |         Returns:
340 | |         bool: Description of return value.
341 | |         """
342 | |         return self.monitor._auto_respond_to_selection(prompt_info)
343 | |
344 | |     def check_for_prompt(self, content: str) -> PromptInfo | None:
345 | |         """Check if content contains a prompt waiting for input (deprecated - use monitor methods).
346 | |
347 | |         Returns:
348 | |         object: Description of return value.
349 | |         """
350 | |         return self.monitor._check_for_prompt(content)
351 | |
352 | |     def clear_prompt_state(self) -> None:
353 | |         """Clear prompt state (deprecated - use monitor methods)."""
354 | |         self.monitor._clear_prompt_state()
    | |__________________________________________^ PLR0904
    |

libs/core/claude_monitor.py:24:1: PLR0904 Too many public methods (31 > 20)
    |
 24 | / class ClaudeMonitor:
 25 | |     """Handles Claude monitoring and auto-response logic."""
 26 | |
 27 | |     def __init__(self, session_manager: object, process_controller: object, status_manager: object) -> None:
 28 | |         self.session_manager = session_manager
 29 | |         self.process_controller = process_controller
 30 | |         self.status_manager = status_manager
 31 | |         self.session_name = session_manager.session_name
 32 | |
 33 | |         # Monitoring state
 34 | |         self.is_running = False
 35 | |         self._monitor_thread: threading.Thread | None = None
 36 | |         self._loop: asyncio.AbstractEventLoop | None = None
 37 | |
 38 | |         # Auto-response settings
 39 | |         self.is_auto_next_enabled = True
 40 | |         self.yn_mode = "Auto"
 41 | |         self.yn_response = "y"
 42 | |         self.mode12 = "Auto"
 43 | |         self.mode12_response = "1"
 44 | |         self.mode123 = "Auto"
 45 | |         self.mode123_response = "1"
 46 | |
 47 | |         # Prompt detection
 48 | |         self.prompt_detector = ClaudePromptDetector()
 49 | |         self.content_collector = ClaudeContentCollector(session_manager.session_name)
 50 | |         self.current_prompt: PromptInfo | None = None
 51 | |         self.waiting_for_input = False
 52 | |
 53 | |         # AI-powered adaptive response system
 54 | |         self.adaptive_response = AdaptiveResponse(
 55 | |             config=AdaptiveConfig(
 56 | |                 min_confidence_threshold=0.7,
 57 | |                 learning_enabled=True,
 58 | |                 auto_response_enabled=True,
 59 | |                 response_delay_ms=1500,  # Slightly longer delay for more natural interaction
 60 | |             ),
 61 | |         )
 62 | |
 63 | |         # Context-aware automation system
 64 | |         self.automation_manager = AutomationManager(
 65 | |             project_path=None,  # Will use current working directory
 66 | |         )
 67 | |
 68 | |         # Project health monitoring system
 69 | |         self.health_calculator = HealthCalculator(
 70 | |             project_path=None,  # Will use current working directory
 71 | |         )
 72 | |
 73 | |         # High-performance async logging system
 74 | |         self.async_logger: AsyncLogger | None = None
 75 | |
 76 | |         self.logger = logging.getLogger(f"yesman.claude_monitor.{self.session_name}")
 77 | |
 78 | |     def set_auto_next(self, enabled: bool) -> None:  # noqa: FBT001
 79 | |         """Enable or disable auto-next responses."""
 80 | |         self.is_auto_next_enabled = enabled
 81 | |         status = "enabled" if enabled else "disabled"
 82 | |         self.status_manager.update_status(f"[cyan]Auto next {status}[/]")
 83 | |
 84 | |     def set_mode_yn(self, mode: str, response: str) -> None:
 85 | |         """Set manual override for Y/N prompts."""
 86 | |         self.yn_mode = mode
 87 | |         self.yn_response = response
 88 | |
 89 | |     def set_mode_12(self, mode: str, response: str) -> None:
 90 | |         """Set manual override for 1/2 prompts."""
 91 | |         self.mode12 = mode
 92 | |         self.mode12_response = response
 93 | |
 94 | |     def set_mode_123(self, mode: str, response: str) -> None:
 95 | |         """Set manual override for 1/2/3 prompts."""
 96 | |         self.mode123 = mode
 97 | |         self.mode123_response = response
 98 | |
 99 | |     def start_monitoring(self) -> bool:
100 | |         """Start the monitoring loop."""
101 | |         if not self.session_manager.get_claude_pane():
102 | |             self.status_manager.update_status("[red]Cannot start: No Claude pane in session[/]")
103 | |             return False
104 | |
105 | |         if self.is_running:
106 | |             self.status_manager.update_status("[yellow]Monitor already running[/]")
107 | |             return False
108 | |
109 | |         try:
110 | |             self.is_running = True
111 | |             self.status_manager.update_status(f"[green]Starting claude monitor for {self.session_name}[/]")
112 | |
113 | |             # Start monitoring in a separate thread with its own event loop
114 | |             self._monitor_thread = threading.Thread(target=self._run_monitor_loop, daemon=True)
115 | |             self._monitor_thread.start()
116 | |
117 | |             return True
118 | |
119 | |         except Exception as e:
120 | |             self.is_running = False
121 | |             self.status_manager.update_status(f"[red]Failed to start claude monitor: {e}[/]")
122 | |             self.logger.error("Failed to start claude monitor: {e}", exc_info=True)
123 | |             return False
124 | |
125 | |     def stop_monitoring(self) -> bool:
126 | |         """Stop the monitoring loop."""
127 | |         if not self.is_running:
128 | |             self.status_manager.update_status("[yellow]Claude monitor not running[/]")
129 | |             return False
130 | |
131 | |         self.is_running = False
132 | |
133 | |         # Stop the event loop if it's running
134 | |         if self._loop and self._loop.is_running():
135 | |             self._loop.call_soon_threadsafe(self._loop.stop)
136 | |
137 | |         # Wait for thread to finish
138 | |         if self._monitor_thread and self._monitor_thread.is_alive():
139 | |             self._monitor_thread.join(timeout=2.0)
140 | |
141 | |         self.status_manager.update_status(f"[red]Stopped claude monitor for {self.session_name}[/]")
142 | |         return True
143 | |
144 | |     def _run_monitor_loop(self) -> None:
145 | |         """Run the monitor loop in its own thread with event loop."""
146 | |         self._loop = asyncio.new_event_loop()
147 | |         asyncio.set_event_loop(self._loop)
148 | |
149 | |         try:
150 | |             self._loop.run_until_complete(self._monitor_loop())
151 | |         except Exception:
152 | |             self.logger.error("Monitor loop error: {e}", exc_info=True)
153 | |         finally:
154 | |             self._loop.close()
155 | |             self.is_running = False
156 | |
157 | |     async def _monitor_loop(self) -> None:
158 | |         """Main monitoring loop that runs in background."""
159 | |         if not self.session_manager.get_claude_pane():
160 | |             self.logger.error("Cannot start monitoring: no Claude pane for {self.session_name}")
161 | |             self.is_running = False
162 | |             return
163 | |
164 | |         self.logger.info("Starting monitoring loop for {self.session_name}")
165 | |         last_content = ""
166 | |
167 | |         try:
168 | |             while self.is_running:
169 | |                 await asyncio.sleep(1)  # Check every second
170 | |
171 | |                 try:
172 | |                     content = self.session_manager.capture_pane_content()
173 | |
174 | |                     # Check if Claude is still running
175 | |                     if not self.process_controller.is_claude_running():
176 | |                         if self.is_auto_next_enabled:
177 | |                             self.status_manager.update_activity("ğŸ”„ Auto-restarting Claude...")
178 | |                             self.process_controller.restart_claude_pane()
179 | |                             continue
180 | |                         self.status_manager.update_status("[yellow]Claude not running. Auto-restart disabled.[/]")
181 | |                         continue
182 | |
183 | |                     # Check for prompts and auto-respond if enabled
184 | |                     prompt_info = self._check_for_prompt(content)
185 | |
186 | |                     if prompt_info:
187 | |                         # Try adaptive AI-powered response first if auto_next is enabled
188 | |                         if self.is_auto_next_enabled:
189 | |                             context = f"session:{self.session_name}, type:{prompt_info.type.value}"
190 | |                             (
191 | |                                 should_respond,
192 | |                                 ai_response,
193 | |                                 confidence,
194 | |                             ) = await self.adaptive_response.should_auto_respond(
195 | |                                 prompt_info.question,
196 | |                                 context,
197 | |                                 self.session_name,
198 | |                             )
199 | |
200 | |                             if should_respond:
201 | |                                 # Send AI-predicted response
202 | |                                 success = await self.adaptive_response.send_adaptive_response(
203 | |                                     prompt_info.question,
204 | |                                     ai_response,
205 | |                                     confidence,
206 | |                                     context,
207 | |                                     self.session_name,
208 | |                                 )
209 | |
210 | |                                 if success:
211 | |                                     self.process_controller.send_input(ai_response)
212 | |                                     self.status_manager.update_activity(f"ğŸ¤– AI auto-responded: '{ai_response}' (confidence: {confidenâ€¦
213 | |                                     self.status_manager.record_response(prompt_info.type.value, ai_response, content)
214 | |                                     self.adaptive_response.confirm_response_success(
215 | |                                         prompt_info.question,
216 | |                                         ai_response,
217 | |                                         context,
218 | |                                         self.session_name,
219 | |                                         True,
220 | |                                     )
221 | |                                     self._clear_prompt_state()
222 | |                                     continue
223 | |
224 | |                             # Fall back to legacy pattern-based auto-response if AI didn't handle it
225 | |                             if self._auto_respond_to_selection(prompt_info):
226 | |                                 response = self._get_legacy_response(prompt_info)
227 | |                                 self.status_manager.update_activity(f"âœ… Legacy auto-responded: '{response}' to {prompt_info.type.valuâ€¦
228 | |                                 self.status_manager.record_response(prompt_info.type.value, response, content)
229 | |                                 # Learn from legacy response for future AI improvements
230 | |                                 self.adaptive_response.learn_from_manual_response(
231 | |                                     prompt_info.question,
232 | |                                     response,
233 | |                                     context,
234 | |                                     self.session_name,
235 | |                                 )
236 | |                                 self._clear_prompt_state()
237 | |                                 continue
238 | |
239 | |                         # If auto-response didn't handle it, show waiting status
240 | |                         self.status_manager.update_activity(f"â³ Waiting for input: {prompt_info.type.value}")
241 | |                         self.logger.debug("Prompt detected: {prompt_info.type.value} - {prompt_info.question}")
242 | |                     elif self.waiting_for_input:
243 | |                         self.status_manager.update_activity("â³ Waiting for user input...")
244 | |                     else:
245 | |                         # Clear prompt state if no longer waiting
246 | |                         self._clear_prompt_state()
247 | |
248 | |                     # Periodically update AI patterns
249 | |                     await self.adaptive_response.update_patterns()
250 | |
251 | |                     # Analyze content for automation contexts
252 | |                     if content != last_content and len(content.strip()) > 0:
253 | |                         automation_contexts = self.automation_manager.analyze_content_for_context(content, self.session_name)
254 | |                         for auto_context in automation_contexts:
255 | |                             self.logger.info("Automation context detected: {auto_context.context_type.value} (confidence: {auto_conteâ€¦
256 | |
257 | |                     # Check for Claude idle automation opportunities
258 | |                     if hasattr(self.status_manager, "last_activity_time"):
259 | |                         idle_context = self.automation_manager.analyze_claude_idle(
260 | |                             self.status_manager.last_activity_time,
261 | |                             idle_threshold=60,
262 | |                         )
263 | |                         if idle_context:
264 | |                             self.logger.debug("Claude idle context: {idle_context.confidence:.2f}")
265 | |
266 | |                     # Collect content for pattern analysis
267 | |                     if content != last_content and len(content.strip()) > 0:
268 | |                         try:
269 | |                             # Convert PromptInfo to dict for collection compatibility
270 | |                             prompt_dict = None
271 | |                             if prompt_info:
272 | |                                 prompt_dict = {
273 | |                                     "type": prompt_info.type.value,
274 | |                                     "question": prompt_info.question,
275 | |                                     "options": prompt_info.options,
276 | |                                     "confidence": prompt_info.confidence,
277 | |                                 }
278 | |                             self.content_collector.collect_interaction(content, prompt_dict, None)
279 | |                         except Exception:
280 | |                             self.logger.exception("Failed to collect content: {e}")
281 | |
282 | |                     # Update activity if content changed
283 | |                     if content != last_content:
284 | |                         self.status_manager.update_activity("ğŸ“ Content updated")
285 | |                         last_content = content
286 | |
287 | |                 except Exception:
288 | |                     self.logger.exception("Error in monitoring loop: {e}")
289 | |                     await asyncio.sleep(5)  # Wait longer on errors
290 | |
291 | |         except asyncio.CancelledError:
292 | |             self.logger.info("Monitoring loop cancelled")
293 | |         except Exception:
294 | |             self.logger.exception("Monitoring loop error: {e}")
295 | |         finally:
296 | |             self.is_running = False
297 | |             self.status_manager.update_status("[red]Claude monitor stopped[/]")
298 | |
299 | |     def _check_for_prompt(self, content: str) -> PromptInfo | None:
300 | |         """Check if content contains a prompt waiting for input."""
301 | |         prompt_info = self.prompt_detector.detect_prompt(content)
302 | |
303 | |         if prompt_info:
304 | |             self.current_prompt = prompt_info
305 | |             self.waiting_for_input = True
306 | |             self.logger.info("Prompt detected: {prompt_info.type.value} - {prompt_info.question}")
307 | |         else:
308 | |             # Check if we're still waiting for input based on content patterns
309 | |             self.waiting_for_input = self.prompt_detector.is_waiting_for_input(content)
310 | |
311 | |         return prompt_info
312 | |
313 | |     def _clear_prompt_state(self) -> None:
314 | |         """Clear the current prompt state."""
315 | |         self.current_prompt = None
316 | |         self.waiting_for_input = False
317 | |
318 | |     def _auto_respond_to_selection(self, prompt_info: PromptInfo) -> bool:
319 | |         """Auto-respond to selection prompts based on patterns and manual overrides."""
320 | |         if not self.is_auto_next_enabled:
321 | |             self.logger.debug("Auto-response disabled, skipping")
322 | |             return False
323 | |
324 | |         self.logger.info("Attempting auto-response for prompt type: {prompt_info.type.value}")
325 | |
326 | |         try:
327 | |             if prompt_info.type == PromptType.NUMBERED_SELECTION:
328 | |                 return self._handle_numbered_selection(prompt_info)
329 | |             if prompt_info.type == PromptType.BINARY_CHOICE:
330 | |                 return self._handle_binary_choice(prompt_info)
331 | |             if prompt_info.type == PromptType.CONFIRMATION:
332 | |                 return self._handle_binary_selection(prompt_info)
333 | |             if prompt_info.type == PromptType.LOGIN_REDIRECT:
334 | |                 return self._handle_login_redirect(prompt_info)
335 | |
336 | |         except Exception:
337 | |             self.logger.exception("Error in auto_respond_to_selection: {e}")
338 | |
339 | |         return False
340 | |
341 | |     def _handle_numbered_selection(self, prompt_info: PromptInfo) -> bool:
342 | |         """Handle numbered selection prompts (1, 2, 3 options)."""
343 | |         opts_count = prompt_info.metadata.get("option_count", len(prompt_info.options))
344 | |
345 | |         # Check manual overrides
346 | |         if opts_count == 2 and self.mode12 == "Manual":
347 | |             response = self.mode12_response
348 | |         elif opts_count >= 3 and self.mode123 == "Manual":
349 | |             response = self.mode123_response
350 | |         else:
351 | |             # Use pattern-based response or fallback
352 | |             response = getattr(prompt_info, "recommended_response", None) or "1"
353 | |
354 | |         self.process_controller.send_input(response)
355 | |         self.status_manager.record_response(prompt_info.type.value, response, prompt_info.question)
356 | |         self.logger.info("Auto-responding to numbered selection with: {response}")
357 | |         return True
358 | |
359 | |     def _handle_binary_choice(self, prompt_info: PromptInfo) -> bool:
360 | |         """Handle binary choice prompts (y/n)."""
361 | |         # Check manual override
362 | |         if self.yn_mode == "Manual":
363 | |             response = self.yn_response.lower() if isinstance(self.yn_response, str) else str(self.yn_response)
364 | |         else:
365 | |             # Use pattern-based response or fallback
366 | |             response = getattr(prompt_info, "recommended_response", None) or "y"
367 | |
368 | |         self.process_controller.send_input(response)
369 | |         self.status_manager.record_response(prompt_info.type.value, response, prompt_info.question)
370 | |         self.logger.info("Auto-responding to binary choice with: {response}")
371 | |         return True
372 | |
373 | |     def _handle_binary_selection(self, prompt_info: PromptInfo) -> bool:
374 | |         """Handle binary selection prompts ([1] [2])."""
375 | |         # Check manual override
376 | |         if self.mode12 == "Manual":
377 | |             response = self.mode12_response
378 | |         else:
379 | |             # Use pattern-based response or fallback
380 | |             response = getattr(prompt_info, "recommended_response", None) or "1"
381 | |
382 | |         self.process_controller.send_input(response)
383 | |         self.status_manager.record_response(prompt_info.type.value, response, prompt_info.question)
384 | |         self.logger.info("Auto-responding to binary selection with: {response}")
385 | |         return True
386 | |
387 | |     def _handle_login_redirect(self, prompt_info: PromptInfo) -> bool:
388 | |         """Handle login redirect prompts."""
389 | |         question = prompt_info.question.lower()
390 | |
391 | |         if "continue" in question or "press enter" in question:
392 | |             response = ""  # Just press Enter
393 | |             self.process_controller.send_input(response)
394 | |             self.status_manager.record_response(prompt_info.type.value, "Enter", prompt_info.question)
395 | |             self.logger.info("Auto-responding to login redirect with Enter")
396 | |             return True
397 | |
398 | |         return False
399 | |
400 | |     # Public interface methods
401 | |     def is_waiting_for_input(self) -> bool:
402 | |         """Check if Claude is currently waiting for user input."""
403 | |         return self.waiting_for_input
404 | |
405 | |     def get_current_prompt(self) -> PromptInfo | None:
406 | |         """Get the current prompt information."""
407 | |         return self.current_prompt
408 | |
409 | |     def get_collection_stats(self) -> dict:
410 | |         """Get content collection statistics."""
411 | |         return self.content_collector.get_collection_stats()
412 | |
413 | |     def cleanup_old_collections(self, days_to_keep: int = 7) -> int:
414 | |         """Clean up old collection files."""
415 | |         return self.content_collector.cleanup_old_files(days_to_keep)
416 | |
417 | |     def _get_legacy_response(self, prompt_info: PromptInfo) -> str:
418 | |         """Get response that would be used by legacy auto-response system."""
419 | |         try:
420 | |             if prompt_info.type == PromptType.NUMBERED_SELECTION:
421 | |                 opts_count = prompt_info.metadata.get("option_count", len(prompt_info.options))
422 | |                 if opts_count == 2 and self.mode12 == "Manual":
423 | |                     return self.mode12_response
424 | |                 if opts_count >= 3 and self.mode123 == "Manual":
425 | |                     return self.mode123_response
426 | |                 return getattr(prompt_info, "recommended_response", None) or "1"
427 | |
428 | |             if prompt_info.type == PromptType.BINARY_CHOICE:
429 | |                 if self.yn_mode == "Manual":
430 | |                     return self.yn_response.lower() if isinstance(self.yn_response, str) else str(self.yn_response)
431 | |                 return getattr(prompt_info, "recommended_response", None) or "y"
432 | |
433 | |             if prompt_info.type == PromptType.CONFIRMATION:
434 | |                 if self.mode12 == "Manual":
435 | |                     return self.mode12_response
436 | |                 return getattr(prompt_info, "recommended_response", None) or "1"
437 | |
438 | |             if prompt_info.type == PromptType.LOGIN_REDIRECT:
439 | |                 question = prompt_info.question.lower()
440 | |                 if "continue" in question or "press enter" in question:
441 | |                     return ""  # Just press Enter
442 | |
443 | |         except Exception:
444 | |             self.logger.exception("Error getting legacy response: {e}")
445 | |
446 | |         return "1"  # Safe fallback
447 | |
448 | |     # Adaptive response management methods
449 | |     def get_adaptive_statistics(self) -> dict:
450 | |         """Get statistics from the adaptive response system."""
451 | |         return self.adaptive_response.get_learning_statistics()
452 | |
453 | |     def set_adaptive_confidence_threshold(self, threshold: float) -> None:
454 | |         """Adjust the confidence threshold for adaptive responses."""
455 | |         self.adaptive_response.adjust_confidence_threshold(threshold)
456 | |
457 | |     def enable_adaptive_response(self, enabled: bool = True) -> None:  # noqa: FBT001
458 | |         """Enable or disable adaptive response functionality."""
459 | |         self.adaptive_response.enable_auto_response(enabled)
460 | |
461 | |     def enable_adaptive_learning(self, enabled: bool = True) -> None:  # noqa: FBT001
462 | |         """Enable or disable adaptive learning functionality."""
463 | |         self.adaptive_response.enable_learning(enabled)
464 | |
465 | |     def export_adaptive_data(self, output_path: str) -> bool:
466 | |         """Export adaptive learning data for analysis."""
467 | |         return self.adaptive_response.export_learning_data(Path(output_path))
468 | |
469 | |     def learn_from_user_input(self, prompt_text: str, user_response: str, context: str = "") -> None:
470 | |         """Learn from manual user input for future improvements."""
471 | |         self.adaptive_response.learn_from_manual_response(
472 | |             prompt_text,
473 | |             user_response,
474 | |             context,
475 | |             self.session_name,
476 | |         )
477 | |
478 | |     # Context-aware automation methods
479 | |     async def start_automation_monitoring(self, monitor_interval: int = 10) -> bool:
480 | |         """Start context-aware automation monitoring."""
481 | |         return await self.automation_manager.start_monitoring(monitor_interval)
482 | |
483 | |     async def stop_automation_monitoring(self) -> bool:
484 | |         """Stop context-aware automation monitoring."""
485 | |         return await self.automation_manager.stop_monitoring()
486 | |
487 | |     def get_automation_status(self) -> dict:
488 | |         """Get automation system status."""
489 | |         return self.automation_manager.get_automation_status()
490 | |
491 | |     def register_automation_workflow(self, workflow: object) -> None:
492 | |         """Register a custom automation workflow."""
493 | |         self.automation_manager.register_custom_workflow(workflow)
494 | |
495 | |     async def test_automation(self, context_type_name: str) -> dict:
496 | |         """Test automation with simulated context."""
497 | |         try:
498 | |             context_type = ContextType(context_type_name)
499 | |             return await self.automation_manager.test_automation_chain(context_type)
500 | |         except ValueError:
501 | |             return {"error": f"Invalid context type: {context_type_name}"}
502 | |
503 | |     def get_automation_execution_history(self, limit: int = 10) -> list:
504 | |         """Get recent automation execution history."""
505 | |         return self.automation_manager.get_execution_history(limit)
506 | |
507 | |     def save_automation_config(self) -> None:
508 | |         """Save automation configuration."""
509 | |         self.automation_manager.save_automation_config()
510 | |
511 | |     def load_automation_config(self) -> None:
512 | |         """Load automation configuration."""
513 | |         self.automation_manager.load_automation_config()
514 | |
515 | |     # Project health monitoring methods
516 | |     async def calculate_project_health(self, force_refresh: bool = False) -> dict:  # noqa: FBT001
517 | |         """Calculate comprehensive project health."""
518 | |         health = await self.health_calculator.calculate_health(force_refresh)
519 | |         return health.to_dict()
520 | |
521 | |     def get_health_summary(self) -> dict:
522 | |         """Get a quick health summary."""
523 | |         # This would be cached from last calculation
524 | |         try:
525 | |             # For now, return a default summary - in real implementation this would use cached data
526 | |             return {
527 | |                 "overall": {"score": 75, "level": "good", "emoji": "ğŸŸ¡"},
528 | |                 "categories": {},
529 | |                 "metrics_count": 0,
530 | |                 "last_assessment": time.time(),
531 | |                 "project_path": str(self.health_calculator.project_path),
532 | |             }
533 | |         except Exception as e:
534 | |             self.logger.exception("Error getting health summary: {e}")
535 | |             return {"error": str(e)}
536 | |
537 | |     # Asynchronous logging methods
538 | |     async def _start_async_logging(self) -> None:
539 | |         """Start the async logging system."""
540 | |         if self.async_logger:
541 | |             return
542 | |
543 | |         config = AsyncLoggerConfig(
544 | |             name=f"yesman.claude_monitor.{self.session_name}",
545 | |             level=LogLevel.INFO,
546 | |             max_queue_size=5000,
547 | |             batch_size=25,
548 | |             flush_interval=3.0,
549 | |             enable_console=False,  # Use standard logger for console
550 | |             enable_file=True,
551 | |             enable_batch_processor=True,
552 | |         )
553 | |
554 | |         self.async_logger = AsyncLogger(config)
555 | |         await self.async_logger.start()
556 | |         self.logger.info("Async logging system started")
557 | |
558 | |     async def _stop_async_logging(self) -> None:
559 | |         """Stop the async logging system."""
560 | |         if self.async_logger:
561 | |             await self.async_logger.stop()
562 | |             self.async_logger = None
563 | |             self.logger.info("Async logging system stopped")
564 | |
565 | |     def _async_log(self, level: LogLevel, message: str, **kwargs: Any) -> None:
566 | |         """Log message to async logger (safe for sync contexts)."""
567 | |         if self.async_logger:
568 | |             self.async_logger.log(level, message, **kwargs)
569 | |         else:
570 | |             # Fallback to standard logger
571 | |             self.logger.log(level.level_value, message)
572 | |
573 | |     async def start_async_monitoring(self) -> bool:
574 | |         """Start monitoring with async logging enabled."""
575 | |         try:
576 | |             await self._start_async_logging()
577 | |             result = self.start_monitoring()
578 | |             if result:
579 | |                 self._async_log(
580 | |                     LogLevel.INFO,
581 | |                     "Claude monitor started with async logging",
582 | |                     session=self.session_name,
583 | |                 )
584 | |             return result
585 | |         except Exception:
586 | |             self.logger.exception("Failed to start async monitoring: {e}")
587 | |             return False
588 | |
589 | |     async def stop_async_monitoring(self) -> bool:
590 | |         """Stop monitoring and async logging."""
591 | |         try:
592 | |             result = self.stop_monitoring()
593 | |             await self._stop_async_logging()
594 | |             return result
595 | |         except Exception:
596 | |             self.logger.exception("Failed to stop async monitoring: {e}")
597 | |             return False
598 | |
599 | |     def get_async_logging_stats(self) -> dict:
600 | |         """Get async logging statistics."""
601 | |         if self.async_logger:
602 | |             return self.async_logger.get_statistics()
603 | |         return {"error": "Async logging not enabled"}
604 | |
605 | |     async def flush_async_logs(self) -> None:
606 | |         """Force flush all pending async logs."""
607 | |         if self.async_logger:
608 | |             await self.async_logger.flush()
609 | |
610 | |     @staticmethod
611 | |     def _detect_trust_prompt(content: str) -> bool:
612 | |         """Detect trust prompt in content (legacy compatibility method)."""
613 | |         # Check if content contains trust-related prompts
614 | |         trust_keywords = ["trust", "certificate", "security", "authenticate", "verify"]
615 | |         content_lower = content.lower()
616 | |         return any(keyword in content_lower for keyword in trust_keywords)
617 | |
618 | |     @staticmethod
619 | |     def _auto_trust_if_needed() -> bool:
620 | |         """Auto-respond to trust prompts if detected (legacy compatibility method)."""
621 | |         # For safety, this method returns False by default
622 | |         # Individual implementations should override based on security requirements
623 | |         return False
    | |____________________^ PLR0904
    |

libs/core/claude_monitor.py:117:13: TRY300 Consider moving this statement to an `else` block
    |
115 |             self._monitor_thread.start()
116 |
117 |             return True
    |             ^^^^^^^^^^^ TRY300
118 |
119 |         except Exception as e:
    |

libs/core/claude_monitor.py:167:9: PLR1702 Too many nested blocks (7 > 5)
    |
165 |           last_content = ""
166 |
167 | /         try:
168 | |             while self.is_running:
169 | |                 await asyncio.sleep(1)  # Check every second
170 | |
171 | |                 try:
172 | |                     content = self.session_manager.capture_pane_content()
173 | |
174 | |                     # Check if Claude is still running
175 | |                     if not self.process_controller.is_claude_running():
176 | |                         if self.is_auto_next_enabled:
177 | |                             self.status_manager.update_activity("ğŸ”„ Auto-restarting Claude...")
178 | |                             self.process_controller.restart_claude_pane()
179 | |                             continue
180 | |                         self.status_manager.update_status("[yellow]Claude not running. Auto-restart disabled.[/]")
181 | |                         continue
182 | |
183 | |                     # Check for prompts and auto-respond if enabled
184 | |                     prompt_info = self._check_for_prompt(content)
185 | |
186 | |                     if prompt_info:
187 | |                         # Try adaptive AI-powered response first if auto_next is enabled
188 | |                         if self.is_auto_next_enabled:
189 | |                             context = f"session:{self.session_name}, type:{prompt_info.type.value}"
190 | |                             (
191 | |                                 should_respond,
192 | |                                 ai_response,
193 | |                                 confidence,
194 | |                             ) = await self.adaptive_response.should_auto_respond(
195 | |                                 prompt_info.question,
196 | |                                 context,
197 | |                                 self.session_name,
198 | |                             )
199 | |
200 | |                             if should_respond:
201 | |                                 # Send AI-predicted response
202 | |                                 success = await self.adaptive_response.send_adaptive_response(
203 | |                                     prompt_info.question,
204 | |                                     ai_response,
205 | |                                     confidence,
206 | |                                     context,
207 | |                                     self.session_name,
208 | |                                 )
209 | |
210 | |                                 if success:
211 | |                                     self.process_controller.send_input(ai_response)
212 | |                                     self.status_manager.update_activity(f"ğŸ¤– AI auto-responded: '{ai_response}' (confidence: {confidenâ€¦
213 | |                                     self.status_manager.record_response(prompt_info.type.value, ai_response, content)
214 | |                                     self.adaptive_response.confirm_response_success(
215 | |                                         prompt_info.question,
216 | |                                         ai_response,
217 | |                                         context,
218 | |                                         self.session_name,
219 | |                                         True,
220 | |                                     )
221 | |                                     self._clear_prompt_state()
222 | |                                     continue
223 | |
224 | |                             # Fall back to legacy pattern-based auto-response if AI didn't handle it
225 | |                             if self._auto_respond_to_selection(prompt_info):
226 | |                                 response = self._get_legacy_response(prompt_info)
227 | |                                 self.status_manager.update_activity(f"âœ… Legacy auto-responded: '{response}' to {prompt_info.type.valuâ€¦
228 | |                                 self.status_manager.record_response(prompt_info.type.value, response, content)
229 | |                                 # Learn from legacy response for future AI improvements
230 | |                                 self.adaptive_response.learn_from_manual_response(
231 | |                                     prompt_info.question,
232 | |                                     response,
233 | |                                     context,
234 | |                                     self.session_name,
235 | |                                 )
236 | |                                 self._clear_prompt_state()
237 | |                                 continue
238 | |
239 | |                         # If auto-response didn't handle it, show waiting status
240 | |                         self.status_manager.update_activity(f"â³ Waiting for input: {prompt_info.type.value}")
241 | |                         self.logger.debug("Prompt detected: {prompt_info.type.value} - {prompt_info.question}")
242 | |                     elif self.waiting_for_input:
243 | |                         self.status_manager.update_activity("â³ Waiting for user input...")
244 | |                     else:
245 | |                         # Clear prompt state if no longer waiting
246 | |                         self._clear_prompt_state()
247 | |
248 | |                     # Periodically update AI patterns
249 | |                     await self.adaptive_response.update_patterns()
250 | |
251 | |                     # Analyze content for automation contexts
252 | |                     if content != last_content and len(content.strip()) > 0:
253 | |                         automation_contexts = self.automation_manager.analyze_content_for_context(content, self.session_name)
254 | |                         for auto_context in automation_contexts:
255 | |                             self.logger.info("Automation context detected: {auto_context.context_type.value} (confidence: {auto_conteâ€¦
256 | |
257 | |                     # Check for Claude idle automation opportunities
258 | |                     if hasattr(self.status_manager, "last_activity_time"):
259 | |                         idle_context = self.automation_manager.analyze_claude_idle(
260 | |                             self.status_manager.last_activity_time,
261 | |                             idle_threshold=60,
262 | |                         )
263 | |                         if idle_context:
264 | |                             self.logger.debug("Claude idle context: {idle_context.confidence:.2f}")
265 | |
266 | |                     # Collect content for pattern analysis
267 | |                     if content != last_content and len(content.strip()) > 0:
268 | |                         try:
269 | |                             # Convert PromptInfo to dict for collection compatibility
270 | |                             prompt_dict = None
271 | |                             if prompt_info:
272 | |                                 prompt_dict = {
273 | |                                     "type": prompt_info.type.value,
274 | |                                     "question": prompt_info.question,
275 | |                                     "options": prompt_info.options,
276 | |                                     "confidence": prompt_info.confidence,
277 | |                                 }
278 | |                             self.content_collector.collect_interaction(content, prompt_dict, None)
279 | |                         except Exception:
280 | |                             self.logger.exception("Failed to collect content: {e}")
281 | |
282 | |                     # Update activity if content changed
283 | |                     if content != last_content:
284 | |                         self.status_manager.update_activity("ğŸ“ Content updated")
285 | |                         last_content = content
286 | |
287 | |                 except Exception:
288 | |                     self.logger.exception("Error in monitoring loop: {e}")
289 | |                     await asyncio.sleep(5)  # Wait longer on errors
290 | |
291 | |         except asyncio.CancelledError:
292 | |             self.logger.info("Monitoring loop cancelled")
293 | |         except Exception:
294 | |             self.logger.exception("Monitoring loop error: {e}")
295 | |         finally:
296 | |             self.is_running = False
297 | |             self.status_manager.update_status("[red]Claude monitor stopped[/]")
    | |_______________________________________________________________________________^ PLR1702
298 |
299 |       def _check_for_prompt(self, content: str) -> PromptInfo | None:
    |

libs/core/claude_monitor.py:167:9: PLR1702 Too many nested blocks (6 > 5)
    |
165 |           last_content = ""
166 |
167 | /         try:
168 | |             while self.is_running:
169 | |                 await asyncio.sleep(1)  # Check every second
170 | |
171 | |                 try:
172 | |                     content = self.session_manager.capture_pane_content()
173 | |
174 | |                     # Check if Claude is still running
175 | |                     if not self.process_controller.is_claude_running():
176 | |                         if self.is_auto_next_enabled:
177 | |                             self.status_manager.update_activity("ğŸ”„ Auto-restarting Claude...")
178 | |                             self.process_controller.restart_claude_pane()
179 | |                             continue
180 | |                         self.status_manager.update_status("[yellow]Claude not running. Auto-restart disabled.[/]")
181 | |                         continue
182 | |
183 | |                     # Check for prompts and auto-respond if enabled
184 | |                     prompt_info = self._check_for_prompt(content)
185 | |
186 | |                     if prompt_info:
187 | |                         # Try adaptive AI-powered response first if auto_next is enabled
188 | |                         if self.is_auto_next_enabled:
189 | |                             context = f"session:{self.session_name}, type:{prompt_info.type.value}"
190 | |                             (
191 | |                                 should_respond,
192 | |                                 ai_response,
193 | |                                 confidence,
194 | |                             ) = await self.adaptive_response.should_auto_respond(
195 | |                                 prompt_info.question,
196 | |                                 context,
197 | |                                 self.session_name,
198 | |                             )
199 | |
200 | |                             if should_respond:
201 | |                                 # Send AI-predicted response
202 | |                                 success = await self.adaptive_response.send_adaptive_response(
203 | |                                     prompt_info.question,
204 | |                                     ai_response,
205 | |                                     confidence,
206 | |                                     context,
207 | |                                     self.session_name,
208 | |                                 )
209 | |
210 | |                                 if success:
211 | |                                     self.process_controller.send_input(ai_response)
212 | |                                     self.status_manager.update_activity(f"ğŸ¤– AI auto-responded: '{ai_response}' (confidence: {confidenâ€¦
213 | |                                     self.status_manager.record_response(prompt_info.type.value, ai_response, content)
214 | |                                     self.adaptive_response.confirm_response_success(
215 | |                                         prompt_info.question,
216 | |                                         ai_response,
217 | |                                         context,
218 | |                                         self.session_name,
219 | |                                         True,
220 | |                                     )
221 | |                                     self._clear_prompt_state()
222 | |                                     continue
223 | |
224 | |                             # Fall back to legacy pattern-based auto-response if AI didn't handle it
225 | |                             if self._auto_respond_to_selection(prompt_info):
226 | |                                 response = self._get_legacy_response(prompt_info)
227 | |                                 self.status_manager.update_activity(f"âœ… Legacy auto-responded: '{response}' to {prompt_info.type.valuâ€¦
228 | |                                 self.status_manager.record_response(prompt_info.type.value, response, content)
229 | |                                 # Learn from legacy response for future AI improvements
230 | |                                 self.adaptive_response.learn_from_manual_response(
231 | |                                     prompt_info.question,
232 | |                                     response,
233 | |                                     context,
234 | |                                     self.session_name,
235 | |                                 )
236 | |                                 self._clear_prompt_state()
237 | |                                 continue
238 | |
239 | |                         # If auto-response didn't handle it, show waiting status
240 | |                         self.status_manager.update_activity(f"â³ Waiting for input: {prompt_info.type.value}")
241 | |                         self.logger.debug("Prompt detected: {prompt_info.type.value} - {prompt_info.question}")
242 | |                     elif self.waiting_for_input:
243 | |                         self.status_manager.update_activity("â³ Waiting for user input...")
244 | |                     else:
245 | |                         # Clear prompt state if no longer waiting
246 | |                         self._clear_prompt_state()
247 | |
248 | |                     # Periodically update AI patterns
249 | |                     await self.adaptive_response.update_patterns()
250 | |
251 | |                     # Analyze content for automation contexts
252 | |                     if content != last_content and len(content.strip()) > 0:
253 | |                         automation_contexts = self.automation_manager.analyze_content_for_context(content, self.session_name)
254 | |                         for auto_context in automation_contexts:
255 | |                             self.logger.info("Automation context detected: {auto_context.context_type.value} (confidence: {auto_conteâ€¦
256 | |
257 | |                     # Check for Claude idle automation opportunities
258 | |                     if hasattr(self.status_manager, "last_activity_time"):
259 | |                         idle_context = self.automation_manager.analyze_claude_idle(
260 | |                             self.status_manager.last_activity_time,
261 | |                             idle_threshold=60,
262 | |                         )
263 | |                         if idle_context:
264 | |                             self.logger.debug("Claude idle context: {idle_context.confidence:.2f}")
265 | |
266 | |                     # Collect content for pattern analysis
267 | |                     if content != last_content and len(content.strip()) > 0:
268 | |                         try:
269 | |                             # Convert PromptInfo to dict for collection compatibility
270 | |                             prompt_dict = None
271 | |                             if prompt_info:
272 | |                                 prompt_dict = {
273 | |                                     "type": prompt_info.type.value,
274 | |                                     "question": prompt_info.question,
275 | |                                     "options": prompt_info.options,
276 | |                                     "confidence": prompt_info.confidence,
277 | |                                 }
278 | |                             self.content_collector.collect_interaction(content, prompt_dict, None)
279 | |                         except Exception:
280 | |                             self.logger.exception("Failed to collect content: {e}")
281 | |
282 | |                     # Update activity if content changed
283 | |                     if content != last_content:
284 | |                         self.status_manager.update_activity("ğŸ“ Content updated")
285 | |                         last_content = content
286 | |
287 | |                 except Exception:
288 | |                     self.logger.exception("Error in monitoring loop: {e}")
289 | |                     await asyncio.sleep(5)  # Wait longer on errors
290 | |
291 | |         except asyncio.CancelledError:
292 | |             self.logger.info("Monitoring loop cancelled")
293 | |         except Exception:
294 | |             self.logger.exception("Monitoring loop error: {e}")
295 | |         finally:
296 | |             self.is_running = False
297 | |             self.status_manager.update_status("[red]Claude monitor stopped[/]")
    | |_______________________________________________________________________________^ PLR1702
298 |
299 |       def _check_for_prompt(self, content: str) -> PromptInfo | None:
    |

libs/core/claude_monitor.py:167:9: PLR1702 Too many nested blocks (6 > 5)
    |
165 |           last_content = ""
166 |
167 | /         try:
168 | |             while self.is_running:
169 | |                 await asyncio.sleep(1)  # Check every second
170 | |
171 | |                 try:
172 | |                     content = self.session_manager.capture_pane_content()
173 | |
174 | |                     # Check if Claude is still running
175 | |                     if not self.process_controller.is_claude_running():
176 | |                         if self.is_auto_next_enabled:
177 | |                             self.status_manager.update_activity("ğŸ”„ Auto-restarting Claude...")
178 | |                             self.process_controller.restart_claude_pane()
179 | |                             continue
180 | |                         self.status_manager.update_status("[yellow]Claude not running. Auto-restart disabled.[/]")
181 | |                         continue
182 | |
183 | |                     # Check for prompts and auto-respond if enabled
184 | |                     prompt_info = self._check_for_prompt(content)
185 | |
186 | |                     if prompt_info:
187 | |                         # Try adaptive AI-powered response first if auto_next is enabled
188 | |                         if self.is_auto_next_enabled:
189 | |                             context = f"session:{self.session_name}, type:{prompt_info.type.value}"
190 | |                             (
191 | |                                 should_respond,
192 | |                                 ai_response,
193 | |                                 confidence,
194 | |                             ) = await self.adaptive_response.should_auto_respond(
195 | |                                 prompt_info.question,
196 | |                                 context,
197 | |                                 self.session_name,
198 | |                             )
199 | |
200 | |                             if should_respond:
201 | |                                 # Send AI-predicted response
202 | |                                 success = await self.adaptive_response.send_adaptive_response(
203 | |                                     prompt_info.question,
204 | |                                     ai_response,
205 | |                                     confidence,
206 | |                                     context,
207 | |                                     self.session_name,
208 | |                                 )
209 | |
210 | |                                 if success:
211 | |                                     self.process_controller.send_input(ai_response)
212 | |                                     self.status_manager.update_activity(f"ğŸ¤– AI auto-responded: '{ai_response}' (confidence: {confidenâ€¦
213 | |                                     self.status_manager.record_response(prompt_info.type.value, ai_response, content)
214 | |                                     self.adaptive_response.confirm_response_success(
215 | |                                         prompt_info.question,
216 | |                                         ai_response,
217 | |                                         context,
218 | |                                         self.session_name,
219 | |                                         True,
220 | |                                     )
221 | |                                     self._clear_prompt_state()
222 | |                                     continue
223 | |
224 | |                             # Fall back to legacy pattern-based auto-response if AI didn't handle it
225 | |                             if self._auto_respond_to_selection(prompt_info):
226 | |                                 response = self._get_legacy_response(prompt_info)
227 | |                                 self.status_manager.update_activity(f"âœ… Legacy auto-responded: '{response}' to {prompt_info.type.valuâ€¦
228 | |                                 self.status_manager.record_response(prompt_info.type.value, response, content)
229 | |                                 # Learn from legacy response for future AI improvements
230 | |                                 self.adaptive_response.learn_from_manual_response(
231 | |                                     prompt_info.question,
232 | |                                     response,
233 | |                                     context,
234 | |                                     self.session_name,
235 | |                                 )
236 | |                                 self._clear_prompt_state()
237 | |                                 continue
238 | |
239 | |                         # If auto-response didn't handle it, show waiting status
240 | |                         self.status_manager.update_activity(f"â³ Waiting for input: {prompt_info.type.value}")
241 | |                         self.logger.debug("Prompt detected: {prompt_info.type.value} - {prompt_info.question}")
242 | |                     elif self.waiting_for_input:
243 | |                         self.status_manager.update_activity("â³ Waiting for user input...")
244 | |                     else:
245 | |                         # Clear prompt state if no longer waiting
246 | |                         self._clear_prompt_state()
247 | |
248 | |                     # Periodically update AI patterns
249 | |                     await self.adaptive_response.update_patterns()
250 | |
251 | |                     # Analyze content for automation contexts
252 | |                     if content != last_content and len(content.strip()) > 0:
253 | |                         automation_contexts = self.automation_manager.analyze_content_for_context(content, self.session_name)
254 | |                         for auto_context in automation_contexts:
255 | |                             self.logger.info("Automation context detected: {auto_context.context_type.value} (confidence: {auto_conteâ€¦
256 | |
257 | |                     # Check for Claude idle automation opportunities
258 | |                     if hasattr(self.status_manager, "last_activity_time"):
259 | |                         idle_context = self.automation_manager.analyze_claude_idle(
260 | |                             self.status_manager.last_activity_time,
261 | |                             idle_threshold=60,
262 | |                         )
263 | |                         if idle_context:
264 | |                             self.logger.debug("Claude idle context: {idle_context.confidence:.2f}")
265 | |
266 | |                     # Collect content for pattern analysis
267 | |                     if content != last_content and len(content.strip()) > 0:
268 | |                         try:
269 | |                             # Convert PromptInfo to dict for collection compatibility
270 | |                             prompt_dict = None
271 | |                             if prompt_info:
272 | |                                 prompt_dict = {
273 | |                                     "type": prompt_info.type.value,
274 | |                                     "question": prompt_info.question,
275 | |                                     "options": prompt_info.options,
276 | |                                     "confidence": prompt_info.confidence,
277 | |                                 }
278 | |                             self.content_collector.collect_interaction(content, prompt_dict, None)
279 | |                         except Exception:
280 | |                             self.logger.exception("Failed to collect content: {e}")
281 | |
282 | |                     # Update activity if content changed
283 | |                     if content != last_content:
284 | |                         self.status_manager.update_activity("ğŸ“ Content updated")
285 | |                         last_content = content
286 | |
287 | |                 except Exception:
288 | |                     self.logger.exception("Error in monitoring loop: {e}")
289 | |                     await asyncio.sleep(5)  # Wait longer on errors
290 | |
291 | |         except asyncio.CancelledError:
292 | |             self.logger.info("Monitoring loop cancelled")
293 | |         except Exception:
294 | |             self.logger.exception("Monitoring loop error: {e}")
295 | |         finally:
296 | |             self.is_running = False
297 | |             self.status_manager.update_status("[red]Claude monitor stopped[/]")
    | |_______________________________________________________________________________^ PLR1702
298 |
299 |       def _check_for_prompt(self, content: str) -> PromptInfo | None:
    |

libs/core/claude_monitor.py:565:67: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
563 |             self.logger.info("Async logging system stopped")
564 |
565 |     def _async_log(self, level: LogLevel, message: str, **kwargs: Any) -> None:
    |                                                                   ^^^ ANN401
566 |         """Log message to async logger (safe for sync contexts)."""
567 |         if self.async_logger:
    |

libs/core/claude_monitor.py:584:13: TRY300 Consider moving this statement to an `else` block
    |
582 |                     session=self.session_name,
583 |                 )
584 |             return result
    |             ^^^^^^^^^^^^^ TRY300
585 |         except Exception:
586 |             self.logger.exception("Failed to start async monitoring: {e}")
    |

libs/core/claude_monitor.py:594:13: TRY300 Consider moving this statement to an `else` block
    |
592 |             result = self.stop_monitoring()
593 |             await self._stop_async_logging()
594 |             return result
    |             ^^^^^^^^^^^^^ TRY300
595 |         except Exception:
596 |             self.logger.exception("Failed to stop async monitoring: {e}")
    |

libs/core/claude_process_controller.py:15:24: ANN001 Missing type annotation for function argument `session_manager`
   |
13 |     """Controls Claude process lifecycle (start, stop, restart)."""
14 |
15 |     def __init__(self, session_manager, status_manager) -> None:
   |                        ^^^^^^^^^^^^^^^ ANN001
16 |         self.session_manager = session_manager
17 |         self.status_manager = status_manager
   |

libs/core/claude_process_controller.py:15:41: ANN001 Missing type annotation for function argument `status_manager`
   |
13 |     """Controls Claude process lifecycle (start, stop, restart)."""
14 |
15 |     def __init__(self, session_manager, status_manager) -> None:
   |                                         ^^^^^^^^^^^^^^ ANN001
16 |         self.session_manager = session_manager
17 |         self.status_manager = status_manager
   |

libs/core/claude_process_controller.py:47:13: TRY300 Consider moving this statement to an `else` block
   |
46 |             self.status_manager.update_status(f"[green]Claude pane restarted with {self.selected_model} model[/]")
47 |             return True
   |             ^^^^^^^^^^^ TRY300
48 |
49 |         except Exception as e:
   |

libs/core/claude_session_manager.py:67:13: TRY300 Consider moving this statement to an `else` block
   |
66 |             self.logger.info("Successfully initialized session '%s'", self.session_name)
67 |             return True
   |             ^^^^^^^^^^^ TRY300
68 |
69 |         except Exception:
   |

libs/core/config_cache.py:239:24: ANN001 Missing type annotation for function argument `base_loader`
    |
237 |     """Configuration loader with caching capabilities."""
238 |
239 |     def __init__(self, base_loader, cache_ttl: float = 300.0) -> None:
    |                        ^^^^^^^^^^^ ANN001
240 |         """Initialize cached config loader.
    |

libs/core/content_collector.py:124:13: TRY300 Consider moving this statement to an `else` block
    |
122 |             self.logger.info("Collected interaction %s - prompt: %s, response: %s", interaction["interaction_id"], prompt_info is notâ€¦
123 |
124 |             return True
    |             ^^^^^^^^^^^ TRY300
125 |
126 |         except Exception as e:
    |

libs/core/content_collector.py:169:13: TRY300 Consider moving this statement to an `else` block
    |
168 |             self.logger.debug("Collected raw content - hash: %s", content_hash)
169 |             return True
    |             ^^^^^^^^^^^ TRY300
170 |
171 |         except Exception as e:
    |

libs/core/content_collector.py:224:13: TRY300 Consider moving this statement to an `else` block
    |
222 |                 self.logger.info("Cleaned up %d old collection files", files_deleted)
223 |
224 |             return files_deleted
    |             ^^^^^^^^^^^^^^^^^^^^ TRY300
225 |
226 |         except Exception as e:
    |

libs/core/error_handling.py:133:21: SyntaxError: Expected ',', found ':'
    |
131 |             category=ErrorCategory.CONFIGURATION,
132 |             context=context,
133 |             **kwargs: Any,
    |                     ^
134 |         )
    |

libs/core/error_handling.py:133:23: SyntaxError: Positional argument cannot follow keyword argument unpacking
    |
131 |             category=ErrorCategory.CONFIGURATION,
132 |             context=context,
133 |             **kwargs: Any,
    |                       ^^^
134 |         )
    |

libs/core/error_handling.py:158:21: SyntaxError: Expected ',', found ':'
    |
156 |             category=ErrorCategory.VALIDATION,
157 |             context=context,
158 |             **kwargs: Any,
    |                     ^
159 |         )
    |

libs/core/error_handling.py:158:23: SyntaxError: Positional argument cannot follow keyword argument unpacking
    |
156 |             category=ErrorCategory.VALIDATION,
157 |             context=context,
158 |             **kwargs: Any,
    |                       ^^^
159 |         )
    |

libs/core/error_handling.py:183:21: SyntaxError: Expected ',', found ':'
    |
181 |             category=ErrorCategory.SYSTEM,
182 |             context=context,
183 |             **kwargs: Any,
    |                     ^
184 |         )
    |

libs/core/error_handling.py:183:23: SyntaxError: Positional argument cannot follow keyword argument unpacking
    |
181 |             category=ErrorCategory.SYSTEM,
182 |             context=context,
183 |             **kwargs: Any,
    |                       ^^^
184 |         )
    |

libs/core/error_handling.py:200:21: SyntaxError: Expected ',', found ':'
    |
198 |             category=ErrorCategory.NETWORK,
199 |             context=context,
200 |             **kwargs: Any,
    |                     ^
201 |         )
    |

libs/core/error_handling.py:200:23: SyntaxError: Positional argument cannot follow keyword argument unpacking
    |
198 |             category=ErrorCategory.NETWORK,
199 |             context=context,
200 |             **kwargs: Any,
    |                       ^^^
201 |         )
    |

libs/core/error_handling.py:217:21: SyntaxError: Expected ',', found ':'
    |
215 |             category=ErrorCategory.PERMISSION,
216 |             context=context,
217 |             **kwargs: Any,
    |                     ^
218 |         )
    |

libs/core/error_handling.py:217:23: SyntaxError: Positional argument cannot follow keyword argument unpacking
    |
215 |             category=ErrorCategory.PERMISSION,
216 |             context=context,
217 |             **kwargs: Any,
    |                       ^^^
218 |         )
    |

libs/core/error_handling.py:234:21: SyntaxError: Expected ',', found ':'
    |
232 |             category=ErrorCategory.TIMEOUT,
233 |             context=context,
234 |             **kwargs: Any,
    |                     ^
235 |         )
    |

libs/core/error_handling.py:234:23: SyntaxError: Positional argument cannot follow keyword argument unpacking
    |
232 |             category=ErrorCategory.TIMEOUT,
233 |             context=context,
234 |             **kwargs: Any,
    |                       ^^^
235 |         )
    |

libs/core/interfaces.py:151:56: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
149 |     @abstractmethod
150 |     @staticmethod
151 |     def create_controller(session_name: str, **kwargs: Any) -> IController:
    |                                                        ^^^ ANN401
152 |         """Create a new controller."""
    |

libs/core/session_manager.py:91:13: TRY300 Consider moving this statement to an `else` block
   |
89 |                 sessions_info.append(session_info)
90 |
91 |             return sessions_info
   |             ^^^^^^^^^^^^^^^^^^^^ TRY300
92 |
93 |         except Exception:
   |

libs/core/session_manager.py:143:13: PLR1702 Too many nested blocks (6 > 5)
    |
141 |               # Analyze progress if session is running
142 |               progress = None
143 | /             if session:
144 | |                 # Collect output from all Claude panes
145 | |                 claude_output = []
146 | |                 for window_info in windows:
147 | |                     for pane_info in window_info.panes:
148 | |                         if pane_info.is_claude and pane_info.last_output:
149 | |                             # Get full pane content for better analysis
150 | |                             try:
151 | |                                 tmux_pane = session.find_where({"pane_id": pane_info.id})
152 | |                                 if tmux_pane:
153 | |                                     pane_content = tmux_pane.cmd("capture-pane", "-p").stdout
154 | |                                     claude_output.extend(pane_content)
155 | |                             except Exception as e:
156 | |                                 self.logger.warning("Could not get pane content for %s: %s", pane_info.id, e)
157 | |
158 | |                 # Analyze progress
159 | |                 if claude_output:
160 | |                     progress = self.progress_analyzer.analyze_pane_output(session_name, claude_output)
    | |______________________________________________________________________________________________________^ PLR1702
161 |
162 |               return SessionInfo(
    |

libs/core/session_manager.py:208:9: PLR0914 Too many local variables (22/15)
    |
206 |         )
207 |
208 |     def _get_detailed_pane_info(self, pane: object) -> PaneInfo:
    |         ^^^^^^^^^^^^^^^^^^^^^^^ PLR0914
209 |         """Get detailed information for a single pane including metrics.
    |

libs/core/session_manager.py:415:13: TRY300 Consider moving this statement to an `else` block
    |
413 |               attach_cmd = f"tmux attach-session -t {session_name} \\; select-window -t {window_index} \\; select-pane -t {pane_id}"
414 |
415 | /             return {
416 | |                 "success": True,
417 | |                 "session_name": session_name,
418 | |                 "window_index": window_index,
419 | |                 "pane_id": pane_id,
420 | |                 "attach_command": attach_cmd,
421 | |                 "action": "attach",
422 | |                 "message": f"Ready to attach to pane {pane_id} in {session_name}:{window_index}",
423 | |             }
    | |_____________^ TRY300
424 |
425 |           except Exception as e:
    |

libs/core/session_manager.py:526:13: TRY300 Consider moving this statement to an `else` block
    |
525 |             self.logger.info("Created attachment script: %s", script_path)
526 |             return script_path
    |             ^^^^^^^^^^^^^^^^^^ TRY300
527 |
528 |         except Exception:
    |

libs/core/session_setup.py:91:21: TRY300 Consider moving this statement to an `else` block
   |
89 |                     click.echo(f"âœ… Created directory: {expanded_dir}")
90 |                     config_dict["start_directory"] = expanded_dir
91 |                     return True
   |                     ^^^^^^^^^^^ TRY300
92 |                 except Exception as e:
93 |                     self.validation_errors.append(f"Failed to create directory '{expanded_dir}': {e}")
   |

libs/core/session_setup.py:179:21: TRY300 Consider moving this statement to an `else` block
    |
177 |                     os.makedirs(expanded_window_dir, exist_ok=True)
178 |                     click.echo(f"âœ… Created directory: {expanded_window_dir}")
179 |                     return True
    |                     ^^^^^^^^^^^ TRY300
180 |                 except Exception as e:
181 |                     self.validation_errors.append(
    |

libs/core/session_setup.py:363:13: TRY300 Consider moving this statement to an `else` block
    |
361 |             self._create_session(config_dict)
362 |             click.echo(f"âœ… Successfully created session: {session_name}")
363 |             return True
    |             ^^^^^^^^^^^ TRY300
364 |
365 |         except CommandError as e:
    |

libs/core/session_setup_refactored.py:82:21: TRY300 Consider moving this statement to an `else` block
   |
80 |                     click.echo(f"âœ… Created directory: {expanded_dir}")
81 |                     config_dict["start_directory"] = expanded_dir
82 |                     return True
   |                     ^^^^^^^^^^^ TRY300
83 |                 except Exception as e:
84 |                     self.validation_errors.append(f"Failed to create directory '{expanded_dir}': {e}")
   |

libs/core/session_setup_refactored.py:168:21: TRY300 Consider moving this statement to an `else` block
    |
166 |                     os.makedirs(expanded_window_dir, exist_ok=True)
167 |                     click.echo(f"âœ… Created directory: {expanded_window_dir}")
168 |                     return True
    |                     ^^^^^^^^^^^ TRY300
169 |                 except Exception as e:
170 |                     self.validation_errors.append(
    |

libs/core/session_setup_refactored.py:344:13: TRY300 Consider moving this statement to an `else` block
    |
342 |             self._create_session(config_dict)
343 |             click.echo(f"âœ… Successfully created session: {session_name}")
344 |             return True
    |             ^^^^^^^^^^^ TRY300
345 |
346 |         except CommandError as e:
    |

libs/dashboard/dashboard_launcher.py:276:13: TRY300 Consider moving this statement to an `else` block
    |
274 |         try:
275 |             __import__(package)
276 |             return True
    |             ^^^^^^^^^^^ TRY300
277 |         except ImportError:
278 |             return False
    |

libs/dashboard/dashboard_launcher.py:333:17: TRY300 Consider moving this statement to an `else` block
    |
331 |                 # Rich might not have __version__ in older versions
332 |                 version = getattr(rich, "__version__", "unknown")
333 |                 return True, f"Rich {version}"
    |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
334 |             except ImportError:
335 |                 return False, "Rich not installed"
    |

libs/dashboard/health_calculator.py:781:13: TRY300 Consider moving this statement to an `else` block
    |
779 |             outdated_deps = max(1, total_deps // 10)
780 |
781 |             return total_deps, outdated_deps
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
782 |
783 |         except (OSError, json.JSONDecodeError, KeyError):
    |

libs/dashboard/health_calculator.py:797:13: TRY300 Consider moving this statement to an `else` block
    |
795 |             outdated_deps = max(1, total_deps // 7)
796 |
797 |             return total_deps, outdated_deps
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
798 |
799 |         except (OSError, UnicodeDecodeError):
    |

libs/dashboard/health_calculator.py:861:13: TRY300 Consider moving this statement to an `else` block
    |
859 |             outdated_deps = max(1, deps_count // 10)
860 |
861 |             return deps_count, outdated_deps
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
862 |
863 |         except (OSError, UnicodeDecodeError):
    |

libs/dashboard/health_calculator.py:868:9: PLR1702 Too many nested blocks (6 > 5)
    |
866 |       async def _assess_code_documentation(self) -> int:
867 |           """Assess code documentation coverage."""
868 | /         try:
869 | |             # Look for common source files
870 | |             source_patterns = ["**/*.py", "**/*.js", "**/*.ts", "**/*.rs", "**/*.go"]
871 | |
872 | |             total_files = 0
873 | |             documented_files = 0
874 | |
875 | |             for pattern in source_patterns:
876 | |                 for file_path in self.project_path.glob(pattern):
877 | |                     if file_path.is_file() and "node_modules" not in str(file_path):
878 | |                         total_files += 1
879 | |                         try:
880 | |                             content = file_path.read_text()
881 | |                             # Simple heuristic: look for docstrings/comments
882 | |                             if (
883 | |                                 '"""' in content
884 | |                                 or "'''" in content  # Python docstrings
885 | |                                 or "/**" in content  # JS/TS JSDoc
886 | |                                 or "///" in content  # Rust doc comments
887 | |                                 or "// " in content
888 | |                             ):  # General comments
889 | |                                 documented_files += 1
890 | |                         except (OSError, UnicodeDecodeError) as e:
891 | |                             self.logger.warning("Could not parse file %s: %s", file_path, e)
892 | |                             continue
893 | |
894 | |             if total_files > 0:
895 | |                 doc_ratio = documented_files / total_files
896 | |                 return int(doc_ratio * 100)
897 | |             return 80  # Default if no source files found
898 | |
899 | |         except (OSError, ValueError):
900 | |             return 50  # Default on error
    | |_____________________^ PLR1702
901 |
902 |       @staticmethod
    |

libs/dashboard/health_calculator.py:897:13: TRY300 Consider moving this statement to an `else` block
    |
895 |                 doc_ratio = documented_files / total_files
896 |                 return int(doc_ratio * 100)
897 |             return 80  # Default if no source files found
    |             ^^^^^^^^^ TRY300
898 |
899 |         except (OSError, ValueError):
    |

libs/dashboard/keyboard_navigation.py:146:1: PLR0904 Too many public methods (30 > 20)
    |
146 | / class KeyboardNavigationManager:
147 | |     """Universal keyboard navigation manager.
148 | |
149 | |     Provides consistent keyboard navigation across all dashboard interfaces
150 | |     with context-aware bindings, focus management, and accessibility support.
151 | |     """
152 | |
153 | |     _instance: Optional["KeyboardNavigationManager"] = None
154 | |
155 | |     def __init__(self) -> None:
156 | |         """Initialize keyboard navigation manager."""
157 | |         if KeyboardNavigationManager._instance is not None:
158 | |             msg = "KeyboardNavigationManager is a singleton, use get_instance()"
159 | |             raise RuntimeError(msg)
160 | |         self.bindings: dict[str, list[KeyBinding]] = {}
161 | |         self.actions: dict[str, Callable] = {}
162 | |         self.focusable_elements: list[FocusableElement] = []
163 | |         self.current_focus_index: int = -1
164 | |         self.current_context: NavigationContext = NavigationContext.GLOBAL
165 | |         self.vim_mode_enabled: bool = False
166 | |         self.vim_mode: NavigationContext = NavigationContext.VIM_NORMAL
167 | |
168 | |         # Initialize default bindings
169 | |         self._setup_default_bindings()
170 | |
171 | |     def _setup_default_bindings(self) -> None:
172 | |         """Setup default keyboard bindings."""
173 | |         # Global navigation
174 | |         self.register_binding("tab", [], "focus_next", "Next element", NavigationContext.GLOBAL)
175 | |         self.register_binding(
176 | |             "tab",
177 | |             [KeyModifier.SHIFT],
178 | |             "focus_prev",
179 | |             "Previous element",
180 | |             NavigationContext.GLOBAL,
181 | |         )
182 | |         self.register_binding("enter", [], "activate", "Activate element", NavigationContext.GLOBAL)
183 | |         self.register_binding("escape", [], "cancel", "Cancel/Close", NavigationContext.GLOBAL)
184 | |
185 | |         # Common shortcuts
186 | |         self.register_binding("r", [KeyModifier.CTRL], "refresh", "Refresh", NavigationContext.GLOBAL)
187 | |         self.register_binding("f", [KeyModifier.CTRL], "find", "Find", NavigationContext.GLOBAL)
188 | |         self.register_binding("s", [KeyModifier.CTRL], "save", "Save", NavigationContext.GLOBAL)
189 | |         self.register_binding("z", [KeyModifier.CTRL], "undo", "Undo", NavigationContext.GLOBAL)
190 | |         self.register_binding("y", [KeyModifier.CTRL], "redo", "Redo", NavigationContext.GLOBAL)
191 | |
192 | |         # Dashboard navigation
193 | |         self.register_binding("1", [], "switch_view_1", "View 1", NavigationContext.DASHBOARD)
194 | |         self.register_binding("2", [], "switch_view_2", "View 2", NavigationContext.DASHBOARD)
195 | |         self.register_binding("3", [], "switch_view_3", "View 3", NavigationContext.DASHBOARD)
196 | |         self.register_binding("4", [], "switch_view_4", "View 4", NavigationContext.DASHBOARD)
197 | |         self.register_binding("5", [], "switch_view_5", "View 5", NavigationContext.DASHBOARD)
198 | |
199 | |         # Arrow key navigation
200 | |         self.register_binding("arrowup", [], "navigate_up", "Navigate up", NavigationContext.GLOBAL)
201 | |         self.register_binding("arrowdown", [], "navigate_down", "Navigate down", NavigationContext.GLOBAL)
202 | |         self.register_binding("arrowleft", [], "navigate_left", "Navigate left", NavigationContext.GLOBAL)
203 | |         self.register_binding(
204 | |             "arrowright",
205 | |             [],
206 | |             "navigate_right",
207 | |             "Navigate right",
208 | |             NavigationContext.GLOBAL,
209 | |         )
210 | |
211 | |         # Vim mode bindings
212 | |         self.register_binding(":", [], "vim_command_mode", "Command mode", NavigationContext.VIM_NORMAL)
213 | |         self.register_binding("i", [], "vim_insert_mode", "Insert mode", NavigationContext.VIM_NORMAL)
214 | |         self.register_binding("v", [], "vim_visual_mode", "Visual mode", NavigationContext.VIM_NORMAL)
215 | |         self.register_binding("h", [], "vim_left", "Left", NavigationContext.VIM_NORMAL)
216 | |         self.register_binding("j", [], "vim_down", "Down", NavigationContext.VIM_NORMAL)
217 | |         self.register_binding("k", [], "vim_up", "Up", NavigationContext.VIM_NORMAL)
218 | |         self.register_binding("l", [], "vim_right", "Right", NavigationContext.VIM_NORMAL)
219 | |         self.register_binding("escape", [], "vim_normal_mode", "Normal mode", NavigationContext.VIM_INSERT)
220 | |
221 | |         # Register default actions
222 | |         self._setup_default_actions()
223 | |
224 | |     def _setup_default_actions(self) -> None:
225 | |         """Setup default action handlers."""
226 | |         self.register_action("focus_next", self.focus_next)
227 | |         self.register_action("focus_prev", self.focus_prev)
228 | |         self.register_action("activate", self.activate_element)
229 | |         self.register_action("cancel", self.cancel_action)
230 | |         self.register_action("refresh", self.refresh_action)
231 | |         self.register_action("find", self.find_action)
232 | |         self.register_action("navigate_up", lambda: self.navigate_direction("up"))
233 | |         self.register_action("navigate_down", lambda: self.navigate_direction("down"))
234 | |         self.register_action("navigate_left", lambda: self.navigate_direction("left"))
235 | |         self.register_action("navigate_right", lambda: self.navigate_direction("right"))
236 | |
237 | |         # Vim mode actions
238 | |         self.register_action("vim_command_mode", self.vim_command_mode)
239 | |         self.register_action("vim_insert_mode", self.vim_insert_mode)
240 | |         self.register_action("vim_visual_mode", self.vim_visual_mode)
241 | |         self.register_action("vim_normal_mode", self.vim_normal_mode)
242 | |         self.register_action("vim_left", lambda: self.navigate_direction("left"))
243 | |         self.register_action("vim_down", lambda: self.navigate_direction("down"))
244 | |         self.register_action("vim_up", lambda: self.navigate_direction("up"))
245 | |         self.register_action("vim_right", lambda: self.navigate_direction("right"))
246 | |
247 | |     @classmethod
248 | |     def get_instance(cls) -> "KeyboardNavigationManager":
249 | |         """Get the singleton instance of the keyboard manager."""
250 | |         if cls._instance is None:
251 | |             cls._instance = KeyboardNavigationManager()
252 | |         return cls._instance
253 | |
254 | |     @classmethod
255 | |     def reset_instance(cls) -> None:
256 | |         """Reset the singleton instance."""
257 | |         cls._instance = None
258 | |
259 | |     def register_binding(
260 | |         self,
261 | |         key: str,
262 | |         modifiers: list[KeyModifier],
263 | |         action: str,
264 | |         description: str = "",
265 | |         context: NavigationContext | None = None,
266 | |         priority: int = 0,
267 | |     ) -> None:
268 | |         """Register a keyboard binding.
269 | |
270 | |         Args:
271 | |             key: Key name (e.g., 'a', 'enter', 'arrowup')
272 | |             modifiers: List of modifier keys
273 | |             action: Action name to execute
274 | |             description: Human-readable description
275 | |             context: Context where binding is active
276 | |             priority: Binding priority (higher takes precedence)
277 | |         """
278 | |         binding = KeyBinding(
279 | |             key=key,
280 | |             modifiers=modifiers,
281 | |             action=action,
282 | |             description=description,
283 | |             context=context,
284 | |             priority=priority,
285 | |         )
286 | |
287 | |         key_combo = binding.key_combination
288 | |         if key_combo not in self.bindings:
289 | |             self.bindings[key_combo] = []
290 | |
291 | |         # Insert binding in priority order (highest first)
292 | |         self.bindings[key_combo].append(binding)
293 | |         self.bindings[key_combo].sort(key=lambda b: b.priority, reverse=True)
294 | |
295 | |         logger.debug("Registered key binding: %s -> %s", key_combo, action)
296 | |
297 | |     def unregister_binding(
298 | |         self,
299 | |         key: str,
300 | |         modifiers: list[KeyModifier],
301 | |         context: NavigationContext | None = None,
302 | |     ) -> bool:
303 | |         """Unregister a keyboard binding.
304 | |
305 | |         Args:
306 | |             key: Key name
307 | |             modifiers: List of modifier keys
308 | |             context: Context to remove binding from (None for all)
309 | |
310 | |         Returns:
311 | |             True if binding was removed, False otherwise
312 | |         """
313 | |         key_combo = "+".join([mod.value for mod in modifiers] + [key.lower()])
314 | |
315 | |         if key_combo not in self.bindings:
316 | |             return False
317 | |
318 | |         original_count = len(self.bindings[key_combo])
319 | |
320 | |         if context is None:
321 | |             # Remove all bindings for this key combination
322 | |             del self.bindings[key_combo]
323 | |         else:
324 | |             # Remove only bindings for specific context
325 | |             self.bindings[key_combo] = [b for b in self.bindings[key_combo] if b.context != context]
326 | |
327 | |             # Clean up empty key combination
328 | |             if not self.bindings[key_combo]:
329 | |                 del self.bindings[key_combo]
330 | |
331 | |         removed_count = original_count - len(self.bindings.get(key_combo, []))
332 | |         logger.debug("Removed %s bindings for %s", removed_count, key_combo)
333 | |         return removed_count > 0
334 | |
335 | |     def register_action(self, action_name: str, handler: Callable) -> None:
336 | |         """Register an action handler.
337 | |
338 | |         Args:
339 | |             action_name: Name of the action
340 | |             handler: Function to call when action is triggered
341 | |         """
342 | |         self.actions[action_name] = handler
343 | |         logger.debug("Registered action: %s", action_name)
344 | |
345 | |     def unregister_action(self, action_name: str) -> bool:
346 | |         """Unregister an action handler.
347 | |
348 | |         Args:
349 | |             action_name: Name of the action to remove
350 | |
351 | |         Returns:
352 | |             True if action was removed, False otherwise
353 | |         """
354 | |         if action_name in self.actions:
355 | |             del self.actions[action_name]
356 | |             logger.debug("Unregistered action: %s", action_name)
357 | |             return True
358 | |         return False
359 | |
360 | |     def handle_key_event(
361 | |         self,
362 | |         key: str,
363 | |         modifiers: list[KeyModifier],
364 | |         context: NavigationContext | None = None,
365 | |     ) -> bool:
366 | |         """Handle a keyboard event.
367 | |
368 | |         Args:
369 | |             key: Key that was pressed
370 | |             modifiers: Active modifier keys
371 | |             context: Current context (uses self.current_context if None)
372 | |
373 | |         Returns:
374 | |             True if event was handled, False otherwise
375 | |         """
376 | |         if context is None:
377 | |             context = self.current_context
378 | |
379 | |         # Special handling for Vim mode
380 | |         if self.vim_mode_enabled and context in {
381 | |             NavigationContext.VIM_NORMAL,
382 | |             NavigationContext.VIM_INSERT,
383 | |             NavigationContext.VIM_VISUAL,
384 | |         }:
385 | |             context = self.vim_mode
386 | |
387 | |         key_combo = "+".join([mod.value for mod in modifiers] + [key.lower()])
388 | |
389 | |         if key_combo not in self.bindings:
390 | |             logger.debug("No bindings found for %s", key_combo)
391 | |             return False
392 | |
393 | |         # Find best matching binding based on context and priority
394 | |         best_binding = None
395 | |
396 | |         for binding in self.bindings[key_combo]:
397 | |             if not binding.enabled:
398 | |                 continue
399 | |
400 | |             # Check context match
401 | |             if (binding.context is None or binding.context in {context, NavigationContext.GLOBAL}) and (best_binding is None or bindiâ€¦
402 | |                 best_binding = binding
403 | |
404 | |         if best_binding is None:
405 | |             logger.debug("No matching binding for %s in context %s", key_combo, context)
406 | |             return False
407 | |
408 | |         # Execute action
409 | |         return self.execute_action(best_binding.action)
410 | |
411 | |     def execute_action(self, action_name: str, *args: Any, **kwargs: Any) -> bool:
412 | |         """Execute a registered action.
413 | |
414 | |         Args:
415 | |             action_name: Name of the action to execute
416 | |             *args, **kwargs: Arguments to pass to action handler
417 | |
418 | |         Returns:
419 | |             True if action was executed, False otherwise
420 | |         """
421 | |         if action_name not in self.actions:
422 | |             logger.warning("Unknown action: %s", action_name)
423 | |             return False
424 | |
425 | |         try:
426 | |             handler = self.actions[action_name]
427 | |
428 | |             # Handle async actions
429 | |             if asyncio.iscoroutinefunction(handler):
430 | |                 asyncio.create_task(handler(*args, **kwargs))
431 | |             else:
432 | |                 handler(*args, **kwargs)
433 | |
434 | |             logger.debug("Executed action: %s", action_name)
435 | |             return True
436 | |
437 | |         except Exception as e:
438 | |             logger.exception("Error executing action %s")
439 | |             return False
440 | |
441 | |     def set_context(self, context: NavigationContext) -> None:
442 | |         """Set the current navigation context."""
443 | |         self.current_context = context
444 | |         logger.debug("Navigation context changed to: %s", context)
445 | |
446 | |     def add_focusable_element(
447 | |         self,
448 | |         element_id: str,
449 | |         element_type: str = "generic",
450 | |         tab_index: int = 0,
451 | |         context: NavigationContext | None = None,
452 | |     ) -> None:
453 | |         """Add a focusable element to the navigation system.
454 | |
455 | |         Args:
456 | |             element_id: Unique identifier for the element
457 | |             element_type: Type of element (button, input, etc.)
458 | |             tab_index: Tab order index
459 | |             context: Context where element is focusable
460 | |         """
461 | |         element = FocusableElement(
462 | |             element_id=element_id,
463 | |             element_type=element_type,
464 | |             tab_index=tab_index,
465 | |             context=context,
466 | |         )
467 | |
468 | |         # Insert in tab order
469 | |         insert_index = len(self.focusable_elements)
470 | |         for i, existing in enumerate(self.focusable_elements):
471 | |             if existing.tab_index > tab_index:
472 | |                 insert_index = i
473 | |                 break
474 | |
475 | |         self.focusable_elements.insert(insert_index, element)
476 | |         logger.debug("Added focusable element: %s", element_id)
477 | |
478 | |     def remove_focusable_element(self, element_id: str) -> bool:
479 | |         """Remove a focusable element.
480 | |
481 | |         Args:
482 | |             element_id: ID of element to remove
483 | |
484 | |         Returns:
485 | |             True if element was removed, False otherwise
486 | |         """
487 | |         for i, element in enumerate(self.focusable_elements):
488 | |             if element.element_id == element_id:
489 | |                 self.focusable_elements.pop(i)
490 | |
491 | |                 # Adjust focus index if necessary
492 | |                 if i <= self.current_focus_index:
493 | |                     self.current_focus_index -= 1
494 | |
495 | |                 logger.debug("Removed focusable element: %s", element_id)
496 | |                 return True
497 | |
498 | |         return False
499 | |
500 | |     def focus_next(self) -> bool:
501 | |         """Focus the next element in tab order."""
502 | |         if not self.focusable_elements:
503 | |             return False
504 | |
505 | |         start_index = self.current_focus_index
506 | |         next_index = (start_index + 1) % len(self.focusable_elements)
507 | |
508 | |         # Find next enabled element in current context
509 | |         attempts = 0
510 | |         while attempts < len(self.focusable_elements):
511 | |             element = self.focusable_elements[next_index]
512 | |
513 | |             if element.enabled and (element.context is None or element.context == self.current_context):
514 | |                 self.current_focus_index = next_index
515 | |                 self._focus_element(element)
516 | |                 return True
517 | |
518 | |             next_index = (next_index + 1) % len(self.focusable_elements)
519 | |             attempts += 1
520 | |
521 | |         return False
522 | |
523 | |     def focus_prev(self) -> bool:
524 | |         """Focus the previous element in tab order."""
525 | |         if not self.focusable_elements:
526 | |             return False
527 | |
528 | |         start_index = self.current_focus_index
529 | |         prev_index = (start_index - 1) % len(self.focusable_elements)
530 | |
531 | |         # Find previous enabled element in current context
532 | |         attempts = 0
533 | |         while attempts < len(self.focusable_elements):
534 | |             element = self.focusable_elements[prev_index]
535 | |
536 | |             if element.enabled and (element.context is None or element.context == self.current_context):
537 | |                 self.current_focus_index = prev_index
538 | |                 self._focus_element(element)
539 | |                 return True
540 | |
541 | |             prev_index = (prev_index - 1) % len(self.focusable_elements)
542 | |             attempts += 1
543 | |
544 | |         return False
545 | |
546 | |     def focus_element(self, element_id: str) -> bool:
547 | |         """Focus a specific element by ID.
548 | |
549 | |         Args:
550 | |             element_id: ID of element to focus
551 | |
552 | |         Returns:
553 | |             True if element was focused, False otherwise
554 | |         """
555 | |         for i, element in enumerate(self.focusable_elements):
556 | |             if element.element_id == element_id and element.enabled:
557 | |                 self.current_focus_index = i
558 | |                 self._focus_element(element)
559 | |                 return True
560 | |
561 | |         return False
562 | |
563 | |     @staticmethod
564 | |     def _focus_element(element: FocusableElement) -> None:
565 | |         """Internal method to focus an element."""
566 | |         logger.debug("Focusing element: %s", element.element_id)
567 | |         # This would be implemented by subclasses or interface-specific handlers
568 | |
569 | |     def get_current_focus(self) -> FocusableElement | None:
570 | |         """Get the currently focused element."""
571 | |         if 0 <= self.current_focus_index < len(self.focusable_elements):
572 | |             return self.focusable_elements[self.current_focus_index]
573 | |         return None
574 | |
575 | |     # Default action implementations
576 | |
577 | |     def activate_element(self) -> None:
578 | |         """Activate the currently focused element."""
579 | |         element = self.get_current_focus()
580 | |         if element:
581 | |             logger.debug("Activating element: %s", element.element_id)
582 | |
583 | |     @staticmethod
584 | |     def cancel_action() -> None:
585 | |         """Handle cancel/escape action."""
586 | |         logger.debug("Cancel action triggered")
587 | |
588 | |     @staticmethod
589 | |     def refresh_action() -> None:
590 | |         """Handle refresh action."""
591 | |         logger.debug("Refresh action triggered")
592 | |
593 | |     @staticmethod
594 | |     def find_action() -> None:
595 | |         """Handle find action."""
596 | |         logger.debug("Find action triggered")
597 | |
598 | |     @staticmethod
599 | |     def navigate_direction(direction: str) -> None:
600 | |         """Handle directional navigation."""
601 | |         logger.debug("Navigate %s", direction)
602 | |
603 | |     # Vim mode methods
604 | |
605 | |     def enable_vim_mode(self) -> None:
606 | |         """Enable Vim-style keyboard navigation."""
607 | |         self.vim_mode_enabled = True
608 | |         self.vim_mode = NavigationContext.VIM_NORMAL
609 | |         logger.debug("Vim mode enabled")
610 | |
611 | |     def disable_vim_mode(self) -> None:
612 | |         """Disable Vim-style keyboard navigation."""
613 | |         self.vim_mode_enabled = False
614 | |         logger.debug("Vim mode disabled")
615 | |
616 | |     def vim_command_mode(self) -> None:
617 | |         """Enter Vim command mode."""
618 | |         if self.vim_mode_enabled:
619 | |             logger.debug("Vim command mode")
620 | |
621 | |     def vim_insert_mode(self) -> None:
622 | |         """Enter Vim insert mode."""
623 | |         if self.vim_mode_enabled:
624 | |             self.vim_mode = NavigationContext.VIM_INSERT
625 | |             logger.debug("Vim insert mode")
626 | |
627 | |     def vim_visual_mode(self) -> None:
628 | |         """Enter Vim visual mode."""
629 | |         if self.vim_mode_enabled:
630 | |             self.vim_mode = NavigationContext.VIM_VISUAL
631 | |             logger.debug("Vim visual mode")
632 | |
633 | |     def vim_normal_mode(self) -> None:
634 | |         """Enter Vim normal mode."""
635 | |         if self.vim_mode_enabled:
636 | |             self.vim_mode = NavigationContext.VIM_NORMAL
637 | |             logger.debug("Vim normal mode")
638 | |
639 | |     # Serialization methods
640 | |
641 | |     def export_bindings(self) -> dict[str, object]:
642 | |         """Export all key bindings to dictionary."""
643 | |         exported = {}
644 | |         for key_combo, bindings in self.bindings.items():
645 | |             exported[key_combo] = [binding.to_dict() for binding in bindings]
646 | |         return exported
647 | |
648 | |     def import_bindings(self, bindings_data: dict[str, object]) -> None:
649 | |         """Import key bindings from dictionary."""
650 | |         self.bindings.clear()
651 | |
652 | |         for key_combo, binding_list in bindings_data.items():
653 | |             self.bindings[key_combo] = [KeyBinding.from_dict(binding_data) for binding_data in binding_list]
654 | |
655 | |     def get_help_text(self, context: NavigationContext | None = None) -> list[str]:
656 | |         """Get help text for current context."""
657 | |         help_lines = []
658 | |         processed_actions = set()
659 | |
660 | |         valid_contexts = {None, context, NavigationContext.GLOBAL}
661 | |
662 | |         for bindings in self.bindings.values():
663 | |             for binding in bindings:
664 | |                 if binding.context in valid_contexts and binding.action not in processed_actions:
665 | |                     help_lines.append(f"{binding.key_combination:<20} {binding.description}")
666 | |                     processed_actions.add(binding.action)
667 | |
668 | |         return help_lines
    | |_________________________^ PLR0904
    |

libs/dashboard/keyboard_navigation.py:259:9: PLR0917 Too many positional arguments (6/5)
    |
257 |         cls._instance = None
258 |
259 |     def register_binding(
    |         ^^^^^^^^^^^^^^^^ PLR0917
260 |         self,
261 |         key: str,
    |

libs/dashboard/keyboard_navigation.py:411:55: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `*args`
    |
409 |         return self.execute_action(best_binding.action)
410 |
411 |     def execute_action(self, action_name: str, *args: Any, **kwargs: Any) -> bool:
    |                                                       ^^^ ANN401
412 |         """Execute a registered action.
    |

libs/dashboard/keyboard_navigation.py:411:70: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
409 |         return self.execute_action(best_binding.action)
410 |
411 |     def execute_action(self, action_name: str, *args: Any, **kwargs: Any) -> bool:
    |                                                                      ^^^ ANN401
412 |         """Execute a registered action.
    |

libs/dashboard/keyboard_navigation.py:435:13: TRY300 Consider moving this statement to an `else` block
    |
434 |             logger.debug("Executed action: %s", action_name)
435 |             return True
    |             ^^^^^^^^^^^ TRY300
436 |
437 |         except Exception as e:
    |

libs/dashboard/keyboard_navigation.py:489:17: B909 Mutation to loop iterable `self.focusable_elements` during iteration
    |
487 |         for i, element in enumerate(self.focusable_elements):
488 |             if element.element_id == element_id:
489 |                 self.focusable_elements.pop(i)
    |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^ B909
490 |
491 |                 # Adjust focus index if necessary
    |

libs/dashboard/performance_optimizer.py:174:32: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `*args`
    |
173 |             @wraps(func)
174 |             def wrapper(*args: Any, **kwargs: Any) -> Any:
    |                                ^^^ ANN401
175 |                 with self.measure(name):
176 |                     return func(*args, **kwargs)
    |

libs/dashboard/performance_optimizer.py:174:47: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
173 |             @wraps(func)
174 |             def wrapper(*args: Any, **kwargs: Any) -> Any:
    |                                               ^^^ ANN401
175 |                 with self.measure(name):
176 |                     return func(*args, **kwargs)
    |

libs/dashboard/performance_optimizer.py:174:55: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `wrapper`
    |
173 |             @wraps(func)
174 |             def wrapper(*args: Any, **kwargs: Any) -> Any:
    |                                                       ^^^ ANN401
175 |                 with self.measure(name):
176 |                     return func(*args, **kwargs)
    |

libs/dashboard/performance_optimizer.py:524:27: ARG004 Unused static method argument: `strategy`
    |
523 |     @staticmethod
524 |     def _optimize_caching(strategy: OptimizationStrategy) -> None:  # noqa: ARG002  # noqa: ARG004
    |                           ^^^^^^^^ ARG004
525 |         """Optimize caching behavior."""
526 |         # This would integrate with actual cache systems
    |

libs/dashboard/performance_optimizer.py:536:30: ARG004 Unused static method argument: `strategy`
    |
535 |     @staticmethod
536 |     def _optimize_animations(strategy: OptimizationStrategy) -> None:  # noqa: ARG002  # noqa: ARG004
    |                              ^^^^^^^^ ARG004
537 |         """Optimize animations."""
538 |         # This would integrate with animation system
    |

libs/dashboard/renderers/optimizations.py:225:13: ANN202 Missing return type annotation for private function `wrapper`
    |
223 |     def decorator(func: Callable) -> Callable:
224 |         @wraps(func)
225 |         def wrapper(
    |             ^^^^^^^ ANN202
226 |             self: object | None,
227 |             widget_type: WidgetType,
    |
    = help: Add return type annotation

libs/dashboard/renderers/optimizations.py:261:13: ANN202 Missing return type annotation for private function `wrapper`
    |
259 |     def decorator(func: Callable) -> Callable:
260 |         @wraps(func)
261 |         def wrapper(
    |             ^^^^^^^ ANN202
262 |             self: object | None,
263 |             widgets: list[dict[str, str | int | float | bool | object]],
    |
    = help: Add return type annotation

libs/dashboard/renderers/optimizations.py:435:17: TRY300 Consider moving this statement to an `else` block
    |
433 |             try:
434 |                 result = self.renderer.render_widget(widget_type, data, options or {})
435 |                 return index, result
    |                 ^^^^^^^^^^^^^^^^^^^^ TRY300
436 |             except Exception as e:
437 |                 return index, f"Error: {e}"
    |

libs/dashboard/renderers/optimizations.py:563:28: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `*args`
    |
561 |     def decorator(func: Callable) -> Callable:
562 |         @wraps(func)
563 |         def wrapper(*args: Any, **kwargs: Any) -> object:
    |                            ^^^ ANN401
564 |             name = operation_name or f"{func.__module__}.{func.__name__}"
565 |             with global_profiler.time_operation(name):
    |

libs/dashboard/renderers/optimizations.py:563:43: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
561 |     def decorator(func: Callable) -> Callable:
562 |         @wraps(func)
563 |         def wrapper(*args: Any, **kwargs: Any) -> object:
    |                                           ^^^ ANN401
564 |             name = operation_name or f"{func.__module__}.{func.__name__}"
565 |             with global_profiler.time_operation(name):
    |

libs/dashboard/renderers/registry.py:39:13: TRY004 Prefer `TypeError` exception for invalid type
   |
37 |         if not issubclass(renderer_class, BaseRenderer):
38 |             msg = f"Renderer class must be a subclass of BaseRenderer, got {renderer_class}"
39 |             raise ValueError(msg)
   |             ^^^^^^^^^^^^^^^^^^^^^ TRY004
40 |
41 |         self._renderers[format_type] = renderer_class
   |

libs/dashboard/renderers/registry.py:74:65: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
72 |         return self._renderers.get(format_type)
73 |
74 |     def get_renderer(self, format_type: RenderFormat, **kwargs: Any) -> BaseRenderer | None:
   |                                                                 ^^^ ANN401
75 |         """Get a renderer instance for a specific format.
   |

libs/dashboard/renderers/registry.py:101:46: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
 99 |         return instance
100 |
101 |     def get_default_renderer(self, **kwargs: Any) -> BaseRenderer | None:
    |                                              ^^^ ANN401
102 |         """Get the default renderer instance.
    |

libs/dashboard/renderers/registry.py:192:55: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
192 | def get_renderer(format_type: RenderFormat, **kwargs: Any) -> BaseRenderer | None:
    |                                                       ^^^ ANN401
193 |     """Convenience function to get a renderer from global registry.
    |

libs/dashboard/renderers/registry.py:205:36: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
205 | def get_default_renderer(**kwargs: Any) -> BaseRenderer | None:
    |                                    ^^^ ANN401
206 |     """Convenience function to get default renderer from global registry.
    |

libs/dashboard/renderers/renderer_factory.py:103:60: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
102 |     @classmethod
103 |     def create(cls, render_format: RenderFormat, **kwargs: Any) -> BaseRenderer:
    |                                                            ^^^ ANN401
104 |         """Create a renderer instance for the specified format.
    |

libs/dashboard/renderers/renderer_factory.py:136:67: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
135 |     @classmethod
136 |     def get_singleton(cls, render_format: RenderFormat, **kwargs: Any) -> BaseRenderer:
    |                                                                   ^^^ ANN401
137 |         """Get or create a singleton renderer instance.
    |

libs/dashboard/renderers/renderer_factory.py:236:17: TRY300 Consider moving this statement to an `else` block
    |
234 |                 renderer = cls.get_singleton(fmt)
235 |                 result = renderer.render_widget(widget_type, data, options)
236 |                 return fmt, result, None
    |                 ^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
237 |             except Exception as e:
238 |                 return fmt, None, str(e)
    |

libs/dashboard/renderers/renderer_factory.py:348:60: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
348 | def create_renderer(render_format: RenderFormat, **kwargs: Any) -> BaseRenderer:
    |                                                            ^^^ ANN401
349 |     """Create a new renderer instance.
    |

libs/dashboard/renderers/renderer_factory.py:361:57: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
361 | def get_renderer(render_format: RenderFormat, **kwargs: Any) -> BaseRenderer:
    |                                                         ^^^ ANN401
362 |     """Get singleton renderer instance.
    |

libs/dashboard/renderers/tauri_renderer.py:648:65: ARG004 Unused static method argument: `options`
    |
647 |     @staticmethod
648 |     def _process_chart(widget_json: dict[str], data: ChartData, options: dict[str]) -> None:  # noqa: ARG002  # noqa: ARG004
    |                                                                 ^^^^^^^ ARG004
649 |         """Process chart widget."""
650 |         if not isinstance(data, ChartData):
    |

libs/dashboard/renderers/tauri_renderer.py:698:65: ARG004 Unused static method argument: `options`
    |
697 |     @staticmethod
698 |     def _process_table(widget_json: dict[str], data: dict[str], options: dict[str]) -> None:  # noqa: ARG002  # noqa: ARG004
    |                                                                 ^^^^^^^ ARG004
699 |         """Process table widget."""
700 |         rows = data.get("rows", []) if isinstance(data, dict) else []
    |

libs/dashboard/renderers/tui_renderer.py:648:69: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `data`
    |
646 |         return capture.get()
647 |
648 |     def _render_generic_widget(self, widget_type: WidgetType, data: Any, options: dict[str, Any]) -> str:  # noqa: ARG002
    |                                                                     ^^^ ANN401
649 |         """Render generic widget fallback."""
650 |         content = Text()
    |

libs/dashboard/renderers/web_renderer.py:891:9: ARG004 Unused static method argument: `options`
    |
889 |         widget_type: WidgetType,
890 |         data: object,
891 |         options: dict[str, Any],  # noqa: ARG002
    |         ^^^^^^^ ARG004
892 |         component_id: str,
893 |     ) -> str:
    |

libs/dashboard/tui_dashboard.py:59:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
57 |         title: str = "",
58 |         update_interval: float = 2.0,
59 |         **kwargs: Any,
   |                   ^^^ ANN401
60 |     ) -> None:
61 |         """Initialize dashboard widget.
   |

libs/dashboard/tui_dashboard.py:135:34: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
133 |     """Sessions monitoring view."""
134 |
135 |     def __init__(self, **kwargs: Any) -> None:
    |                                  ^^^ ANN401
136 |         super().__init__(
137 |             widget_type=WidgetType.SESSION_BROWSER,
    |

libs/dashboard/tui_dashboard.py:170:34: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
168 |     """Project health monitoring view."""
169 |
170 |     def __init__(self, **kwargs: Any) -> None:
    |                                  ^^^ ANN401
171 |         super().__init__(
172 |             widget_type=WidgetType.PROJECT_HEALTH,
    |

libs/dashboard/tui_dashboard.py:203:34: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
201 |     """Activity monitoring view."""
202 |
203 |     def __init__(self, **kwargs: Any) -> None:
    |                                  ^^^ ANN401
204 |         super().__init__(
205 |             widget_type=WidgetType.ACTIVITY_HEATMAP,
    |

libs/dashboard/tui_dashboard.py:229:34: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
227 |     """Logs monitoring view."""
228 |
229 |     def __init__(self, **kwargs: Any) -> None:
    |                                  ^^^ ANN401
230 |         super().__init__(**kwargs)
231 |         self.log_buffer: list[str] = []
    |

libs/dashboard/tui_dashboard.py:274:34: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
272 |     """Settings and configuration view."""
273 |
274 |     def __init__(self, **kwargs: Any) -> None:
    |                                  ^^^ ANN401
275 |         super().__init__(**kwargs)
276 |         self.settings: dict[str, object] = {
    |

libs/dashboard/tui_dashboard.py:352:34: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
350 |     refresh_interval = reactive(3.0)
351 |
352 |     def __init__(self, **kwargs: Any) -> None:
    |                                  ^^^ ANN401
353 |         """Initialize TUI Dashboard."""
354 |         super().__init__(**kwargs)
    |

libs/dashboard/widgets/activity_heatmap.py:29:9: PLR1702 Too many nested blocks (6 > 5)
   |
27 |           """
28 |           start_time_collect = datetime.now(UTC)
29 | /         try:
30 | |             log_path_str = self.config.get("log_path", "~/.scripton/yesman/logs/")
31 | |             safe_session_name = "".join(c for c in session_name if c.isalnum() or c in {"-", "_"}).rstrip()
32 | |             log_file = Path(log_path_str).expanduser() / f"{safe_session_name}.log"
33 | |
34 | |             if not log_file.exists():
35 | |                 log_file = Path(log_path_str).expanduser() / "yesman.log"
36 | |                 if not log_file.exists():
37 | |                     logger.warning(f"Log file not found for session {session_name}. Returning empty activity.")  # noqa: G004
38 | |                     return {}
39 | |
40 | |             activity_counts: defaultdict[str, int] = defaultdict(int)
41 | |
42 | |             now = datetime.now(UTC)
43 | |             start_time_filter = now - timedelta(days=days)
44 | |
45 | |             with open(log_file, encoding="utf-8") as f:
46 | |                 for line in f:
47 | |                     match = re.match(r"\[(.*?)\]", line)
48 | |                     if match:
49 | |                         timestamp_str = match.group(1).split(",")[0]
50 | |                         try:
51 | |                             log_time = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")
52 | |                             if log_time > start_time_filter:
53 | |                                 hour_timestamp = log_time.strftime("%Y-%m-%dT%H:00:00")
54 | |                                 activity_counts[hour_timestamp] += 1
55 | |                         except ValueError:
56 | |                             continue
57 | |             logger.info(f"Collected activity for {session_name} in {(datetime.now(UTC) - start_time_collect).total_seconds():.4f} secoâ€¦
58 | |             return activity_counts
59 | |         except Exception:
60 | |             logger.exception("Error collecting session activity for {session_name}")  # noqa: G004
61 | |             return {}
   | |_____________________^ PLR1702
62 |
63 |       def generate_heatmap_data(self, sessions: list[str], days: int = 7) -> dict[str]:
   |

libs/dashboard/widgets/activity_heatmap.py:58:13: TRY300 Consider moving this statement to an `else` block
   |
56 | â€¦                         continue
57 | â€¦         logger.info(f"Collected activity for {session_name} in {(datetime.now(UTC) - start_time_collect).total_seconds():.4f} secondâ€¦
58 | â€¦         return activity_counts
   |           ^^^^^^^^^^^^^^^^^^^^^^ TRY300
59 | â€¦     except Exception:
60 | â€¦         logger.exception("Error collecting session activity for {session_name}")  # noqa: G004
   |

libs/dashboard/widgets/progress_tracker.py:79:13: TRY300 Consider moving this statement to an `else` block
   |
77 |             content = path.read_text(encoding="utf-8")
78 |             self.todos = self._parse_markdown_todos(content)
79 |             return True
   |             ^^^^^^^^^^^ TRY300
80 |
81 |         except Exception:
   |

libs/logging/async_logger.py:78:9: PLR0917 Too many positional arguments (10/5)
   |
76 |     """Configuration for AsyncLogger."""
77 |
78 |     def __init__(
   |         ^^^^^^^^ PLR0917
79 |         self,
80 |         name: str = "yesman_async",
   |

libs/logging/async_logger.py:298:60: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
296 |             self.standard_logger.exception("Failed to queue log entry")
297 |
298 |     def log(self, level: LogLevel, message: str, **kwargs: Any) -> None:
    |                                                            ^^^ ANN401
299 |         """Log a message at specified level."""
300 |         # Extract caller information
    |

libs/logging/async_logger.py:338:45: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
337 |     # Convenience methods for different log levels
338 |     def trace(self, message: str, **kwargs: Any) -> None:
    |                                             ^^^ ANN401
339 |         """Log a trace message."""
340 |         self.log(LogLevel.TRACE, message, **kwargs)
    |

libs/logging/async_logger.py:342:45: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
340 |         self.log(LogLevel.TRACE, message, **kwargs)
341 |
342 |     def debug(self, message: str, **kwargs: Any) -> None:
    |                                             ^^^ ANN401
343 |         """Log a debug message."""
344 |         self.log(LogLevel.DEBUG, message, **kwargs)
    |

libs/logging/async_logger.py:346:44: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
344 |         self.log(LogLevel.DEBUG, message, **kwargs)
345 |
346 |     def info(self, message: str, **kwargs: Any) -> None:
    |                                            ^^^ ANN401
347 |         """Log an info message."""
348 |         self.log(LogLevel.INFO, message, **kwargs)
    |

libs/logging/async_logger.py:350:47: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
348 |         self.log(LogLevel.INFO, message, **kwargs)
349 |
350 |     def warning(self, message: str, **kwargs: Any) -> None:
    |                                               ^^^ ANN401
351 |         """Log a warning message."""
352 |         self.log(LogLevel.WARNING, message, **kwargs)
    |

libs/logging/async_logger.py:354:45: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
352 |         self.log(LogLevel.WARNING, message, **kwargs)
353 |
354 |     def error(self, message: str, **kwargs: Any) -> None:
    |                                             ^^^ ANN401
355 |         """Log an error message."""
356 |         self.log(LogLevel.ERROR, message, **kwargs)
    |

libs/logging/async_logger.py:358:48: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
356 |         self.log(LogLevel.ERROR, message, **kwargs)
357 |
358 |     def critical(self, message: str, **kwargs: Any) -> None:
    |                                                ^^^ ANN401
359 |         """Log a critical message."""
360 |         self.log(LogLevel.CRITICAL, message, **kwargs)
    |

libs/logging/async_logger.py:363:15: ANN204 Missing return type annotation for special method `__aenter__`
    |
362 |     # Context manager support
363 |     async def __aenter__(self):
    |               ^^^^^^^^^^ ANN204
364 |         """Async context manager entry."""
365 |         await self.start()
    |
    = help: Add return type annotation

libs/logging/async_logger_refactored.py:80:9: PLR0917 Too many positional arguments (8/5)
   |
78 |     """High-performance asynchronous logger with batch processing."""
79 |
80 |     def __init__(
   |         ^^^^^^^^ PLR0917
81 |         self,
82 |         name: str = "async_logger",
   |

libs/logging/async_logger_refactored.py:287:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
285 |         message: str,
286 |         exc_info: Exception | None = None,
287 |         **kwargs: Any,
    |                   ^^^ ANN401
288 |     ) -> None:
289 |         """Internal method to queue a log entry."""
    |

libs/logging/async_logger_refactored.py:334:51: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
333 |     # Convenience methods for different log levels
334 |     async def trace(self, message: str, **kwargs: Any) -> None:
    |                                                   ^^^ ANN401
335 |         """Log a trace message."""
336 |         await self._log(LogLevel.TRACE, message, **kwargs)
    |

libs/logging/async_logger_refactored.py:338:51: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
336 |         await self._log(LogLevel.TRACE, message, **kwargs)
337 |
338 |     async def debug(self, message: str, **kwargs: Any) -> None:
    |                                                   ^^^ ANN401
339 |         """Log a debug message."""
340 |         await self._log(LogLevel.DEBUG, message, **kwargs)
    |

libs/logging/async_logger_refactored.py:342:50: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
340 |         await self._log(LogLevel.DEBUG, message, **kwargs)
341 |
342 |     async def info(self, message: str, **kwargs: Any) -> None:
    |                                                  ^^^ ANN401
343 |         """Log an info message."""
344 |         await self._log(LogLevel.INFO, message, **kwargs)
    |

libs/logging/async_logger_refactored.py:346:53: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
344 |         await self._log(LogLevel.INFO, message, **kwargs)
345 |
346 |     async def warning(self, message: str, **kwargs: Any) -> None:
    |                                                     ^^^ ANN401
347 |         """Log a warning message."""
348 |         await self._log(LogLevel.WARNING, message, **kwargs)
    |

libs/logging/async_logger_refactored.py:350:86: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
348 |         await self._log(LogLevel.WARNING, message, **kwargs)
349 |
350 |     async def error(self, message: str, exc_info: Exception | None = None, **kwargs: Any) -> None:
    |                                                                                      ^^^ ANN401
351 |         """Log an error message."""
352 |         await self._log(LogLevel.ERROR, message, exc_info=exc_info, **kwargs)
    |

libs/logging/async_logger_refactored.py:354:89: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
352 |         await self._log(LogLevel.ERROR, message, exc_info=exc_info, **kwargs)
353 |
354 |     async def critical(self, message: str, exc_info: Exception | None = None, **kwargs: Any) -> None:
    |                                                                                         ^^^ ANN401
355 |         """Log a critical message."""
356 |         await self._log(LogLevel.CRITICAL, message, exc_info=exc_info, **kwargs)
    |

libs/multi_agent/agent_pool.py:34:1: PLR0904 Too many public methods (32 > 20)
     |
  34 | / class AgentPool:
  35 | |     """Manages a pool of agents for parallel task execution."""
  36 | |
  37 | |     def __init__(self, max_agents: int = 3, work_dir: str | None = None) -> None:
  38 | |         """Initialize agent pool.
  39 | |
  40 | |         Args:
  41 | |             max_agents: Maximum number of concurrent agents
  42 | |             work_dir: Directory for agent work and metadata
  43 | |         """
  44 | |         self.max_agents = max_agents
  45 | |         self.work_dir = Path(work_dir) if work_dir else Path.cwd() / ".yesman-agents"
  46 | |         self.work_dir.mkdir(parents=True, exist_ok=True)
  47 | |
  48 | |         # Agent and task management
  49 | |         self.agents: dict[str, Agent] = {}
  50 | |         self.tasks: dict[str, Task] = {}
  51 | |         self.task_queue: asyncio.Queue[Task] = asyncio.Queue()
  52 | |         self.completed_tasks: list[str] = []
  53 | |
  54 | |         # Intelligent task scheduling
  55 | |         self.scheduler = TaskScheduler()
  56 | |         self.intelligent_scheduling = True
  57 | |
  58 | |         # Event callbacks
  59 | |         self.task_started_callbacks: list[Callable[[Task], Awaitable[None]]] = []
  60 | |         self.task_completed_callbacks: list[Callable[[Task], Awaitable[None]]] = []
  61 | |         self.task_failed_callbacks: list[Callable[[Task], Awaitable[None]]] = []
  62 | |         self.agent_error_callbacks: list[Callable[[Agent, Exception], Awaitable[None]]] = []
  63 | |
  64 | |         # Control
  65 | |         self._running = False
  66 | |         self._shutdown_event = asyncio.Event()
  67 | |
  68 | |         # Auto-rebalancing
  69 | |         self._auto_rebalancing_enabled = False
  70 | |         self._auto_rebalancing_interval = 300
  71 | |
  72 | |         # Branch testing integration
  73 | |         self.branch_test_manager = None
  74 | |         self._test_integration_enabled = False
  75 | |
  76 | |         # Recovery and rollback system
  77 | |         self.recovery_engine = None
  78 | |         self._recovery_enabled = False
  79 | |
  80 | |         # Load persistent state
  81 | |         self._load_state()
  82 | |
  83 | |     def _get_state_file(self) -> Path:
  84 | |         """Get path to state file."""
  85 | |         return self.work_dir / "pool_state.json"
  86 | |
  87 | |     def _load_state(self) -> None:
  88 | |         """Load agent pool state from disk."""
  89 | |         state_file = self._get_state_file()
  90 | |
  91 | |         if state_file.exists():
  92 | |             try:
  93 | |                 with state_file.open() as f:
  94 | |                     data = json.load(f)
  95 | |
  96 | |                 # Load agents (without active processes)
  97 | |                 for agent_data in data.get("agents", []):
  98 | |                     agent = Agent.from_dict(agent_data)
  99 | |                     # Reset state for loaded agents
 100 | |                     agent.state = AgentState.IDLE
 101 | |                     agent.current_task = None
 102 | |                     agent.process = None
 103 | |                     self.agents[agent.agent_id] = agent
 104 | |
 105 | |                 # Load tasks
 106 | |                 for task_data in data.get("tasks", []):
 107 | |                     task = Task.from_dict(task_data)
 108 | |                     self.tasks[task.task_id] = task
 109 | |
 110 | |                 self.completed_tasks = data.get("completed_tasks", [])
 111 | |
 112 | |                 logger.info(
 113 | |                     "Loaded %d agents and %d tasks",
 114 | |                     len(self.agents),
 115 | |                     len(self.tasks),
 116 | |                 )
 117 | |
 118 | |             except Exception:
 119 | |                 logger.exception("Failed to load agent pool state:")
 120 | |
 121 | |     def _save_state(self) -> None:
 122 | |         """Save agent pool state to disk."""
 123 | |         state_file = self._get_state_file()
 124 | |
 125 | |         try:
 126 | |             data = {
 127 | |                 "agents": [agent.to_dict() for agent in self.agents.values()],
 128 | |                 "tasks": [task.to_dict() for task in self.tasks.values()],
 129 | |                 "completed_tasks": self.completed_tasks,
 130 | |                 "saved_at": datetime.now(UTC).isoformat(),
 131 | |             }
 132 | |
 133 | |             with state_file.open("w") as f:
 134 | |                 json.dump(data, f, indent=2)
 135 | |
 136 | |         except Exception:
 137 | |             logger.exception("Failed to save agent pool state:")
 138 | |
 139 | |     async def start(self) -> None:
 140 | |         """Start the agent pool."""
 141 | |         if self._running:
 142 | |             logger.warning("Agent pool is already running")
 143 | |             return
 144 | |
 145 | |         self._running = True
 146 | |         self._shutdown_event.clear()
 147 | |
 148 | |         logger.info("Starting agent pool with max %d agents", self.max_agents)
 149 | |
 150 | |         # Start task dispatcher
 151 | |         asyncio.create_task(self._task_dispatcher())
 152 | |
 153 | |         # Start agent monitor
 154 | |         asyncio.create_task(self._agent_monitor())
 155 | |
 156 | |         # Enable auto-rebalancing by default for pools with multiple agents
 157 | |         if self.max_agents > 1:
 158 | |             self.enable_auto_rebalancing()
 159 | |
 160 | |         logger.info("Agent pool started")
 161 | |
 162 | |     async def stop(self) -> None:
 163 | |         """Stop the agent pool."""
 164 | |         if not self._running:
 165 | |             return
 166 | |
 167 | |         logger.info("Stopping agent pool...")
 168 | |         self._running = False
 169 | |
 170 | |         # Disable auto-rebalancing
 171 | |         self._auto_rebalancing_enabled = False
 172 | |
 173 | |         # Terminate all agents
 174 | |         for agent in self.agents.values():
 175 | |             await self._terminate_agent(agent.agent_id)
 176 | |
 177 | |         # Signal shutdown
 178 | |         self._shutdown_event.set()
 179 | |
 180 | |         # Save state
 181 | |         self._save_state()
 182 | |
 183 | |         logger.info("Agent pool stopped")
 184 | |
 185 | |     def add_task(self, task: Task) -> None:
 186 | |         """Add a task to the queue."""
 187 | |         self.tasks[task.task_id] = task
 188 | |
 189 | |         if self.intelligent_scheduling:
 190 | |             # Add to intelligent scheduler
 191 | |             self.scheduler.add_task(task)
 192 | |         else:
 193 | |             # Add to simple queue (non-blocking)
 194 | |             try:
 195 | |                 self.task_queue.put_nowait(task)
 196 | |                 logger.info("Added task %s to queue", task.task_id)
 197 | |             except asyncio.QueueFull:
 198 | |                 logger.exception("Task queue is full, cannot add task %s", task.task_id)
 199 | |
 200 | |     def create_task(
 201 | |         self,
 202 | |         title: str,
 203 | |         command: list[str],
 204 | |         working_directory: str,
 205 | |         description: str = "",
 206 | |         **kwargs: Any,
 207 | |     ) -> Task:
 208 | |         """Create and add a task."""
 209 | |         task = Task(
 210 | |             task_id=str(uuid.uuid4()),
 211 | |             title=title,
 212 | |             description=description,
 213 | |             command=command,
 214 | |             working_directory=working_directory,
 215 | |             **kwargs,
 216 | |         )
 217 | |
 218 | |         self.add_task(task)
 219 | |         return task
 220 | |
 221 | |     async def _task_dispatcher(self) -> None:
 222 | |         """Dispatch tasks to available agents."""
 223 | |         while self._running:
 224 | |             try:
 225 | |                 if self.intelligent_scheduling:
 226 | |                     await self._intelligent_dispatch()
 227 | |                 else:
 228 | |                     await self._simple_dispatch()
 229 | |
 230 | |                 await asyncio.sleep(1)  # Brief pause between dispatch cycles
 231 | |
 232 | |             except Exception:
 233 | |                 logger.exception("Error in task dispatcher:")
 234 | |                 await asyncio.sleep(1)
 235 | |
 236 | |     async def _intelligent_dispatch(self) -> None:
 237 | |         """Intelligent task dispatching using the scheduler."""
 238 | |         # Get all available agents
 239 | |         available_agents = [agent for agent in self.agents.values() if agent.state == AgentState.IDLE]
 240 | |
 241 | |         # Create new agents if needed and under limit
 242 | |         while len(available_agents) < self.max_agents and len(self.agents) < self.max_agents:
 243 | |             new_agent = await self._create_agent()
 244 | |             available_agents.append(new_agent)
 245 | |
 246 | |         if not available_agents:
 247 | |             return
 248 | |
 249 | |         # Get optimal task assignments
 250 | |         assignments = self.scheduler.get_optimal_task_assignment(available_agents)
 251 | |
 252 | |         # Execute assignments
 253 | |         for agent, task in assignments:
 254 | |             await self._assign_task_to_agent(agent, task)
 255 | |
 256 | |     async def _simple_dispatch(self) -> None:
 257 | |         """Simple task dispatching (original logic)."""
 258 | |         try:
 259 | |             # Wait for task or shutdown
 260 | |             task = await asyncio.wait_for(self.task_queue.get(), timeout=1.0)
 261 | |
 262 | |             # Find available agent or create new one
 263 | |             agent = await self._get_available_agent()
 264 | |
 265 | |             if agent:
 266 | |                 await self._assign_task_to_agent(agent, task)
 267 | |             else:
 268 | |                 # Put task back in queue
 269 | |                 await self.task_queue.put(task)
 270 | |
 271 | |         except TimeoutError:
 272 | |             # No task in queue, continue
 273 | |             pass
 274 | |
 275 | |     async def _get_available_agent(self) -> Agent | None:
 276 | |         """Get an available agent or create a new one."""
 277 | |         # Look for idle agents
 278 | |         for agent in self.agents.values():
 279 | |             if agent.state == AgentState.IDLE:
 280 | |                 return agent
 281 | |
 282 | |         # Create new agent if under limit
 283 | |         if len(self.agents) < self.max_agents:
 284 | |             return await self._create_agent()
 285 | |
 286 | |         return None
 287 | |
 288 | |     async def _create_agent(self) -> Agent:
 289 | |         """Create a new agent."""
 290 | |         agent_id = f"agent-{len(self.agents) + 1}-{int(time.time())}"
 291 | |
 292 | |         agent = Agent(agent_id=agent_id)
 293 | |         self.agents[agent_id] = agent
 294 | |
 295 | |         # Register with intelligent scheduler
 296 | |         if self.intelligent_scheduling:
 297 | |             self.scheduler.register_agent(agent)
 298 | |
 299 | |         logger.info("Created agent %s", agent_id)
 300 | |         return agent
 301 | |
 302 | |     async def _assign_task_to_agent(self, agent: Agent, task: Task) -> None:
 303 | |         """Assign a task to an agent."""
 304 | |         # Update task
 305 | |         task.status = TaskStatus.ASSIGNED
 306 | |         task.assigned_agent = agent.agent_id
 307 | |
 308 | |         # Update agent
 309 | |         agent.state = AgentState.WORKING
 310 | |         agent.current_task = task.task_id
 311 | |         agent.last_heartbeat = datetime.now(UTC)
 312 | |
 313 | |         logger.info("Assigned task %s to agent %s", task.task_id, agent.agent_id)
 314 | |
 315 | |         # Start task execution
 316 | |         asyncio.create_task(self._execute_task(agent, task))
 317 | |
 318 | |     async def _execute_task(self, agent: Agent, task: Task) -> None:
 319 | |         """Execute a task on an agent."""
 320 | |         try:
 321 | |             task.status = TaskStatus.RUNNING
 322 | |             task.start_time = datetime.now(UTC)
 323 | |
 324 | |             # Notify callbacks
 325 | |             for callback in self.task_started_callbacks:
 326 | |                 try:
 327 | |                     await callback(task)
 328 | |                 except Exception:
 329 | |                     logger.exception("Error in task started callback:")
 330 | |
 331 | |             logger.info("Agent %s starting task %s", agent.agent_id, task.task_id)
 332 | |
 333 | |             # Prepare environment
 334 | |             env = os.environ.copy()
 335 | |             env.update(task.environment)
 336 | |             env["YESMAN_AGENT_ID"] = agent.agent_id
 337 | |             env["YESMAN_TASK_ID"] = task.task_id
 338 | |
 339 | |             # Execute command
 340 | |             process = await asyncio.create_subprocess_exec(
 341 | |                 *task.command,
 342 | |                 cwd=task.working_directory,
 343 | |                 env=env,
 344 | |                 stdout=asyncio.subprocess.PIPE,
 345 | |                 stderr=asyncio.subprocess.PIPE,
 346 | |             )
 347 | |
 348 | |             agent.process = process  # type: ignore[assignment]
 349 | |
 350 | |             try:
 351 | |                 # Wait for completion with timeout
 352 | |                 stdout, stderr = await asyncio.wait_for(
 353 | |                     process.communicate(),
 354 | |                     timeout=task.timeout,
 355 | |                 )
 356 | |
 357 | |                 # Update task results
 358 | |                 task.output = stdout.decode("utf-8")
 359 | |                 task.error = stderr.decode("utf-8")
 360 | |                 task.exit_code = process.returncode
 361 | |                 task.end_time = datetime.now(UTC)
 362 | |
 363 | |                 if process.returncode == 0:
 364 | |                     task.status = TaskStatus.COMPLETED
 365 | |                     agent.completed_tasks += 1
 366 | |                     self.completed_tasks.append(task.task_id)
 367 | |
 368 | |                     # Notify completion
 369 | |                     for callback in self.task_completed_callbacks:
 370 | |                         try:
 371 | |                             await callback(task)
 372 | |                         except Exception:
 373 | |                             logger.exception("Error in task completed callback:")
 374 | |
 375 | |                     logger.info("Task %s completed successfully", task.task_id)
 376 | |
 377 | |                 else:
 378 | |                     task.status = TaskStatus.FAILED
 379 | |                     agent.failed_tasks += 1
 380 | |
 381 | |                     # Notify failure
 382 | |                     for callback in self.task_failed_callbacks:
 383 | |                         try:
 384 | |                             await callback(task)
 385 | |                         except Exception:
 386 | |                             logger.exception("Error in task failed callback:")
 387 | |
 388 | |                     logger.error(
 389 | |                         "Task %s failed with exit code %d",
 390 | |                         task.task_id,
 391 | |                         process.returncode,
 392 | |                     )
 393 | |
 394 | |             except TimeoutError:
 395 | |                 # Task timed out
 396 | |                 logger.exception(
 397 | |                     "Task %s timed out after %d seconds",
 398 | |                     task.task_id,
 399 | |                     task.timeout,
 400 | |                 )
 401 | |
 402 | |                 # Terminate process
 403 | |                 try:
 404 | |                     process.terminate()
 405 | |                     await asyncio.wait_for(process.wait(), timeout=5)
 406 | |                 except:
 407 | |                     # Force kill if terminate doesn't work
 408 | |                     with contextlib.suppress(Exception):
 409 | |                         process.kill()
 410 | |
 411 | |                 task.status = TaskStatus.FAILED
 412 | |                 task.error = f"Task timed out after {task.timeout} seconds"
 413 | |                 task.end_time = datetime.now(UTC)
 414 | |                 agent.failed_tasks += 1
 415 | |
 416 | |             # Update execution time
 417 | |             if task.start_time and task.end_time:
 418 | |                 execution_time = (task.end_time - task.start_time).total_seconds()
 419 | |                 agent.total_execution_time += execution_time
 420 | |
 421 | |                 # Update scheduler with performance data
 422 | |                 if self.intelligent_scheduling:
 423 | |                     success = task.status == TaskStatus.COMPLETED
 424 | |                     self.scheduler.update_agent_performance(
 425 | |                         agent.agent_id,
 426 | |                         task,
 427 | |                         success,
 428 | |                         execution_time,
 429 | |                     )
 430 | |
 431 | |         except Exception as e:
 432 | |             logger.exception("Error executing task %s:", task.task_id)
 433 | |
 434 | |             task.status = TaskStatus.FAILED
 435 | |             task.error = str(e)
 436 | |             task.end_time = datetime.now(UTC)
 437 | |             agent.failed_tasks += 1
 438 | |             agent.state = AgentState.ERROR
 439 | |
 440 | |             # Notify task failure callbacks
 441 | |             for callback in self.task_failed_callbacks:
 442 | |                 try:
 443 | |                     await callback(task)
 444 | |                 except Exception:
 445 | |                     logger.exception("Error in agent error callback:")
 446 | |
 447 | |         finally:
 448 | |             # Reset agent state
 449 | |             agent.current_task = None
 450 | |             agent.process = None
 451 | |             if agent.state != AgentState.ERROR:
 452 | |                 agent.state = AgentState.IDLE
 453 | |
 454 | |             # Save state
 455 | |             self._save_state()
 456 | |
 457 | |     async def _agent_monitor(self) -> None:
 458 | |         """Monitor agent health and status."""
 459 | |         while self._running:
 460 | |             try:
 461 | |                 current_time = datetime.now(UTC)
 462 | |
 463 | |                 for agent in list(self.agents.values()):
 464 | |                     # Check for stale agents (no heartbeat in 5 minutes)
 465 | |                     if (current_time - agent.last_heartbeat).total_seconds() > 300:
 466 | |                         logger.warning(
 467 | |                             "Agent %s appears stale, terminating",
 468 | |                             agent.agent_id,
 469 | |                         )
 470 | |                         await self._terminate_agent(agent.agent_id)
 471 | |
 472 | |                     # Update heartbeat for working agents
 473 | |                     if agent.state == AgentState.WORKING:
 474 | |                         agent.last_heartbeat = current_time
 475 | |
 476 | |                 # Save state periodically
 477 | |                 self._save_state()
 478 | |
 479 | |                 await asyncio.sleep(30)  # Check every 30 seconds
 480 | |
 481 | |             except Exception:
 482 | |                 logger.exception("Error in agent monitor:")
 483 | |                 await asyncio.sleep(30)
 484 | |
 485 | |     async def _terminate_agent(self, agent_id: str) -> None:
 486 | |         """Terminate an agent."""
 487 | |         agent = self.agents.get(agent_id)
 488 | |         if not agent:
 489 | |             return
 490 | |
 491 | |         # Terminate running process
 492 | |         if agent.process:
 493 | |             try:
 494 | |                 agent.process.terminate()
 495 | |                 await asyncio.wait_for(agent.process.wait(), timeout=5.0)  # type: ignore[arg-type]
 496 | |             except:
 497 | |                 with contextlib.suppress(Exception):
 498 | |                     agent.process.kill()
 499 | |
 500 | |         # Update state
 501 | |         agent.state = AgentState.TERMINATED
 502 | |         agent.current_task = None
 503 | |         agent.process = None
 504 | |
 505 | |         logger.info("Terminated agent %s", agent_id)
 506 | |
 507 | |     def get_agent_status(self, agent_id: str) -> dict[str, object] | None:
 508 | |         """Get status of a specific agent."""
 509 | |         agent = self.agents.get(agent_id)
 510 | |         if not agent:
 511 | |             return None
 512 | |
 513 | |         return agent.to_dict()
 514 | |
 515 | |     def get_task_status(self, task_id: str) -> dict[str, object] | None:
 516 | |         """Get status of a specific task."""
 517 | |         task = self.tasks.get(task_id)
 518 | |         if not task:
 519 | |             return None
 520 | |
 521 | |         return task.to_dict()
 522 | |
 523 | |     def list_agents(self) -> list[dict[str, object]]:
 524 | |         """List all agents and their status."""
 525 | |         return [agent.to_dict() for agent in self.agents.values()]
 526 | |
 527 | |     def list_tasks(self, status: TaskStatus | None = None) -> list[dict[str, object]]:
 528 | |         """List tasks, optionally filtered by status."""
 529 | |         tasks = self.tasks.values()
 530 | |
 531 | |         if status:
 532 | |             filtered_tasks = [t for t in tasks if t.status == status]
 533 | |             return [task.to_dict() for task in filtered_tasks]
 534 | |
 535 | |         return [task.to_dict() for task in tasks]
 536 | |
 537 | |     def get_pool_statistics(self) -> dict[str, object]:
 538 | |         """Get pool statistics."""
 539 | |         active_agents = len(
 540 | |             [a for a in self.agents.values() if a.state != AgentState.TERMINATED],
 541 | |         )
 542 | |         working_agents = len(
 543 | |             [a for a in self.agents.values() if a.state == AgentState.WORKING],
 544 | |         )
 545 | |
 546 | |         total_completed = sum(a.completed_tasks for a in self.agents.values())
 547 | |         total_failed = sum(a.failed_tasks for a in self.agents.values())
 548 | |         total_execution_time = sum(a.total_execution_time for a in self.agents.values())
 549 | |
 550 | |         task_counts = {}
 551 | |         for status in TaskStatus:
 552 | |             task_counts[status.value] = len(
 553 | |                 [t for t in self.tasks.values() if t.status == status],
 554 | |             )
 555 | |
 556 | |         return {
 557 | |             "max_agents": self.max_agents,
 558 | |             "active_agents": active_agents,
 559 | |             "working_agents": working_agents,
 560 | |             "idle_agents": active_agents - working_agents,
 561 | |             "total_tasks": len(self.tasks),
 562 | |             "completed_tasks": total_completed,
 563 | |             "failed_tasks": total_failed,
 564 | |             "total_execution_time": total_execution_time,
 565 | |             "average_execution_time": total_execution_time / max(total_completed, 1),
 566 | |             "task_status_counts": task_counts,
 567 | |             "queue_size": self.task_queue.qsize(),
 568 | |             "running": self._running,
 569 | |         }
 570 | |
 571 | |     # Event callback registration
 572 | |     def on_task_started(self, callback: Callable[[Task], Awaitable[None]]) -> None:
 573 | |         """Register callback for task started events."""
 574 | |         self.task_started_callbacks.append(callback)
 575 | |
 576 | |     def on_task_completed(self, callback: Callable[[Task], Awaitable[None]]) -> None:
 577 | |         """Register callback for task completed events."""
 578 | |         self.task_completed_callbacks.append(callback)
 579 | |
 580 | |     def on_task_failed(self, callback: Callable[[Task], Awaitable[None]]) -> None:
 581 | |         """Register callback for task failed events."""
 582 | |         self.task_failed_callbacks.append(callback)
 583 | |
 584 | |     def on_agent_error(
 585 | |         self,
 586 | |         callback: Callable[[Agent, Exception], Awaitable[None]],
 587 | |     ) -> None:
 588 | |         """Register callback for agent error events."""
 589 | |         self.agent_error_callbacks.append(callback)
 590 | |
 591 | |     def get_scheduling_metrics(self) -> dict[str, object]:
 592 | |         """Get intelligent scheduling metrics."""
 593 | |         if self.intelligent_scheduling:
 594 | |             return self.scheduler.get_scheduling_metrics()
 595 | |         return {
 596 | |             "intelligent_scheduling": False,
 597 | |             "queue_size": self.task_queue.qsize(),
 598 | |         }
 599 | |
 600 | |     def set_intelligent_scheduling(self, enabled: bool) -> None:  # noqa: FBT001
 601 | |         """Enable or disable intelligent scheduling."""
 602 | |         self.intelligent_scheduling = enabled
 603 | |
 604 | |         if enabled:
 605 | |             # Register existing agents with scheduler
 606 | |             for agent in self.agents.values():
 607 | |                 self.scheduler.register_agent(agent)
 608 | |
 609 | |         logger.info("Intelligent scheduling %s", "enabled" if enabled else "disabled")
 610 | |
 611 | |     def update_agent_capability(
 612 | |         self,
 613 | |         agent_id: str,
 614 | |         capability: "AgentCapability",
 615 | |     ) -> None:
 616 | |         """Update an agent's capability profile."""
 617 | |         if self.intelligent_scheduling and agent_id in self.agents:
 618 | |             self.scheduler.agent_capabilities[agent_id] = capability
 619 | |             logger.info("Updated capability for agent %s", agent_id)
 620 | |
 621 | |     def rebalance_workload(self) -> list[tuple[str, str]]:
 622 | |         """Trigger workload rebalancing."""
 623 | |         if self.intelligent_scheduling:
 624 | |             rebalancing_actions = self.scheduler.rebalance_tasks()
 625 | |
 626 | |             # Execute rebalancing actions
 627 | |             for overloaded_agent_id, underloaded_agent_id in rebalancing_actions:
 628 | |                 self._execute_rebalancing(overloaded_agent_id, underloaded_agent_id)
 629 | |
 630 | |             return rebalancing_actions
 631 | |         return []
 632 | |
 633 | |     def _execute_rebalancing(self, from_agent_id: str, to_agent_id: str) -> None:
 634 | |         """Execute task rebalancing between two agents."""
 635 | |         try:
 636 | |             # Update agent loads immediately to prevent over-rebalancing
 637 | |             from_agent = self.agents.get(from_agent_id)
 638 | |             to_agent = self.agents.get(to_agent_id)
 639 | |
 640 | |             if not from_agent or not to_agent:
 641 | |                 logger.warning("Cannot rebalance: agent not found")
 642 | |                 return
 643 | |
 644 | |             # Update scheduler agent loads
 645 | |             if self.intelligent_scheduling:
 646 | |                 from_cap = self.scheduler.agent_capabilities.get(from_agent_id)
 647 | |                 to_cap = self.scheduler.agent_capabilities.get(to_agent_id)
 648 | |
 649 | |                 if from_cap and to_cap:
 650 | |                     # Redistribute load (move 10% from overloaded to underloaded)
 651 | |                     load_transfer = min(0.1, from_cap.current_load * 0.2)
 652 | |                     from_cap.current_load = max(0.0, from_cap.current_load - load_transfer)
 653 | |                     to_cap.current_load = min(1.0, to_cap.current_load + load_transfer)
 654 | |
 655 | |                     logger.info(
 656 | |                         "Rebalanced load: %s (%.2f) -> %s (%.2f)",
 657 | |                         from_agent_id,
 658 | |                         from_cap.current_load,
 659 | |                         to_agent_id,
 660 | |                         to_cap.current_load,
 661 | |                     )
 662 | |
 663 | |         except Exception:
 664 | |             logger.exception("Error executing rebalancing:")
 665 | |
 666 | |     def enable_auto_rebalancing(self, interval_seconds: int = 300) -> None:
 667 | |         """Enable automatic workload rebalancing."""
 668 | |         if not self.intelligent_scheduling:
 669 | |             logger.warning("Cannot enable auto-rebalancing without intelligent scheduling")
 670 | |             return
 671 | |
 672 | |         self._auto_rebalancing_enabled = True
 673 | |         self._auto_rebalancing_interval = interval_seconds
 674 | |
 675 | |         # Start auto-rebalancing task
 676 | |         asyncio.create_task(self._auto_rebalancing_loop())
 677 | |         logger.info("Auto-rebalancing enabled with %ds interval", interval_seconds)
 678 | |
 679 | |     async def _auto_rebalancing_loop(self) -> None:
 680 | |         """Automatic rebalancing loop."""
 681 | |         while self._running and getattr(self, "_auto_rebalancing_enabled", False):
 682 | |             try:
 683 | |                 # Check if rebalancing is needed
 684 | |                 metrics = self.scheduler.get_scheduling_metrics()
 685 | |                 load_balancing_score = metrics.get("load_balancing_score", 1.0)
 686 | |
 687 | |                 # Trigger rebalancing if score is below threshold (0.7)
 688 | |                 if load_balancing_score < 0.7:
 689 | |                     logger.info("Load balancing score low (%.3f), triggering rebalancing", load_balancing_score)
 690 | |                     rebalancing_actions = self.rebalance_workload()
 691 | |
 692 | |                     if rebalancing_actions:
 693 | |                         logger.info("Executed %d rebalancing actions", len(rebalancing_actions))
 694 | |
 695 | |                 await asyncio.sleep(getattr(self, "_auto_rebalancing_interval", 300))
 696 | |
 697 | |             except Exception:
 698 | |                 logger.exception("Error in auto-rebalancing loop:")
 699 | |                 await asyncio.sleep(60)  # Wait before retrying
 700 | |
 701 | |     def enable_branch_testing(self, repo_path: str | None = None, results_dir: str | None = None) -> None:
 702 | |         """Enable automatic branch testing integration."""
 703 | |         try:
 704 | |             repo_path = repo_path or "."
 705 | |             results_dir = results_dir or ".scripton/yesman/test_results"
 706 | |
 707 | |             self.branch_test_manager = BranchTestManager(  # type: ignore[assignment]
 708 | |                 repo_path=repo_path,
 709 | |                 results_dir=results_dir,
 710 | |                 agent_pool=self,
 711 | |             )
 712 | |
 713 | |             self._test_integration_enabled = True
 714 | |             logger.info("Branch testing integration enabled")
 715 | |
 716 | |         except Exception:
 717 | |             logger.exception("Failed to enable branch testing:")
 718 | |             self._test_integration_enabled = False
 719 | |
 720 | |     def create_test_task(
 721 | |         self,
 722 | |         branch_name: str,
 723 | |         test_suite_name: str | None = None,
 724 | |         priority: int = 7,
 725 | |         timeout: int = 600,
 726 | |     ) -> Task:
 727 | |         """Create a test task for a specific branch.
 728 | |
 729 | |         Args:
 730 | |             branch_name: Name of the branch to test
 731 | |             test_suite_name: Specific test suite to run (None for all tests)
 732 | |             priority: Task priority (1-10)
 733 | |             timeout: Test timeout in seconds
 734 | |
 735 | |         Returns:
 736 | |             Task object for test execution
 737 | |         """
 738 | |         if not self._test_integration_enabled or not self.branch_test_manager:
 739 | |             msg = "Branch testing is not enabled"
 740 | |             raise RuntimeError(msg)
 741 | |
 742 | |         task_id = f"test-{branch_name}-{test_suite_name or 'all'}-{int(time.time())}"
 743 | |
 744 | |         # Prepare test command
 745 | |         if test_suite_name:
 746 | |             # Run specific test suite
 747 | |             test_command = [
 748 | |                 "python",
 749 | |                 "-c",
 750 | |                 f"import asyncio; "
 751 | |                 f"from libs.multi_agent.branch_test_manager import BranchTestManager; "
 752 | |                 f"btm = BranchTestManager(); "
 753 | |                 f"result = asyncio.run(btm.run_test_suite('{branch_name}', '{test_suite_name}')); "
 754 | |                 f"print(f'Test {{result.status.value}}: {{result.test_id}}')",
 755 | |             ]
 756 | |         else:
 757 | |             # Run all tests
 758 | |             test_command = [
 759 | |                 "python",
 760 | |                 "-c",
 761 | |                 f"import asyncio; "
 762 | |                 f"from libs.multi_agent.branch_test_manager import BranchTestManager; "
 763 | |                 f"btm = BranchTestManager(); "
 764 | |                 f"results = asyncio.run(btm.run_all_tests('{branch_name}')); "
 765 | |                 f"print(f'Completed {{len(results)}} tests on {branch_name}')",
 766 | |             ]
 767 | |
 768 | |         return Task(
 769 | |             task_id=task_id,
 770 | |             title=f"Test {test_suite_name or 'All'} on {branch_name}",
 771 | |             description=f"Execute {'specific' if test_suite_name else 'all'} tests on branch {branch_name}",
 772 | |             command=test_command,
 773 | |             working_directory=".",
 774 | |             timeout=timeout,
 775 | |             priority=priority,
 776 | |             complexity=6,  # Medium complexity
 777 | |             metadata={
 778 | |                 "type": "test",
 779 | |                 "branch": branch_name,
 780 | |                 "test_suite": test_suite_name,
 781 | |                 "auto_generated": True,
 782 | |             },
 783 | |         )
 784 | |
 785 | |     async def auto_test_branch(self, branch_name: str) -> list[str]:
 786 | |         """Automatically create and schedule test tasks for a branch.
 787 | |
 788 | |         Args:
 789 | |             branch_name: Name of the branch to test
 790 | |
 791 | |         Returns:
 792 | |             List of created task IDs
 793 | |         """
 794 | |         if not self._test_integration_enabled or not self.branch_test_manager:
 795 | |             logger.warning("Branch testing is not enabled")
 796 | |             return []
 797 | |
 798 | |         task_ids = []
 799 | |
 800 | |         try:
 801 | |             # Get critical test suites first
 802 | |             critical_suites = [name for name, suite in self.branch_test_manager.test_suites.items() if suite.critical]
 803 | |
 804 | |             # Create tasks for critical tests (higher priority)
 805 | |             for suite_name in critical_suites:
 806 | |                 task = self.create_test_task(
 807 | |                     branch_name=branch_name,
 808 | |                     test_suite_name=suite_name,
 809 | |                     priority=8,  # High priority for critical tests
 810 | |                 )
 811 | |                 self.add_task(task)
 812 | |                 task_ids.append(task.task_id)
 813 | |
 814 | |             # Create task for non-critical tests (combined)
 815 | |             non_critical_suites = [name for name, suite in self.branch_test_manager.test_suites.items() if not suite.critical]
 816 | |
 817 | |             if non_critical_suites:
 818 | |                 # Create a single task for all non-critical tests
 819 | |                 task = self.create_test_task(
 820 | |                     branch_name=branch_name,
 821 | |                     test_suite_name=None,  # All remaining tests
 822 | |                     priority=6,  # Medium priority
 823 | |                 )
 824 | |                 self.add_task(task)
 825 | |                 task_ids.append(task.task_id)
 826 | |
 827 | |             logger.info("Created %d test tasks for branch %s", len(task_ids), branch_name)
 828 | |
 829 | |         except Exception:
 830 | |             logger.exception("Error creating test tasks for %s:", branch_name)
 831 | |
 832 | |         return task_ids
 833 | |
 834 | |     def get_branch_test_status(self, branch_name: str) -> dict[str, object]:
 835 | |         """Get test status summary for a branch."""
 836 | |         if not self._test_integration_enabled or not self.branch_test_manager:
 837 | |             return {"error": "Branch testing not enabled"}
 838 | |
 839 | |         try:
 840 | |             return self.branch_test_manager.get_branch_test_summary(branch_name)
 841 | |         except Exception as e:
 842 | |             logger.exception("Error getting test status for %s:", branch_name)
 843 | |             return {"error": str(e)}
 844 | |
 845 | |     def get_all_branch_test_status(self) -> dict[str, dict[str, object]]:
 846 | |         """Get test status for all active branches."""
 847 | |         if not self._test_integration_enabled or not self.branch_test_manager:
 848 | |             return {"error": "Branch testing not enabled"}  # type: ignore[dict-item]
 849 | |
 850 | |         try:
 851 | |             return self.branch_test_manager.get_all_branch_summaries()
 852 | |         except Exception as e:
 853 | |             logger.exception("Error getting all branch test status:")
 854 | |             return {"error": str(e)}
 855 | |
 856 | |     def enable_recovery_system(self, work_dir: str | None = None, max_snapshots: int = 50) -> None:
 857 | |         """Enable automatic rollback and error recovery system."""
 858 | |         try:
 859 | |             work_dir = work_dir or ".scripton/yesman"
 860 | |
 861 | |             self.recovery_engine = RecoveryEngine(  # type: ignore[assignment]
 862 | |                 work_dir=work_dir,
 863 | |                 max_snapshots=max_snapshots,
 864 | |             )
 865 | |
 866 | |             self._recovery_enabled = True
 867 | |
 868 | |             # Register recovery callbacks
 869 | |             self._setup_recovery_callbacks()
 870 | |
 871 | |             logger.info("Recovery and rollback system enabled")
 872 | |
 873 | |         except Exception:
 874 | |             logger.exception("Failed to enable recovery system:")
 875 | |             self._recovery_enabled = False
 876 | |
 877 | |     def _setup_recovery_callbacks(self) -> None:
 878 | |         """Setup callbacks for automatic recovery."""
 879 | |         if not self.recovery_engine:
 880 | |             return
 881 | |
 882 | |         # Register task failure callback
 883 | |         async def handle_task_failure(task: Task) -> None:
 884 | |             if self._recovery_enabled and self.recovery_engine:
 885 | |                 operation_id = f"task-{task.task_id}"
 886 | |                 context = {
 887 | |                     "operation_type": "task_execution",
 888 | |                     "task_id": task.task_id,
 889 | |                     "agent_id": task.assigned_agent,
 890 | |                 }
 891 | |
 892 | |                 # Create exception from task error
 893 | |                 exception = Exception(task.error or f"Task failed with exit code {task.exit_code}")
 894 | |
 895 | |                 await self.recovery_engine.handle_operation_failure(
 896 | |                     operation_id=operation_id,
 897 | |                     exception=exception,
 898 | |                     context=context,
 899 | |                     agent_pool=self,
 900 | |                 )
 901 | |
 902 | |         # Register agent error callback
 903 | |         async def handle_agent_error(agent: Agent, exception: Exception) -> None:
 904 | |             if self._recovery_enabled and self.recovery_engine:
 905 | |                 operation_id = f"agent-{agent.agent_id}-{int(time.time())}"
 906 | |                 context = {
 907 | |                     "operation_type": "agent_operation",
 908 | |                     "agent_id": agent.agent_id,
 909 | |                     "task_id": agent.current_task,
 910 | |                 }
 911 | |
 912 | |                 await self.recovery_engine.handle_operation_failure(
 913 | |                     operation_id=operation_id,
 914 | |                     exception=exception,
 915 | |                     context=context,
 916 | |                     agent_pool=self,
 917 | |                 )
 918 | |
 919 | |         # Add callbacks
 920 | |         self.on_task_failed(handle_task_failure)
 921 | |         self.on_agent_error(handle_agent_error)
 922 | |
 923 | |     async def create_operation_snapshot(
 924 | |         self,
 925 | |         operation_type: str,
 926 | |         description: str,
 927 | |         files_to_backup: list[str] | None = None,
 928 | |         context: dict[str, object] | None = None,
 929 | |     ) -> str | None:
 930 | |         """Create a snapshot before a critical operation.
 931 | |
 932 | |         Args:
 933 | |             operation_type: Type of operation (for recovery strategy selection)
 934 | |             description: Human-readable description of the operation
 935 | |             files_to_backup: List of files to backup before the operation
 936 | |             context: Additional context for the operation
 937 | |
 938 | |         Returns:
 939 | |             Snapshot ID if successful, None otherwise
 940 | |         """
 941 | |         if not self._recovery_enabled or not self.recovery_engine:
 942 | |             logger.warning("Recovery system not enabled, cannot create snapshot")
 943 | |             return None
 944 | |
 945 | |         try:
 946 | |             # Map string to enum
 947 | |             op_type_map = {
 948 | |                 "task_execution": OperationType.TASK_EXECUTION,
 949 | |                 "branch_operation": OperationType.BRANCH_OPERATION,
 950 | |                 "agent_assignment": OperationType.AGENT_ASSIGNMENT,
 951 | |                 "file_modification": OperationType.FILE_MODIFICATION,
 952 | |                 "system_config": OperationType.SYSTEM_CONFIG,
 953 | |                 "test_execution": OperationType.TEST_EXECUTION,
 954 | |             }
 955 | |
 956 | |             op_type = op_type_map.get(operation_type, OperationType.TASK_EXECUTION)
 957 | |
 958 | |             return await self.recovery_engine.create_snapshot(
 959 | |                 operation_type=op_type,
 960 | |                 description=description,
 961 | |                 agent_pool=self,
 962 | |                 branch_manager=getattr(self, "branch_manager", None),
 963 | |                 files_to_backup=files_to_backup,
 964 | |                 operation_context=context,
 965 | |             )
 966 | |
 967 | |         except Exception:
 968 | |             logger.exception("Failed to create operation snapshot:")
 969 | |             return None
 970 | |
 971 | |     async def rollback_to_snapshot(self, snapshot_id: str) -> bool:
 972 | |         """Manually rollback to a specific snapshot.
 973 | |
 974 | |         Args:
 975 | |             snapshot_id: ID of the snapshot to rollback to
 976 | |
 977 | |         Returns:
 978 | |             True if rollback successful, False otherwise
 979 | |         """
 980 | |         if not self._recovery_enabled or not self.recovery_engine:
 981 | |             logger.error("Recovery system not enabled")
 982 | |             return False
 983 | |
 984 | |         try:
 985 | |             success = await self.recovery_engine.manual_rollback(
 986 | |                 snapshot_id=snapshot_id,
 987 | |                 agent_pool=self,
 988 | |                 branch_manager=getattr(self, "branch_manager", None),
 989 | |             )
 990 | |
 991 | |             if success:
 992 | |                 logger.info("Successfully rolled back to snapshot %s", snapshot_id)
 993 | |             else:
 994 | |                 logger.error("Failed to rollback to snapshot %s", snapshot_id)
 995 | |
 996 | |             return success
 997 | |
 998 | |         except Exception:
 999 | |             logger.exception("Error during rollback to snapshot %s:", snapshot_id)
1000 | |             return False
1001 | |
1002 | |     def get_recovery_status(self) -> dict[str, object]:
1003 | |         """Get status and metrics of the recovery system."""
1004 | |         if not self._recovery_enabled or not self.recovery_engine:
1005 | |             return {"recovery_enabled": False}
1006 | |
1007 | |         try:
1008 | |             metrics = self.recovery_engine.get_recovery_metrics()
1009 | |             recent_operations = self.recovery_engine.get_recent_operations(10)
1010 | |
1011 | |             return {
1012 | |                 "recovery_enabled": True,
1013 | |                 "metrics": metrics,
1014 | |                 "recent_operations": recent_operations,
1015 | |                 "active_operations": len(self.recovery_engine.active_operations),
1016 | |             }
1017 | |
1018 | |         except Exception as e:
1019 | |             logger.exception("Error getting recovery status:")
1020 | |             return {"recovery_enabled": True, "error": str(e)}
1021 | |
1022 | |     def list_recovery_snapshots(self) -> list[dict[str, object]]:
1023 | |         """List available recovery snapshots."""
1024 | |         if not self._recovery_enabled or not self.recovery_engine:
1025 | |             return []
1026 | |
1027 | |         try:
1028 | |             snapshots = []
1029 | |             for snapshot_id, snapshot in self.recovery_engine.snapshots.items():
1030 | |                 snapshots.append(
1031 | |                     {
1032 | |                         "snapshot_id": snapshot_id,
1033 | |                         "operation_type": snapshot.operation_type.value,
1034 | |                         "description": snapshot.description,
1035 | |                         "timestamp": snapshot.timestamp.isoformat(),
1036 | |                         "agent_count": len(snapshot.agent_states),
1037 | |                         "task_count": len(snapshot.task_states),
1038 | |                         "file_count": len(snapshot.file_states),
1039 | |                     }
1040 | |                 )
1041 | |
1042 | |             # Sort by timestamp (newest first)
1043 | |             snapshots.sort(key=lambda x: x["timestamp"], reverse=True)
1044 | |             return snapshots
1045 | |
1046 | |         except Exception:
1047 | |             logger.exception("Error listing recovery snapshots:")
1048 | |             return []
1049 | |
1050 | |     async def execute_with_recovery(
1051 | |         self,
1052 | |         operation_func: Callable[[], Awaitable[object]],
1053 | |         operation_type: str,
1054 | |         description: str,
1055 | |         files_to_backup: list[str] | None = None,
1056 | |         max_retries: int = 3,
1057 | |         context: dict[str, object] | None = None,
1058 | |     ) -> tuple[bool, object]:
1059 | |         """Execute an operation with automatic snapshot and recovery.
1060 | |
1061 | |         Args:
1062 | |             operation_func: Async function to execute
1063 | |             operation_type: Type of operation for recovery strategy
1064 | |             description: Description of the operation
1065 | |             files_to_backup: Files to backup before operation
1066 | |             max_retries: Maximum number of retry attempts
1067 | |             context: Additional context for recovery
1068 | |
1069 | |         Returns:
1070 | |             Tuple of (success, result)
1071 | |         """
1072 | |         if not self._recovery_enabled or not self.recovery_engine:
1073 | |             logger.warning("Recovery system not enabled, executing without protection")
1074 | |             try:
1075 | |                 result = await operation_func()
1076 | |                 return True, result
1077 | |             except Exception as e:
1078 | |                 logger.exception("Operation failed without recovery:")
1079 | |                 return False, str(e)
1080 | |
1081 | |         # Create snapshot before operation
1082 | |         snapshot_id = await self.create_operation_snapshot(
1083 | |             operation_type=operation_type,
1084 | |             description=description,
1085 | |             files_to_backup=files_to_backup,
1086 | |             context=context,
1087 | |         )
1088 | |
1089 | |         if not snapshot_id:
1090 | |             logger.warning("Failed to create snapshot, proceeding without recovery protection")
1091 | |
1092 | |         operation_id = f"op-{int(time.time())}-{str(uuid.uuid4())[:8]}"
1093 | |
1094 | |         if snapshot_id:
1095 | |             self.recovery_engine.start_operation(operation_id, snapshot_id)
1096 | |
1097 | |         retry_count = 0
1098 | |         while retry_count <= max_retries:
1099 | |             try:
1100 | |                 # Execute the operation
1101 | |                 result = await operation_func()
1102 | |
1103 | |                 # Mark operation as successful
1104 | |                 if self.recovery_engine:
1105 | |                     self.recovery_engine.complete_operation(operation_id)
1106 | |
1107 | |                 logger.info("Operation completed successfully: %s", description)
1108 | |                 return True, result
1109 | |
1110 | |             except Exception as e:
1111 | |                 logger.exception("Operation failed (attempt %d):", retry_count + 1)
1112 | |
1113 | |                 if retry_count < max_retries:
1114 | |                     # Attempt recovery
1115 | |                     if self.recovery_engine:
1116 | |                         recovery_success = await self.recovery_engine.handle_operation_failure(
1117 | |                             operation_id=operation_id,
1118 | |                             exception=e,
1119 | |                             context={
1120 | |                                 **(context or {}),
1121 | |                                 "operation_type": operation_type,
1122 | |                                 "retry_count": retry_count,
1123 | |                             },
1124 | |                             agent_pool=self,
1125 | |                             branch_manager=getattr(self, "branch_manager", None),
1126 | |                         )
1127 | |
1128 | |                         if recovery_success:
1129 | |                             logger.info("Recovery successful, retrying operation")
1130 | |                             retry_count += 1
1131 | |                             continue
1132 | |
1133 | |                     # Simple retry without recovery
1134 | |                     logger.info("Retrying operation without recovery (attempt %d)", retry_count + 2)
1135 | |                     retry_count += 1
1136 | |                     await asyncio.sleep(2**retry_count)  # Exponential backoff
1137 | |                 else:
1138 | |                     # Max retries exceeded
1139 | |                     logger.exception("Operation failed after %d attempts")
1140 | |                     return False, str(e)
1141 | |
1142 | |         return False, "Max retries exceeded"
1143 | |
1144 | |     def all_tasks_completed(self) -> bool:
1145 | |         """Check if all tasks have been completed."""
1146 | |         if not self.tasks:
1147 | |             return True
1148 | |
1149 | |         incomplete_tasks = [task for task in self.tasks.values() if task.status not in {TaskStatus.COMPLETED, TaskStatus.FAILED}]
1150 | |         return len(incomplete_tasks) == 0
1151 | |
1152 | |     def reset(self) -> None:
1153 | |         """Reset the agent pool state."""
1154 | |         # Clear all tasks and completed task list
1155 | |         self.tasks.clear()
1156 | |         self.completed_tasks.clear()
1157 | |
1158 | |         # Clear task queue
1159 | |         while not self.task_queue.empty():
1160 | |             try:
1161 | |                 self.task_queue.get_nowait()
1162 | |             except asyncio.QueueEmpty:
1163 | |                 break
1164 | |
1165 | |         # Reset scheduler if using intelligent scheduling
1166 | |         if hasattr(self.scheduler, "reset"):
1167 | |             self.scheduler.reset()
1168 | |         else:
1169 | |             # Recreate scheduler if reset method doesn't exist
1170 | |
1171 | |             self.scheduler = TaskScheduler()
1172 | |
1173 | |         logger.info("Agent pool reset completed")
     | |_________________________________________________^ PLR0904
     |

libs/multi_agent/agent_pool.py:206:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
204 |         working_directory: str,
205 |         description: str = "",
206 |         **kwargs: Any,
    |                   ^^^ ANN401
207 |     ) -> Task:
208 |         """Create and add a task."""
    |

libs/multi_agent/agent_pool.py:996:13: TRY300 Consider moving this statement to an `else` block
    |
994 |                 logger.error("Failed to rollback to snapshot %s", snapshot_id)
995 |
996 |             return success
    |             ^^^^^^^^^^^^^^ TRY300
997 |
998 |         except Exception:
    |

libs/multi_agent/agent_pool.py:1044:13: TRY300 Consider moving this statement to an `else` block
     |
1042 |             # Sort by timestamp (newest first)
1043 |             snapshots.sort(key=lambda x: x["timestamp"], reverse=True)
1044 |             return snapshots
     |             ^^^^^^^^^^^^^^^^ TRY300
1045 |
1046 |         except Exception:
     |

libs/multi_agent/agent_pool.py:1050:15: PLR0917 Too many positional arguments (6/5)
     |
1048 |             return []
1049 |
1050 |     async def execute_with_recovery(
     |               ^^^^^^^^^^^^^^^^^^^^^ PLR0917
1051 |         self,
1052 |         operation_func: Callable[[], Awaitable[object]],
     |

libs/multi_agent/agent_pool.py:1076:17: TRY300 Consider moving this statement to an `else` block
     |
1074 |             try:
1075 |                 result = await operation_func()
1076 |                 return True, result
     |                 ^^^^^^^^^^^^^^^^^^^ TRY300
1077 |             except Exception as e:
1078 |                 logger.exception("Operation failed without recovery:")
     |

libs/multi_agent/agent_pool.py:1108:17: TRY300 Consider moving this statement to an `else` block
     |
1107 |                 logger.info("Operation completed successfully: %s", description)
1108 |                 return True, result
     |                 ^^^^^^^^^^^^^^^^^^^ TRY300
1109 |
1110 |             except Exception as e:
     |

libs/multi_agent/auto_resolver.py:93:9: PLR0917 Too many positional arguments (6/5)
   |
91 |     """Comprehensive automatic conflict resolution system."""
92 |
93 |     def __init__(
   |         ^^^^^^^^ PLR0917
94 |         self,
95 |         semantic_analyzer: SemanticAnalyzer,
   |

libs/multi_agent/auto_resolver.py:266:13: TRY300 Consider moving this statement to an `else` block
    |
265 |             logger.info("Auto-resolution completed: %s", outcome.value)
266 |             return result
    |             ^^^^^^^^^^^^^ TRY300
267 |
268 |         except Exception as e:
    |

libs/multi_agent/auto_resolver.py:517:13: TRY300 Consider moving this statement to an `else` block
    |
515 |             # 5. Commit changes
516 |
517 |             return True
    |             ^^^^^^^^^^^ TRY300
518 |
519 |         except Exception:
    |

libs/multi_agent/auto_resolver.py:585:15: PLR6301 Method `_apply_preventive_measures` could be a function, class method, or static method
    |
583 |         return strategies
584 |
585 |     async def _apply_preventive_measures(
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^ PLR6301
586 |         self,
587 |         predictions: list[PredictionResult],  # noqa: ARG002
    |

libs/multi_agent/auto_resolver.py:619:9: ARG004 Unused static method argument: `strategy`
    |
617 |     async def _apply_preventive_measure(
618 |         measure: str,
619 |         strategy: dict[str],  # noqa: ARG002
    |         ^^^^^^^^ ARG004
620 |     ) -> bool:
621 |         """Apply a specific preventive measure."""
    |

libs/multi_agent/auto_resolver.py:775:40: UP031 Use format specifiers instead of percent format
    |
773 |         for pattern, failures in common_failures:
774 |             if len(failures) > MIN_FAILURE_COUNT:
775 |                 recommendations.append("Improve resolution strategy for %s conflicts (failed %d times)" % (pattern, len(failures)))
    |                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ UP031
776 |
777 |         return recommendations
    |
    = help: Replace with format specifiers

libs/multi_agent/branch_manager.py:286:13: TRY300 Consider moving this statement to an `else` block
    |
284 |             self._run_git_command(["checkout", branch_name])
285 |             logger.info("Switched to branch: %s", branch_name)
286 |             return True
    |             ^^^^^^^^^^^ TRY300
287 |         except subprocess.CalledProcessError:
288 |             logger.exception("Failed to switch branch")
    |

libs/multi_agent/branch_test_manager.py:559:13: TRY300 Consider moving this statement to an `else` block
    |
558 |             await process.communicate()
559 |             return process.returncode == 0
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
560 |
561 |         except Exception:
    |

libs/multi_agent/branch_test_manager.py:636:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
634 |         test_type: TestType,
635 |         command: list[str],
636 |         **kwargs: Any,
    |                   ^^^ ANN401
637 |     ) -> None:
638 |         """Configure or update a test suite."""
    |

libs/multi_agent/branch_test_manager.py:664:9: PLR1702 Too many nested blocks (6 > 5)
    |
662 |           # This would integrate with git hooks or file watching
663 |           # For now, we'll implement a simple polling mechanism
664 | /         while self.auto_testing_enabled:
665 | |             try:
666 | |                 # Check for new commits on active branches
667 | |                 try:
668 | |                     active_branches = self.branch_manager.list_active_branches()
669 | |                 except Exception as e:
670 | |                     logger.warning("Failed to get active branches for monitoring: %s", e)
671 | |                     await asyncio.sleep(60)
672 | |                     continue
673 | |
674 | |                 for branch_info in active_branches:
675 | |                     # Check if branch has new commits since last test
676 | |                     last_test_time = self._get_last_test_time(branch_info.name)
677 | |
678 | |                     # Get last commit time (simplified check)
679 | |                     try:
680 | |                         result = subprocess.run(
681 | |                             [
682 | |                                 "git",
683 | |                                 "log",
684 | |                                 "-1",
685 | |                                 "--pretty=format:%ct",
686 | |                                 branch_info.name,
687 | |                             ],
688 | |                             check=False,
689 | |                             cwd=self.repo_path,
690 | |                             capture_output=True,
691 | |                             text=True,
692 | |                             timeout=10,
693 | |                         )
694 | |
695 | |                         if result.returncode == 0 and result.stdout.strip():
696 | |                             last_commit_time = datetime.fromtimestamp(int(result.stdout.strip()), tz=UTC)
697 | |
698 | |                             # Run tests if there are new commits
699 | |                             if last_commit_time > last_test_time:
700 | |                                 logger.info("New commits detected on %s, running tests", branch_info.name)
701 | |                                 await self.auto_test_on_commit(branch_info.name)
702 | |
703 | |                     except Exception as e:
704 | |                         logger.debug("Error checking commits for %s: %s", branch_info.name, e)
705 | |
706 | |                 # Wait before next check
707 | |                 await asyncio.sleep(60)  # Check every minute
708 | |
709 | |             except Exception:
710 | |                 logger.exception("Error in test monitor")
711 | |                 await asyncio.sleep(60)
    | |_______________________________________^ PLR1702
712 |
713 |       def _get_last_test_time(self, branch_name: str) -> datetime:
    |

libs/multi_agent/code_review_engine.py:246:15: PLR0917 Too many positional arguments (6/5)
    |
244 |             await asyncio.gather(*tasks, return_exceptions=True)
245 |
246 |     async def initiate_review(
    |               ^^^^^^^^^^^^^^^ PLR0917
247 |         self,
248 |         branch_name: str,
    |

libs/multi_agent/code_review_engine.py:749:9: PLR1702 Too many nested blocks (6 > 5)
    |
747 |           findings = []
748 |
749 | /         for file_path in file_paths:
750 | |             if not file_path.endswith(".py"):
751 | |                 continue
752 | |
753 | |             full_path = self.repo_path / file_path
754 | |             if not full_path.exists():
755 | |                 continue
756 | |
757 | |             try:
758 | |                 with full_path.open(encoding="utf-8") as f:
759 | |                     content = f.read()
760 | |
761 | |                 # Parse AST for performance analysis
762 | |                 tree = ast.parse(content)
763 | |
764 | |                 for node in ast.walk(tree):
765 | |                     # Check for inefficient loops
766 | |                     if isinstance(node, ast.For):
767 | |                         if isinstance(node.iter, ast.Call) and isinstance(node.iter.func, ast.Name) and node.iter.func.id == "range":
768 | |                             # Check for range(len(list)) pattern
769 | |                             if len(node.iter.args) == 1 and isinstance(node.iter.args[0], ast.Call) and isinstance(node.iter.args[0].â€¦
770 | |                                 finding = ReviewFinding(
771 | |                                     finding_id=f"perf_range_len_{file_path}_{node.lineno}",
772 | |                                     review_type=ReviewType.PERFORMANCE,
773 | |                                     severity=ReviewSeverity.MEDIUM,
774 | |                                     file_path=file_path,
775 | |                                     line_number=node.lineno,
776 | |                                     message="Consider using enumerate() instead of range(len())",
777 | |                                     description="Using enumerate() is more Pythonic and potentially faster",
778 | |                                     suggestion="Replace 'for i in range(len(items)):' with 'for i, item in enumerate(items):'",
779 | |                                 )
780 | |                                 findings.append(finding)
781 | |
782 | |                     # Check for string concatenation in loops
783 | |                     elif (
784 | |                         isinstance(node, ast.AugAssign)
785 | |                         and isinstance(
786 | |                             node.op,
787 | |                             ast.Add,
788 | |                         )
789 | |                         and isinstance(node.target, ast.Name)
790 | |                     ):
791 | |                         # This is a simplified check - could be more sophisticated
792 | |                         finding = ReviewFinding(
793 | |                             finding_id=f"perf_string_concat_{file_path}_{node.lineno}",
794 | |                             review_type=ReviewType.PERFORMANCE,
795 | |                             severity=ReviewSeverity.LOW,
796 | |                             file_path=file_path,
797 | |                             line_number=node.lineno,
798 | |                             message="Consider using join() for string concatenation",
799 | |                             description="String concatenation in loops can be inefficient",
800 | |                             suggestion="Use ''.join(list) for better performance when concatenating many strings",
801 | |                         )
802 | |                         findings.append(finding)
803 | |
804 | |             except Exception:
805 | |                 logger.exception("Error checking performance for %s:", file_path)
    | |_________________________________________________________________________________^ PLR1702
806 |
807 |           return findings
    |

libs/multi_agent/code_review_engine.py:926:9: PLR1702 Too many nested blocks (6 > 5)
    |
924 |           findings = []
925 |
926 | /         for file_path in file_paths:
927 | |             if not file_path.endswith(".py"):
928 | |                 continue
929 | |
930 | |             full_path = self.repo_path / file_path
931 | |             if not full_path.exists():
932 | |                 continue
933 | |
934 | |             try:
935 | |                 with full_path.open(encoding="utf-8") as f:
936 | |                     content = f.read()
937 | |
938 | |                 tree = ast.parse(content)
939 | |
940 | |                 # Check for missing docstrings
941 | |                 for node in ast.walk(tree):
942 | |                     if isinstance(
943 | |                         node,
944 | |                         ast.FunctionDef | ast.AsyncFunctionDef | ast.ClassDef,
945 | |                     ):
946 | |                         # Check if function/class has docstring
947 | |                         has_docstring = len(node.body) > 0 and isinstance(node.body[0], ast.Expr) and isinstance(node.body[0].value, â€¦
948 | |
949 | |                         if not has_docstring:
950 | |                             # Skip private methods (start with _) unless they're special methods
951 | |                             if node.name.startswith("_") and not (node.name.startswith("__") and node.name.endswith("__")):
952 | |                                 continue
953 | |
954 | |                             severity = ReviewSeverity.LOW
955 | |                             if isinstance(
956 | |                                 node,
957 | |                                 ast.ClassDef,
958 | |                             ) or not node.name.startswith("_"):
959 | |                                 severity = ReviewSeverity.MEDIUM
960 | |
961 | |                             finding = ReviewFinding(
962 | |                                 finding_id=f"doc_missing_{file_path}_{node.lineno}",
963 | |                                 review_type=ReviewType.DOCUMENTATION,
964 | |                                 severity=severity,
965 | |                                 file_path=file_path,
966 | |                                 line_number=node.lineno,
967 | |                                 message=f"Missing docstring for {type(node).__name__.lower()} '{node.name}'",
968 | |                                 description="Public functions and classes should have docstrings",
969 | |                                 suggestion="Add a docstring describing the purpose, parameters, and return value",
970 | |                             )
971 | |                             findings.append(finding)
972 | |
973 | |             except Exception:
974 | |                 logger.exception("Error checking documentation for %s:", file_path)
    | |___________________________________________________________________________________^ PLR1702
975 |
976 |           return findings
    |

libs/multi_agent/code_review_engine.py:926:9: PLR1702 Too many nested blocks (6 > 5)
    |
924 |           findings = []
925 |
926 | /         for file_path in file_paths:
927 | |             if not file_path.endswith(".py"):
928 | |                 continue
929 | |
930 | |             full_path = self.repo_path / file_path
931 | |             if not full_path.exists():
932 | |                 continue
933 | |
934 | |             try:
935 | |                 with full_path.open(encoding="utf-8") as f:
936 | |                     content = f.read()
937 | |
938 | |                 tree = ast.parse(content)
939 | |
940 | |                 # Check for missing docstrings
941 | |                 for node in ast.walk(tree):
942 | |                     if isinstance(
943 | |                         node,
944 | |                         ast.FunctionDef | ast.AsyncFunctionDef | ast.ClassDef,
945 | |                     ):
946 | |                         # Check if function/class has docstring
947 | |                         has_docstring = len(node.body) > 0 and isinstance(node.body[0], ast.Expr) and isinstance(node.body[0].value, â€¦
948 | |
949 | |                         if not has_docstring:
950 | |                             # Skip private methods (start with _) unless they're special methods
951 | |                             if node.name.startswith("_") and not (node.name.startswith("__") and node.name.endswith("__")):
952 | |                                 continue
953 | |
954 | |                             severity = ReviewSeverity.LOW
955 | |                             if isinstance(
956 | |                                 node,
957 | |                                 ast.ClassDef,
958 | |                             ) or not node.name.startswith("_"):
959 | |                                 severity = ReviewSeverity.MEDIUM
960 | |
961 | |                             finding = ReviewFinding(
962 | |                                 finding_id=f"doc_missing_{file_path}_{node.lineno}",
963 | |                                 review_type=ReviewType.DOCUMENTATION,
964 | |                                 severity=severity,
965 | |                                 file_path=file_path,
966 | |                                 line_number=node.lineno,
967 | |                                 message=f"Missing docstring for {type(node).__name__.lower()} '{node.name}'",
968 | |                                 description="Public functions and classes should have docstrings",
969 | |                                 suggestion="Add a docstring describing the purpose, parameters, and return value",
970 | |                             )
971 | |                             findings.append(finding)
972 | |
973 | |             except Exception:
974 | |                 logger.exception("Error checking documentation for %s:", file_path)
    | |___________________________________________________________________________________^ PLR1702
975 |
976 |           return findings
    |

libs/multi_agent/code_review_engine.py:1286:9: PLR0914 Too many local variables (16/15)
     |
1284 |                 await asyncio.sleep(3600)
1285 |
1286 |     def get_review_summary(self) -> ReviewSummary:
     |         ^^^^^^^^^^^^^^^^^^ PLR0914
1287 |         """Get summary of all reviews.
     |

libs/multi_agent/collaboration_engine.py:220:15: PLR0917 Too many positional arguments (8/5)
    |
218 |         self._tasks.clear()
219 |
220 |     async def send_message(
    |               ^^^^^^^^^^^^ PLR0917
221 |         self,
222 |         sender_id: str,
    |

libs/multi_agent/conflict_prediction.py:565:9: ARG004 Unused static method argument: `branch1`
    |
563 |     @staticmethod
564 |     async def _detect_api_changes(
565 |         branch1: str,  # noqa: ARG002
    |         ^^^^^^^ ARG004
566 |         branch2: str,  # noqa: ARG002
567 |         vector: ConflictVector,  # noqa: ARG002
    |

libs/multi_agent/conflict_prediction.py:566:9: ARG004 Unused static method argument: `branch2`
    |
564 |     async def _detect_api_changes(
565 |         branch1: str,  # noqa: ARG002
566 |         branch2: str,  # noqa: ARG002
    |         ^^^^^^^ ARG004
567 |         vector: ConflictVector,  # noqa: ARG002
568 |     ) -> PredictionResult | None:
    |

libs/multi_agent/conflict_prediction.py:567:9: ARG004 Unused static method argument: `vector`
    |
565 |         branch1: str,  # noqa: ARG002
566 |         branch2: str,  # noqa: ARG002
567 |         vector: ConflictVector,  # noqa: ARG002
    |         ^^^^^^ ARG004
568 |     ) -> PredictionResult | None:
569 |         """Detect potential API breaking changes."""
    |

libs/multi_agent/conflict_prediction.py:576:9: ARG004 Unused static method argument: `branch1`
    |
574 |     @staticmethod
575 |     async def _detect_resource_conflicts(
576 |         branch1: str,  # noqa: ARG002
    |         ^^^^^^^ ARG004
577 |         branch2: str,  # noqa: ARG002
578 |         vector: ConflictVector,  # noqa: ARG002
    |

libs/multi_agent/conflict_prediction.py:577:9: ARG004 Unused static method argument: `branch2`
    |
575 |     async def _detect_resource_conflicts(
576 |         branch1: str,  # noqa: ARG002
577 |         branch2: str,  # noqa: ARG002
    |         ^^^^^^^ ARG004
578 |         vector: ConflictVector,  # noqa: ARG002
579 |     ) -> PredictionResult | None:
    |

libs/multi_agent/conflict_prediction.py:578:9: ARG004 Unused static method argument: `vector`
    |
576 |         branch1: str,  # noqa: ARG002
577 |         branch2: str,  # noqa: ARG002
578 |         vector: ConflictVector,  # noqa: ARG002
    |         ^^^^^^ ARG004
579 |     ) -> PredictionResult | None:
580 |         """Detect potential resource contention conflicts."""
    |

libs/multi_agent/conflict_prediction.py:586:9: ARG004 Unused static method argument: `branch1`
    |
584 |     @staticmethod
585 |     async def _detect_context_loss(
586 |         branch1: str,  # noqa: ARG002
    |         ^^^^^^^ ARG004
587 |         branch2: str,  # noqa: ARG002
588 |         vector: ConflictVector,  # noqa: ARG002
    |

libs/multi_agent/conflict_prediction.py:587:9: ARG004 Unused static method argument: `branch2`
    |
585 |     async def _detect_context_loss(
586 |         branch1: str,  # noqa: ARG002
587 |         branch2: str,  # noqa: ARG002
    |         ^^^^^^^ ARG004
588 |         vector: ConflictVector,  # noqa: ARG002
589 |     ) -> PredictionResult | None:
    |

libs/multi_agent/conflict_prediction.py:588:9: ARG004 Unused static method argument: `vector`
    |
586 |         branch1: str,  # noqa: ARG002
587 |         branch2: str,  # noqa: ARG002
588 |         vector: ConflictVector,  # noqa: ARG002
    |         ^^^^^^ ARG004
589 |     ) -> PredictionResult | None:
590 |         """Detect potential merge context loss scenarios."""
    |

libs/multi_agent/conflict_prediction.py:666:46: ARG004 Unused static method argument: `branch1`
    |
665 |     @staticmethod
666 |     async def _calculate_dependency_coupling(branch1: str, branch2: str) -> float:  # noqa: ARG002  # noqa: ARG004
    |                                              ^^^^^^^ ARG004
667 |         """Calculate dependency coupling between branches."""
668 |         # Simplified implementation
    |

libs/multi_agent/conflict_prediction.py:666:60: ARG004 Unused static method argument: `branch2`
    |
665 |     @staticmethod
666 |     async def _calculate_dependency_coupling(branch1: str, branch2: str) -> float:  # noqa: ARG002  # noqa: ARG004
    |                                                            ^^^^^^^ ARG004
667 |         """Calculate dependency coupling between branches."""
668 |         # Simplified implementation
    |

libs/multi_agent/conflict_prediction.py:672:44: ARG004 Unused static method argument: `branch1`
    |
671 |     @staticmethod
672 |     async def _calculate_semantic_distance(branch1: str, branch2: str) -> float:  # noqa: ARG002  # noqa: ARG004
    |                                            ^^^^^^^ ARG004
673 |         """Calculate semantic distance between branches."""
674 |         # Simplified implementation
    |

libs/multi_agent/conflict_prediction.py:672:58: ARG004 Unused static method argument: `branch2`
    |
671 |     @staticmethod
672 |     async def _calculate_semantic_distance(branch1: str, branch2: str) -> float:  # noqa: ARG002  # noqa: ARG004
    |                                                          ^^^^^^^ ARG004
673 |         """Calculate semantic distance between branches."""
674 |         # Simplified implementation
    |

libs/multi_agent/conflict_prediction.py:678:45: ARG004 Unused static method argument: `branch1`
    |
677 |     @staticmethod
678 |     async def _calculate_temporal_proximity(branch1: str, branch2: str) -> float:  # noqa: ARG002  # noqa: ARG004
    |                                             ^^^^^^^ ARG004
679 |         """Calculate temporal proximity of changes."""
680 |         # Simplified implementation
    |

libs/multi_agent/conflict_prediction.py:678:59: ARG004 Unused static method argument: `branch2`
    |
677 |     @staticmethod
678 |     async def _calculate_temporal_proximity(branch1: str, branch2: str) -> float:  # noqa: ARG002  # noqa: ARG004
    |                                                           ^^^^^^^ ARG004
679 |         """Calculate temporal proximity of changes."""
680 |         # Simplified implementation
    |

libs/multi_agent/conflict_prediction.py:774:9: PLR1702 Too many nested blocks (6 > 5)
    |
772 |           """Extract symbol definitions from a branch."""
773 |           symbols = {}
774 | /         try:
775 | |             python_files = await self.conflict_engine._get_python_files_changed(branch)  # noqa: SLF001
776 | |             for file_path in python_files:
777 | |                 content = await self.conflict_engine._get_file_content(  # noqa: SLF001
778 | |                     file_path,
779 | |                     branch,
780 | |                 )
781 | |                 if content:
782 | |                     try:
783 | |                         tree = ast.parse(content)
784 | |                         for node in ast.walk(tree):
785 | |                             if isinstance(
786 | |                                 node,
787 | |                                 ast.ClassDef | ast.FunctionDef | ast.AsyncFunctionDef,
788 | |                             ):
789 | |                                 symbols[node.name] = f"{file_path}:{node.lineno}"
790 | |                     except SyntaxError:
791 | |                         pass
792 | |         except Exception:
793 | |             logger.exception("Error extracting symbol definitions:")
    | |____________________________________________________________________^ PLR1702
794 |           return symbols
    |

libs/multi_agent/conflict_prediction.py:799:9: PLR1702 Too many nested blocks (8 > 5)
    |
797 |           """Extract class inheritance hierarchies."""
798 |           hierarchies = {}
799 | /         try:
800 | |             python_files = await self.conflict_engine._get_python_files_changed(branch)  # noqa: SLF001
801 | |             for file_path in python_files:
802 | |                 content = await self.conflict_engine._get_file_content(  # noqa: SLF001
803 | |                     file_path,
804 | |                     branch,
805 | |                 )
806 | |                 if content:
807 | |                     try:
808 | |                         tree = ast.parse(content)
809 | |                         for node in ast.walk(tree):
810 | |                             if isinstance(node, ast.ClassDef):
811 | |                                 bases = []
812 | |                                 for base in node.bases:
813 | |                                     if isinstance(base, ast.Name):
814 | |                                         bases.append(base.id)
815 | |                                     elif isinstance(base, ast.Attribute):
816 | |                                         bases.append(ast.unparse(base))
817 | |                                 hierarchies[node.name] = bases
818 | |                     except SyntaxError:
819 | |                         pass
820 | |         except Exception:
821 | |             logger.exception("Error extracting class hierarchies:")
    | |___________________________________________________________________^ PLR1702
822 |           return hierarchies
    |

libs/multi_agent/conflict_prevention.py:194:15: PLR0914 Too many local variables (16/15)
    |
192 |         logger.info("Stopped conflict prevention monitoring")
193 |
194 |     async def analyze_and_prevent_conflicts(
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ PLR0914
195 |         self,
196 |         branches: list[str],
    |

libs/multi_agent/conflict_prevention.py:307:35: F821 Undefined name `ConflictPrevention`
    |
305 |         # Determine strategy based on conflict pattern
306 |         if prediction.pattern == ConflictPattern.OVERLAPPING_IMPORTS:
307 |             measures.extend(await ConflictPrevention._generate_dependency_measures(prediction))
    |                                   ^^^^^^^^^^^^^^^^^^ F821
308 |         elif prediction.pattern == ConflictPattern.FUNCTION_SIGNATURE_DRIFT:
309 |             measures.extend(await ConflictPrevention._generate_coordination_measures(prediction))
    |

libs/multi_agent/conflict_prevention.py:309:35: F821 Undefined name `ConflictPrevention`
    |
307 |             measures.extend(await ConflictPrevention._generate_dependency_measures(prediction))
308 |         elif prediction.pattern == ConflictPattern.FUNCTION_SIGNATURE_DRIFT:
309 |             measures.extend(await ConflictPrevention._generate_coordination_measures(prediction))
    |                                   ^^^^^^^^^^^^^^^^^^ F821
310 |         elif prediction.pattern == ConflictPattern.API_BREAKING_CHANGE:
311 |             measures.extend(await ConflictPrevention._generate_interface_measures(prediction))
    |

libs/multi_agent/conflict_prevention.py:311:35: F821 Undefined name `ConflictPrevention`
    |
309 |             measures.extend(await ConflictPrevention._generate_coordination_measures(prediction))
310 |         elif prediction.pattern == ConflictPattern.API_BREAKING_CHANGE:
311 |             measures.extend(await ConflictPrevention._generate_interface_measures(prediction))
    |                                   ^^^^^^^^^^^^^^^^^^ F821
312 |         elif prediction.pattern == ConflictPattern.RESOURCE_CONTENTION:
313 |             measures.extend(await ConflictPrevention._generate_temporal_measures(prediction))
    |

libs/multi_agent/conflict_prevention.py:313:35: F821 Undefined name `ConflictPrevention`
    |
311 |             measures.extend(await ConflictPrevention._generate_interface_measures(prediction))
312 |         elif prediction.pattern == ConflictPattern.RESOURCE_CONTENTION:
313 |             measures.extend(await ConflictPrevention._generate_temporal_measures(prediction))
    |                                   ^^^^^^^^^^^^^^^^^^ F821
314 |         else:
315 |             # Generic measures
    |

libs/multi_agent/conflict_prevention.py:495:13: TRY300 Consider moving this statement to an `else` block
    |
493 |                 logger.warning("Failed to apply measure: %s", measure.measure_id)
494 |
495 |             return success
    |             ^^^^^^^^^^^^^^ TRY300
496 |
497 |         except Exception as e:
    |

libs/multi_agent/conflict_prevention.py:657:13: TRY300 Consider moving this statement to an `else` block
    |
655 |             branches: list[str] = []
656 |             # This would be implemented based on git activity, agent assignments, etc.
657 |             return branches
    |             ^^^^^^^^^^^^^^^ TRY300
658 |         except Exception:
659 |             logger.exception("Error getting active branches")
    |

libs/multi_agent/conflict_resolution.py:114:51: F821 Undefined name `ConflictResolver`
    |
112 |             ResolutionStrategy.PREFER_MAIN: self._prefer_main_strategy,
113 |             ResolutionStrategy.CUSTOM_MERGE: self._custom_merge_strategy,
114 |             ResolutionStrategy.SEMANTIC_ANALYSIS: ConflictResolver._semantic_analysis_strategy,
    |                                                   ^^^^^^^^^^^^^^^^ F821
115 |         }
    |

libs/multi_agent/dependency_propagation.py:192:15: PLR0917 Too many positional arguments (6/5)
    |
190 |             await asyncio.gather(*tasks, return_exceptions=True)
191 |
192 |     async def track_dependency_change(
    |               ^^^^^^^^^^^^^^^^^^^^^^^ PLR0917
193 |         self,
194 |         file_path: str,
    |

libs/multi_agent/dependency_propagation.py:498:15: PLR6301 Method `_analyze_change_impact` could be a function, class method, or static method
    |
496 |                     self.dependency_graph[matching_file].dependents.add(file_path)
497 |
498 |     async def _analyze_change_impact(
    |               ^^^^^^^^^^^^^^^^^^^^^^ PLR6301
499 |         self,
500 |         file_path: str,  # noqa: ARG002
    |

libs/multi_agent/dependency_propagation.py:628:9: PLR6301 Method `_calculate_complexity_score` could be a function, class method, or static method
    |
626 |         return indirect_files
627 |
628 |     def _calculate_complexity_score(self, node: DependencyNode) -> float:
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^ PLR6301
629 |         """Calculate complexity score for a dependency node.
    |

libs/multi_agent/dependency_propagation.py:641:9: PLR6301 Method `_calculate_risk_level` could be a function, class method, or static method
    |
639 |         return min(10.0, dependency_factor + dependent_factor + export_factor)
640 |
641 |     def _calculate_risk_level(self, complexity_score: float, total_impact: int) -> str:
    |         ^^^^^^^^^^^^^^^^^^^^^ PLR6301
642 |         """Calculate risk level based on complexity and impact.
    |

libs/multi_agent/dependency_propagation.py:742:15: PLR6301 Method `_propagate_change_to_branches` could be a function, class method, or static method
    |
740 |                 )
741 |
742 |     async def _propagate_change_to_branches(
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ PLR6301
743 |         self,
744 |         change: DependencyChange,
    |

libs/multi_agent/graph.py:21:47: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**attrs`
   |
19 |         self.reverse_edges: dict[str, set[str]] = defaultdict(set)
20 |
21 |     def add_node(self, node_id: str, **attrs: Any) -> None:
   |                                               ^^^ ANN401
22 |         """Add a node with attributes."""
23 |         if node_id not in self.nodes:
   |

libs/multi_agent/graph.py:28:59: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**attrs`
   |
26 |             self.nodes[node_id].update(attrs)
27 |
28 |     def add_edge(self, source: str, target: str, **attrs: Any) -> None:
   |                                                           ^^^ ANN401
29 |         """Add an edge with attributes."""
30 |         # Ensure nodes exist
   |

libs/multi_agent/recovery_engine.py:266:9: PLR0917 Too many positional arguments (7/5)
    |
264 |         )
265 |
266 |     def register_recovery_strategy(
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^ PLR0917
267 |         self,
268 |         name: str,
    |

libs/multi_agent/recovery_engine.py:292:15: PLR0917 Too many positional arguments (6/5)
    |
290 |         logger.info("Registered recovery strategy: {name}")
291 |
292 |     async def create_snapshot(
    |               ^^^^^^^^^^^^^^^ PLR0917
293 |         self,
294 |         operation_type: OperationType,
    |

libs/multi_agent/recovery_engine.py:369:13: TRY300 Consider moving this statement to an `else` block
    |
367 |             await self._cleanup_old_snapshots()
368 |
369 |             return snapshot_id
    |             ^^^^^^^^^^^^^^^^^^ TRY300
370 |
371 |         except Exception:
    |

libs/multi_agent/recovery_engine.py:442:13: TRY300 Consider moving this statement to an `else` block
    |
441 |             logger.info("Successfully rolled back to snapshot {snapshot_id}")
442 |             return True
    |             ^^^^^^^^^^^ TRY300
443 |
444 |         except Exception:
    |

libs/multi_agent/recovery_engine.py:451:9: PLR1702 Too many nested blocks (6 > 5)
    |
449 |       async def _restore_agent_states(agent_pool: object | None, agent_states: dict[str, dict[str, str | int | bool | list[str]]]) -> Nâ€¦
450 |           """Restore agent states from snapshot."""
451 | /         for agent_id, agent_data in agent_states.items():
452 | |             try:
453 | |                 if agent_id in agent_pool.agents:
454 | |                     # Update existing agent
455 | |                     agent = agent_pool.agents[agent_id]
456 | |                     agent.state = AgentState(agent_data["state"])
457 | |                     agent.current_task = agent_data.get("current_task")
458 | |                     agent.branch_name = agent_data.get("branch_name")
459 | |
460 | |                     # Terminate any running process
461 | |                     if agent.process:
462 | |                         try:
463 | |                             agent.process.terminate()
464 | |                             await asyncio.wait_for(agent.process.wait(), timeout=5)
465 | |                         except (TimeoutError, ProcessLookupError):
466 | |                             with contextlib.suppress(Exception):
467 | |                                 agent.process.kill()
468 | |                         agent.process = None
469 | |                 else:
470 | |                     # Recreate agent
471 | |                     agent = Agent.from_dict(agent_data)
472 | |                     agent.process = None  # Don't restore processes
473 | |                     agent_pool.agents[agent_id] = agent
474 | |
475 | |             except (KeyError, AttributeError, TypeError, ValueError) as e:
476 | |                 logger.warning("Failed to restore agent %s: %s", agent_id, e)
    | |_____________________________________________________________________________^ PLR1702
477 |
478 |       @staticmethod
    |

libs/multi_agent/recovery_engine.py:644:13: TRY300 Consider moving this statement to an `else` block
    |
642 |                 await self._escalate_failure(operation_id, exception, context)
643 |
644 |             return False
    |             ^^^^^^^^^^^^ TRY300
645 |
646 |         except Exception:
    |

libs/multi_agent/recovery_engine.py:679:15: PLR0917 Too many positional arguments (6/5)
    |
677 |         return self.recovery_strategies.get("generic_failure")
678 |
679 |     async def _execute_recovery_action(
    |               ^^^^^^^^^^^^^^^^^^^^^^^^ PLR0917
680 |         self,
681 |         action: RecoveryAction,
    |

libs/multi_agent/recovery_engine.py:689:9: PLR1702 Too many nested blocks (7 > 5)
    |
687 |       ) -> bool:
688 |           """Execute a specific recovery action."""
689 | /         try:
690 | |             if action == RecoveryAction.RETRY:
691 | |                 # Retry logic should be handled by the caller
692 | |                 return False
693 | |
694 | |             if action == RecoveryAction.ROLLBACK:
695 | |                 if snapshot_id:
696 | |                     return await self.rollback_operation(
697 | |                         snapshot_id,
698 | |                         agent_pool,
699 | |                         branch_manager,
700 | |                     )
701 | |                 logger.warning("No snapshot available for rollback")
702 | |                 return False
703 | |
704 | |             if action == RecoveryAction.RESET_AGENT:
705 | |                 if agent_pool and context.get("agent_id"):
706 | |                     agent_id = context["agent_id"]
707 | |                     if agent_id in agent_pool.agents:
708 | |                         agent = agent_pool.agents[agent_id]
709 | |
710 | |                         # Terminate any running process
711 | |                         if agent.process:
712 | |                             try:
713 | |                                 agent.process.terminate()
714 | |                                 await asyncio.wait_for(agent.process.wait(), timeout=5)
715 | |                             except (TimeoutError, ProcessLookupError):
716 | |                                 with contextlib.suppress(Exception):
717 | |                                     agent.process.kill()
718 | |                             agent.process = None
719 | |
720 | |                         # Reset agent state
721 | |                         agent.state = AgentState.IDLE
722 | |                         agent.current_task = None
723 | |
724 | |                         logger.info("Reset agent {agent_id}")
725 | |                         return True
726 | |                 return False
727 | |
728 | |             if action == RecoveryAction.RESTORE_STATE:
729 | |                 if snapshot_id:
730 | |                     return await self.rollback_operation(
731 | |                         snapshot_id,
732 | |                         agent_pool,
733 | |                         branch_manager,
734 | |                         restore_files=False,
735 | |                     )
736 | |                 return False
737 | |
738 | |             if action == RecoveryAction.SKIP:
739 | |                 # Mark operation as skipped
740 | |                 logger.info("Skipping failed operation")
741 | |                 return True
742 | |
743 | |             if action == RecoveryAction.ESCALATE:
744 | |                 await self._escalate_failure(context.get("operation_id", "unknown"), exception, context)
745 | |                 return False
746 | |
747 | |             return False
748 | |
749 | |         except Exception:
750 | |             logger.exception(f"Failed to execute recovery action {action.value}")  # noqa: G004
751 | |             return False
    | |________________________^ PLR1702
752 |
753 |       async def _escalate_failure(
    |

libs/multi_agent/recovery_engine.py:747:13: TRY300 Consider moving this statement to an `else` block
    |
745 |                 return False
746 |
747 |             return False
    |             ^^^^^^^^^^^^ TRY300
748 |
749 |         except Exception:
    |

libs/multi_agent/semantic_analyzer.py:294:13: TRY300 Consider moving this statement to an `else` block
    |
292 |             self.semantic_contexts[cache_key] = context
293 |
294 |             return context
    |             ^^^^^^^^^^^^^^ TRY300
295 |
296 |         except Exception:
    |

libs/multi_agent/semantic_analyzer.py:300:9: PLR6301 Method `_extract_semantic_context` could be a function, class method, or static method
    |
298 |             return None
299 |
300 |     def _extract_semantic_context(
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^ PLR6301
301 |         self,
302 |         file_path: str,
    |

libs/multi_agent/semantic_analyzer.py:465:9: PLR6301 Method `_detect_import_conflicts` could be a function, class method, or static method
    |
463 |         return conflicts
464 |
465 |     def _detect_import_conflicts(
    |         ^^^^^^^^^^^^^^^^^^^^^^^^ PLR6301
466 |         self,
467 |         context1: SemanticContext,
    |

libs/multi_agent/semantic_analyzer.py:509:9: PLR6301 Method `_detect_variable_conflicts` could be a function, class method, or static method
    |
507 |         return conflicts
508 |
509 |     def _detect_variable_conflicts(
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^ PLR6301
510 |         self,
511 |         context1: SemanticContext,
    |

libs/multi_agent/semantic_analyzer.py:550:9: PLR0917 Too many positional arguments (6/5)
    |
548 |         return conflicts
549 |
550 |     def _detect_method_conflicts(
    |         ^^^^^^^^^^^^^^^^^^^^^^^^ PLR0917
551 |         self,
552 |         class1: ClassDefinition,
    |

libs/multi_agent/semantic_analyzer.py:622:9: PLR6301 Method `_assess_function_conflict_severity` could be a function, class method, or static method
    |
620 |         return func1.decorators != func2.decorators
621 |
622 |     def _assess_function_conflict_severity(
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ PLR6301
623 |         self,
624 |         func1: FunctionSignature,
    |

libs/multi_agent/semantic_analyzer.py:654:9: PLR6301 Method `_analyze_function_impact` could be a function, class method, or static method
    |
652 |         return ConflictSeverity.LOW
653 |
654 |     def _analyze_function_impact(
    |         ^^^^^^^^^^^^^^^^^^^^^^^^ PLR6301
655 |         self,
656 |         func1: FunctionSignature,
    |

libs/multi_agent/semantic_analyzer.py:748:9: PLR6301 Method `_rank_conflicts_by_impact` could be a function, class method, or static method
    |
746 |         return signature
747 |
748 |     def _rank_conflicts_by_impact(
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^ PLR6301
749 |         self,
750 |         conflicts: list[SemanticConflict],
    |

libs/multi_agent/semantic_analyzer.py:789:9: PLR6301 Method `_merge_related_conflicts` could be a function, class method, or static method
    |
787 |         return sorted(conflicts, key=conflict_priority, reverse=True)
788 |
789 |     def _merge_related_conflicts(
    |         ^^^^^^^^^^^^^^^^^^^^^^^^ PLR6301
790 |         self,
791 |         conflicts: list[SemanticConflict],
    |

libs/multi_agent/semantic_analyzer.py:803:15: PLR6301 Method `_get_changed_python_files` could be a function, class method, or static method
    |
801 |     # Helper methods
802 |
803 |     async def _get_changed_python_files(self, branch1: str, branch2: str) -> list[str]:  # noqa: ARG002
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^ PLR6301
804 |         """Get list of Python files changed between branches."""
805 |         try:
    |

libs/multi_agent/semantic_merger.py:313:15: PLR0917 Too many positional arguments (6/5)
    |
311 |         return results
312 |
313 |     async def _apply_merge_strategy(
    |               ^^^^^^^^^^^^^^^^^^^^^ PLR0917
314 |         self,
315 |         merge_id: str,
    |

libs/multi_agent/semantic_merger.py:906:9: ARG004 Unused static method argument: `conflict`
    |
904 |     @staticmethod
905 |     def _conflict_resolved_by_merge(
906 |         conflict: SemanticConflict,  # noqa: ARG002
    |         ^^^^^^^^ ARG004
907 |         merge_result: MergeResult,
908 |     ) -> bool:
    |

libs/multi_agent/semantic_merger.py:936:9: ARG004 Unused static method argument: `tree2`
    |
934 |     def _merge_ast_trees(
935 |         tree1: ast.AST,
936 |         tree2: ast.AST,  # noqa: ARG002
    |         ^^^^^ ARG004
937 |         conflicts: list[SemanticConflict],  # noqa: ARG002
938 |     ) -> ast.AST:
    |

libs/multi_agent/semantic_merger.py:937:9: ARG004 Unused static method argument: `conflicts`
    |
935 |         tree1: ast.AST,
936 |         tree2: ast.AST,  # noqa: ARG002
937 |         conflicts: list[SemanticConflict],  # noqa: ARG002
    |         ^^^^^^^^^ ARG004
938 |     ) -> ast.AST:
939 |         """Merge two AST trees intelligently."""
    |

libs/multi_agent/semantic_merger.py:995:9: ARG004 Unused static method argument: `func1`
    |
993 |     @staticmethod
994 |     def _merge_function_definitions(
995 |         func1: str,  # noqa: ARG002
    |         ^^^^^ ARG004
996 |         func2: str,
997 |         conflict: SemanticConflict,  # noqa: ARG002
    |

libs/multi_agent/semantic_merger.py:997:9: ARG004 Unused static method argument: `conflict`
    |
995 |         func1: str,  # noqa: ARG002
996 |         func2: str,
997 |         conflict: SemanticConflict,  # noqa: ARG002
    |         ^^^^^^^^ ARG004
998 |     ) -> dict[str]:
999 |         """Merge two function definitions."""
    |

libs/multi_agent/semantic_merger.py:1038:9: ARG004 Unused static method argument: `imports`
     |
1036 |     @staticmethod
1037 |     def _reconstruct_from_semantic_elements(
1038 |         imports: list,  # noqa: ARG002
     |         ^^^^^^^ ARG004
1039 |         functions: dict,  # noqa: ARG002
1040 |         classes: dict,  # noqa: ARG002
     |

libs/multi_agent/semantic_merger.py:1039:9: ARG004 Unused static method argument: `functions`
     |
1037 |     def _reconstruct_from_semantic_elements(
1038 |         imports: list,  # noqa: ARG002
1039 |         functions: dict,  # noqa: ARG002
     |         ^^^^^^^^^ ARG004
1040 |         classes: dict,  # noqa: ARG002
1041 |         content1: str,  # noqa: ARG002
     |

libs/multi_agent/semantic_merger.py:1040:9: ARG004 Unused static method argument: `classes`
     |
1038 |         imports: list,  # noqa: ARG002
1039 |         functions: dict,  # noqa: ARG002
1040 |         classes: dict,  # noqa: ARG002
     |         ^^^^^^^ ARG004
1041 |         content1: str,  # noqa: ARG002
1042 |         content2: str,
     |

libs/multi_agent/semantic_merger.py:1041:9: ARG004 Unused static method argument: `content1`
     |
1039 |         functions: dict,  # noqa: ARG002
1040 |         classes: dict,  # noqa: ARG002
1041 |         content1: str,  # noqa: ARG002
     |         ^^^^^^^^ ARG004
1042 |         content2: str,
1043 |     ) -> str:
     |

libs/multi_agent/task_analyzer.py:246:41: PLC0207 Instead of `str.split()`, call `str.rsplit()` and pass `maxsplit=1`
    |
244 |             return True
245 |
246 |         return bool("." in imported and imported.split(".")[-1] == module.split(".")[-1])
    |                                         ^^^^^^^^^^^^^^^^^^^^^^^ PLC0207
247 |
248 |     def create_task_from_files(
    |

libs/multi_agent/task_analyzer.py:246:68: PLC0207 Instead of `str.split()`, call `str.rsplit()` and pass `maxsplit=1`
    |
244 |             return True
245 |
246 |         return bool("." in imported and imported.split(".")[-1] == module.split(".")[-1])
    |                                                                    ^^^^^^^^^^^^^^^^^^^^^ PLC0207
247 |
248 |     def create_task_from_files(
    |

libs/multi_agent/task_analyzer.py:254:19: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
252 |         file_paths: list[str],
253 |         description: str = "",
254 |         **kwargs: Any,
    |                   ^^^ ANN401
255 |     ) -> TaskDefinition:
256 |         """Create a task definition from file paths.
    |

libs/multi_agent/types.py:3:8: S404 `subprocess` module is possibly insecure
  |
1 | # Copyright notice.
2 |
3 | import subprocess
  |        ^^^^^^^^^^ S404
4 | from dataclasses import asdict, dataclass, field
5 | from datetime import datetime
  |

libs/multi_agent/work_environment.py:7:8: S404 `subprocess` module is possibly insecure
  |
5 | import os
6 | import shutil
7 | import subprocess
  |        ^^^^^^^^^^ S404
8 | import venv
9 | from contextlib import contextmanager
  |

libs/multi_agent/work_environment.py:103:13: TRY300 Consider moving this statement to an `else` block
    |
101 |                 logger.error("Command failed: {result.stderr}")
102 |
103 |             return result
    |             ^^^^^^^^^^^^^ TRY300
104 |
105 |         except subprocess.TimeoutExpired:
    |

libs/multi_agent/work_environment.py:401:9: ANN201 Missing return type annotation for public function `work_in_environment`
    |
400 |     @contextmanager
401 |     def work_in_environment(self, branch_name: str):
    |         ^^^^^^^^^^^^^^^^^^^ ANN201
402 |         """Context manager to work in an environment."""
403 |         original_cwd = os.getcwd()
    |
    = help: Add return type annotation

libs/session_helpers.py:98:9: TRY300 Consider moving this statement to an `else` block
    |
 96 |         server = server or get_tmux_server()
 97 |         session = server.find_where({"session_name": session_name})
 98 |         return session is not None
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
 99 |     except Exception:
100 |         logger.exception("Error checking session existence")
    |

libs/session_helpers.py:331:17: TRY301 Abstract `raise` to an inner function
    |
329 |               if not window:
330 |                   msg = f"Window '{window_name}' not found in session '{session_name}'"
331 | /                 raise YesmanError(
332 | |                     msg,
333 | |                     category=ErrorCategory.VALIDATION,
334 | |                 )
    | |_________________^ TRY301
335 |           else:
336 |               # Get active window
    |

libs/session_helpers.py:343:13: TRY301 Abstract `raise` to an inner function
    |
341 |           if not active_pane:
342 |               msg = "No active pane found"
343 | /             raise YesmanError(
344 | |                 msg,
345 | |                 category=ErrorCategory.VALIDATION,
346 | |             )
    | |_____________^ TRY301
347 |
348 |           return PaneInfo(
    |

libs/session_helpers.py:369:5: PLR0917 Too many positional arguments (6/5)
    |
369 | def send_keys_to_pane(
    |     ^^^^^^^^^^^^^^^^^ PLR0917
370 |     session_name: str,
371 |     window_index: int,
    |

libs/session_helpers.py:407:13: TRY301 Abstract `raise` to an inner function
    |
405 |           if not window:
406 |               msg = f"Window index {window_index} not found in session '{session_name}'"
407 | /             raise YesmanError(
408 | |                 msg,
409 | |                 category=ErrorCategory.VALIDATION,
410 | |             )
    | |_____________^ TRY301
411 |
412 |           # Get pane by index
    |

libs/session_helpers.py:421:13: TRY301 Abstract `raise` to an inner function
    |
419 |           if not pane:
420 |               msg = f"Pane index {pane_index} not found in window {window_index}"
421 | /             raise YesmanError(
422 | |                 msg,
423 | |                 category=ErrorCategory.VALIDATION,
424 | |             )
    | |_____________^ TRY301
425 |
426 |           # Send keys
    |

libs/session_helpers.py:510:17: TRY301 Abstract `raise` to an inner function
    |
508 |               else:
509 |                   msg = f"Directory does not exist: {path}"
510 | /                 raise YesmanError(
511 | |                     msg,
512 | |                     category=ErrorCategory.VALIDATION,
513 | |                 )
    | |_________________^ TRY301
514 |
515 |           if not path.is_dir():
    |

libs/session_helpers.py:517:13: TRY301 Abstract `raise` to an inner function
    |
515 |           if not path.is_dir():
516 |               msg = f"Path exists but is not a directory: {path}"
517 | /             raise YesmanError(
518 | |                 msg,
519 | |                 category=ErrorCategory.VALIDATION,
520 | |             )
    | |_____________^ TRY301
521 |
522 |           return path
    |

libs/session_helpers.py:522:9: TRY300 Consider moving this statement to an `else` block
    |
520 |             )
521 |
522 |         return path
    |         ^^^^^^^^^^^ TRY300
523 |
524 |     except Exception as e:
    |

libs/task_runner.py:6:8: S404 `subprocess` module is possibly insecure
  |
4 | import glob
5 | import re
6 | import subprocess
  |        ^^^^^^^^^^ S404
7 | from datetime import UTC, datetime
8 | from pathlib import Path
  |

libs/task_runner.py:313:13: TRY300 Consider moving this statement to an `else` block
    |
311 |             )
312 |
313 |             return True
    |             ^^^^^^^^^^^ TRY300
314 |
315 |         except subprocess.CalledProcessError:
    |

libs/task_runner.py:334:13: TRY300 Consider moving this statement to an `else` block
    |
332 |             )
333 |
334 |             return result.returncode == 0
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TRY300
335 |
336 |         except Exception:
    |

libs/tmux_manager.py:7:8: S404 `subprocess` module is possibly insecure
  |
5 | import logging
6 | import re
7 | import subprocess
  |        ^^^^^^^^^^ S404
8 | from collections import defaultdict
9 | from datetime import UTC, datetime, timedelta
  |

libs/tmux_manager.py:383:9: PLR1702 Too many nested blocks (6 > 5)
    |
381 |           Dict containing the requested data.
382 |           """
383 | /         try:
384 | |             log_path_str = self.config.get("log_path", "~/.scripton/yesman/logs/")
385 | |             safe_session_name = "".join(c for c in session_name if c.isalnum() or c in {"-", "_"}).rstrip()
386 | |             log_file = Path(log_path_str).expanduser() / f"{safe_session_name}.log"
387 | |
388 | |             if not log_file.exists():
389 | |                 # Fallback to the main log if a session-specific log is not found
390 | |                 log_file = Path(log_path_str).expanduser() / "yesman.log"
391 | |                 if not log_file.exists():
392 | |                     return {"session_name": session_name, "activity_data": []}
393 | |
394 | |             # Activity per hour for the last 7 days
395 | |             activity_counts: dict[str, int] = defaultdict(int)
396 | |
397 | |             now = datetime.now(UTC)
398 | |             seven_days_ago = now - timedelta(days=7)
399 | |
400 | |             with log_file.open(encoding="utf-8") as f:
401 | |                 for line in f:
402 | |                     # Extract timestamp from log line
403 | |                     # Example log format: [2025-07-10 14:22:01,161] [INFO] [session:my-session] Log message
404 | |                     match = re.match(r"\[(.*?)\]", line)
405 | |                     if match:
406 | |                         timestamp_str = match.group(1).split(",")[0]  # Get 'YYYY-MM-DD HH:MM:SS'
407 | |                         try:
408 | |                             log_time = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")
409 | |                             if log_time > seven_days_ago:
410 | |                                 # Round down to the hour
411 | |                                 hour_timestamp = log_time.strftime("%Y-%m-%dT%H:00:00")
412 | |                                 activity_counts[hour_timestamp] += 1
413 | |                         except ValueError:
414 | |                             continue
415 | |
416 | |             # Format data for heatmap
417 | |             activity_data = [{"timestamp": ts, "activity": count} for ts, count in activity_counts.items()]
418 | |
419 | |         except (OSError, PermissionError, ValueError) as e:
420 | |             self.logger.exception("Failed to get session activity for %s")
421 | |             return {"session_name": session_name, "activity_data": []}
422 | |         else:
423 | |             return {
424 | |                 "session_name": session_name,
425 | |                 "activity_data": activity_data,
426 | |             }
    | |_____________^ PLR1702
    |

libs/validation.py:194:21: TRY300 Consider moving this statement to an `else` block
    |
192 |                 try:
193 |                     resolved_path.mkdir(parents=True, exist_ok=True)
194 |                     return True, None
    |                     ^^^^^^^^^^^^^^^^^ TRY300
195 |                 except Exception as e:
196 |                     return False, f"Failed to create directory: {e}"
    |

libs/validation.py:320:25: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `func`
    |
318 |     """
319 |
320 |     def decorator(func: Any) -> object:
    |                         ^^^ ANN401
321 |         def wrapper(*args: Any, **kwargs: Any) -> object:
322 |             # Get the value to validate
    |

libs/validation.py:321:28: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `*args`
    |
320 |     def decorator(func: Any) -> object:
321 |         def wrapper(*args: Any, **kwargs: Any) -> object:
    |                            ^^^ ANN401
322 |             # Get the value to validate
323 |             if field_name in kwargs:
    |

libs/validation.py:321:43: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
320 |     def decorator(func: Any) -> object:
321 |         def wrapper(*args: Any, **kwargs: Any) -> object:
    |                                           ^^^ ANN401
322 |             # Get the value to validate
323 |             if field_name in kwargs:
    |

libs/yesman_config.py:160:13: TRY300 Consider moving this statement to an `else` block
    |
158 |         try:
159 |             self._loader.validate(self.config)
160 |             return True
    |             ^^^^^^^^^^^ TRY300
161 |         except ValueError as e:
162 |             self.logger.exception("Configuration validation failed")  # noqa: G004
    |

tests/fixtures/mock_data.py:5:1: E116 Unexpected indentation (comment)
  |
3 | from datetime import UTC, datetime
4 | from typing import Any
5 |     # Import here to avoid circular imports
  | ^^^^ E116
6 |     from .mock_factories import ComponentMockFactory, ManagerMockFactory
  |

tests/fixtures/mock_data.py:6:1: SyntaxError: Unexpected indentation
  |
4 | from typing import Any
5 |     # Import here to avoid circular imports
6 |     from .mock_factories import ComponentMockFactory, ManagerMockFactory
  | ^^^^
7 |
8 | # Copyright (c) 2024 Yesman Claude Project
  |

tests/fixtures/mock_data.py:6:1: E113 Unexpected indentation
  |
4 | from typing import Any
5 |     # Import here to avoid circular imports
6 |     from .mock_factories import ComponentMockFactory, ManagerMockFactory
  | ^^^^ E113
7 |
8 | # Copyright (c) 2024 Yesman Claude Project
  |

tests/fixtures/mock_data.py:11:1: SyntaxError: Expected a statement
   |
 9 | # Licensed under the MIT License
10 |
11 | """ê³µí†µ Mock ë°ì´í„° ì •ì˜
   | ^
12 | í…ŒìŠ¤íŠ¸ì—ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ mock ê°ì²´ë“¤ì„ ì¤‘ì•™í™”.
   |

tests/fixtures/mock_data.py:19:1: E303 Too many blank lines (3)
   |
19 | # Tmux ê´€ë ¨ Mock
   | ^^^^^^^^^^^^^^^^ E303
20 | class MockTmuxSession:
21 |     """Tmux ì„¸ì…˜ Mock ê°ì²´."""
   |
   = help: Remove extraneous blank line(s)

tests/fixtures/mock_factories.py:17:1: E303 Too many blank lines (4)
   |
17 | class ManagerMockFactory:
   | ^^^^^ E303
18 |     """Factory for commonly mocked manager classes."""
   |
   = help: Remove extraneous blank line(s)

tests/fixtures/mock_factories.py:217:79: SyntaxError: Expected ',', found ':'
    |
215 |     def patch_session_manager(**kwargs: object) -> object:
216 |         """Create a patch context for SessionManager with standard mock."""
217 |         mock_manager = ManagerMockFactory.create_session_manager_mock(**kwargs: dict[str, object])
    |                                                                               ^
218 |         return patch("libs.core.session_manager.SessionManager", return_value=mock_manager)
    |

tests/fixtures/mock_factories.py:217:81: SyntaxError: Positional argument cannot follow keyword argument unpacking
    |
215 |     def patch_session_manager(**kwargs: object) -> object:
216 |         """Create a patch context for SessionManager with standard mock."""
217 |         mock_manager = ManagerMockFactory.create_session_manager_mock(**kwargs: dict[str, object])
    |                                                                                 ^^^^^^^^^^^^^^^^^
218 |         return patch("libs.core.session_manager.SessionManager", return_value=mock_manager)
    |

tests/fixtures/mock_factories.py:223:78: SyntaxError: Expected ',', found ':'
    |
221 |     def patch_claude_manager(**kwargs: object) -> object:
222 |         """Create a patch context for ClaudeManager with standard mock."""
223 |         mock_manager = ManagerMockFactory.create_claude_manager_mock(**kwargs: dict[str, object])
    |                                                                              ^
224 |         return patch("libs.core.claude_manager.ClaudeManager", return_value=mock_manager)
    |

tests/fixtures/mock_factories.py:223:80: SyntaxError: Positional argument cannot follow keyword argument unpacking
    |
221 |     def patch_claude_manager(**kwargs: object) -> object:
222 |         """Create a patch context for ClaudeManager with standard mock."""
223 |         mock_manager = ManagerMockFactory.create_claude_manager_mock(**kwargs: dict[str, object])
    |                                                                                ^^^^^^^^^^^^^^^^^
224 |         return patch("libs.core.claude_manager.ClaudeManager", return_value=mock_manager)
    |

tests/fixtures/mock_factories.py:229:76: SyntaxError: Expected ',', found ':'
    |
227 |     def patch_tmux_manager(**kwargs: object) -> object:
228 |         """Create a patch context for TmuxManager with standard mock."""
229 |         mock_manager = ManagerMockFactory.create_tmux_manager_mock(**kwargs: dict[str, object])
    |                                                                            ^
230 |         return patch("libs.tmux_manager.TmuxManager", return_value=mock_manager)
    |

tests/fixtures/mock_factories.py:229:78: SyntaxError: Positional argument cannot follow keyword argument unpacking
    |
227 |     def patch_tmux_manager(**kwargs: object) -> object:
228 |         """Create a patch context for TmuxManager with standard mock."""
229 |         mock_manager = ManagerMockFactory.create_tmux_manager_mock(**kwargs: dict[str, object])
    |                                                                              ^^^^^^^^^^^^^^^^^
230 |         return patch("libs.tmux_manager.TmuxManager", return_value=mock_manager)
    |

tests/fixtures/mock_factories.py:235:76: SyntaxError: Expected ',', found ':'
    |
233 |     def patch_setup_tmux_manager(**kwargs: object) -> object:
234 |         """Create a patch context for TmuxManager in setup commands."""
235 |         mock_manager = ManagerMockFactory.create_tmux_manager_mock(**kwargs: dict[str, object])
    |                                                                            ^
236 |         return patch("commands.setup.TmuxManager", return_value=mock_manager)
    |

tests/fixtures/mock_factories.py:235:78: SyntaxError: Positional argument cannot follow keyword argument unpacking
    |
233 |     def patch_setup_tmux_manager(**kwargs: object) -> object:
234 |         """Create a patch context for TmuxManager in setup commands."""
235 |         mock_manager = ManagerMockFactory.create_tmux_manager_mock(**kwargs: dict[str, object])
    |                                                                              ^^^^^^^^^^^^^^^^^
236 |         return patch("commands.setup.TmuxManager", return_value=mock_manager)
    |

tests/fixtures/mock_factories.py:241:75: SyntaxError: Expected ',', found ':'
    |
239 |     def patch_subprocess_run(**kwargs: object) -> object:
240 |         """Create a patch context for subprocess.run with standard mock."""
241 |         mock_result = ComponentMockFactory.create_subprocess_mock(**kwargs: dict[str, object])
    |                                                                           ^
242 |         return patch("subprocess.run", return_value=mock_result)
    |

tests/fixtures/mock_factories.py:241:77: SyntaxError: Positional argument cannot follow keyword argument unpacking
    |
239 |     def patch_subprocess_run(**kwargs: object) -> object:
240 |         """Create a patch context for subprocess.run with standard mock."""
241 |         mock_result = ComponentMockFactory.create_subprocess_mock(**kwargs: dict[str, object])
    |                                                                             ^^^^^^^^^^^^^^^^^
242 |         return patch("subprocess.run", return_value=mock_result)
    |

tests/fixtures/test_helpers.py:11:1: SyntaxError: Unexpected indentation
   |
 9 | from pathlib import Path
10 | import yaml
11 |     from .mock_data import MockTmuxPane, MockTmuxSession
   | ^^^^
12 | import time
   |

tests/fixtures/test_helpers.py:11:1: E113 Unexpected indentation
   |
 9 | from pathlib import Path
10 | import yaml
11 |     from .mock_data import MockTmuxPane, MockTmuxSession
   | ^^^^ E113
12 | import time
   |

tests/fixtures/test_helpers.py:12:1: SyntaxError: Expected a statement
   |
10 | import yaml
11 |     from .mock_data import MockTmuxPane, MockTmuxSession
12 | import time
   | ^
13 |
14 | # Copyright (c) 2024 Yesman Claude Project
   |

tests/fixtures/test_helpers.py:24:1: E303 Too many blank lines (4)
   |
24 | @contextmanager
   | ^ E303
25 | def temp_directory() -> object:
26 |     """ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„± ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €."""
   |
   = help: Remove extraneous blank line(s)

tests/integration/test_ai_learning_integration.py:7:30: SyntaxError: Expected one or more symbol names after import
   |
 5 | from commands.automate import AutomateDetectCommand
 6 | from libs.ai.learning_engine import LearningEngine
 7 | from .test_framework import (
   |                              ^
 8 |
 9 | # Copyright (c) 2024 Yesman Claude Project
10 | # Licensed under the MIT License
   |

tests/integration/test_ai_learning_integration.py:19:1: SyntaxError: Unexpected indentation
   |
19 |     AsyncIntegrationTestBase,
   | ^^^^
20 |     CommandTestRunner,
21 |     MockClaudeEnvironment,
   |

tests/integration/test_ai_learning_integration.py:19:1: E113 Unexpected indentation
   |
19 |     AsyncIntegrationTestBase,
   | ^^^^ E113
20 |     CommandTestRunner,
21 |     MockClaudeEnvironment,
   |

tests/integration/test_ai_learning_integration.py:19:5: E303 Too many blank lines (2)
   |
19 |     AsyncIntegrationTestBase,
   |     ^^^^^^^^^^^^^^^^^^^^^^^^ E303
20 |     CommandTestRunner,
21 |     MockClaudeEnvironment,
   |
   = help: Remove extraneous blank line(s)

tests/integration/test_ai_learning_integration.py:23:1: SyntaxError: Expected a statement
   |
21 |     MockClaudeEnvironment,
22 |     PerformanceMonitor,
23 | )
   | ^
   |

tests/integration/test_ai_learning_integration.py:23:2: SyntaxError: Expected a statement
   |
21 |     MockClaudeEnvironment,
22 |     PerformanceMonitor,
23 | )
   |  ^
24 |
25 |
26 | class TestAILearningIntegration(AsyncIntegrationTestBase):
   |

tests/integration/test_automation_workflows_integration.py:8:32: SyntaxError: Expected one or more symbol names after import
   |
 6 | import contextlib
 7 | import time
 8 | from commands.automate import (
   |                                ^
 9 | from .test_framework import (
10 |
11 | # Copyright (c) 2024 Yesman Claude Project
   |

tests/integration/test_automation_workflows_integration.py:9:30: SyntaxError: Expected one or more symbol names after import
   |
 7 | import time
 8 | from commands.automate import (
 9 | from .test_framework import (
   |                              ^
10 |
11 | # Copyright (c) 2024 Yesman Claude Project
12 | # Licensed under the MIT License
   |

tests/integration/test_automation_workflows_integration.py:22:1: SyntaxError: Unexpected indentation
   |
22 |     AutomateConfigCommand,
   | ^^^^
23 |     AutomateDetectCommand,
24 |     AutomateMonitorCommand,
   |

tests/integration/test_automation_workflows_integration.py:22:1: E113 Unexpected indentation
   |
22 |     AutomateConfigCommand,
   | ^^^^ E113
23 |     AutomateDetectCommand,
24 |     AutomateMonitorCommand,
   |

tests/integration/test_automation_workflows_integration.py:22:5: E303 Too many blank lines (3)
   |
22 |     AutomateConfigCommand,
   |     ^^^^^^^^^^^^^^^^^^^^^ E303
23 |     AutomateDetectCommand,
24 |     AutomateMonitorCommand,
   |
   = help: Remove extraneous blank line(s)

tests/integration/test_automation_workflows_integration.py:26:1: SyntaxError: Expected a statement
   |
24 |     AutomateMonitorCommand,
25 |     AutomateWorkflowCommand,
26 | )
   | ^
27 |
28 |     AsyncIntegrationTestBase,
   |

tests/integration/test_automation_workflows_integration.py:26:2: SyntaxError: Expected a statement
   |
24 |     AutomateMonitorCommand,
25 |     AutomateWorkflowCommand,
26 | )
   |  ^
27 |
28 |     AsyncIntegrationTestBase,
29 |     CommandTestRunner,
   |

tests/integration/test_automation_workflows_integration.py:28:1: SyntaxError: Unexpected indentation
   |
26 | )
27 |
28 |     AsyncIntegrationTestBase,
   | ^^^^
29 |     CommandTestRunner,
30 |     MockClaudeEnvironment,
   |

tests/integration/test_automation_workflows_integration.py:28:1: E113 Unexpected indentation
   |
26 | )
27 |
28 |     AsyncIntegrationTestBase,
   | ^^^^ E113
29 |     CommandTestRunner,
30 |     MockClaudeEnvironment,
   |

tests/integration/test_automation_workflows_integration.py:32:1: SyntaxError: Expected a statement
   |
30 |     MockClaudeEnvironment,
31 |     PerformanceMonitor,
32 | )
   | ^
   |

tests/integration/test_automation_workflows_integration.py:32:2: SyntaxError: Expected a statement
   |
30 |     MockClaudeEnvironment,
31 |     PerformanceMonitor,
32 | )
   |  ^
33 |
34 |
35 | class TestAutomationWorkflowIntegration(AsyncIntegrationTestBase):
   |

tests/integration/test_command_integration.py:4:8: S404 `subprocess` module is possibly insecure
  |
3 | import os
4 | import subprocess
  |        ^^^^^^^^^^ S404
5 | import tempfile
6 | import time
  |

tests/integration/test_command_integration.py:199:27: ARG004 Unused static method argument: `kwargs`
    |
197 |         class TestCommand(BaseCommand):
198 |             @staticmethod
199 |             def execute(**kwargs: dict[str, object]) -> Never:  # noqa: ARG002  # noqa: ARG004
    |                           ^^^^^^ ARG004
200 |                 msg = "Test validation error"
201 |                 raise ValidationError(
    |

tests/integration/test_cross_module_integration.py:10:30: SyntaxError: Expected one or more symbol names after import
   |
 8 | from commands.status import StatusCommand
 9 | from libs.ai.learning_engine import LearningEngine
10 | from .test_framework import (
   |                              ^
11 | from flask import Flask, jsonify
12 | import sqlite3
13 | import pytest
   |

tests/integration/test_cross_module_integration.py:28:1: SyntaxError: Unexpected indentation
   |
28 |     AsyncIntegrationTestBase,
   | ^^^^
29 |     CommandTestRunner,
30 |     MockClaudeEnvironment,
   |

tests/integration/test_cross_module_integration.py:28:1: E113 Unexpected indentation
   |
28 |     AsyncIntegrationTestBase,
   | ^^^^ E113
29 |     CommandTestRunner,
30 |     MockClaudeEnvironment,
   |

tests/integration/test_cross_module_integration.py:28:5: E303 Too many blank lines (3)
   |
28 |     AsyncIntegrationTestBase,
   |     ^^^^^^^^^^^^^^^^^^^^^^^^ E303
29 |     CommandTestRunner,
30 |     MockClaudeEnvironment,
   |
   = help: Remove extraneous blank line(s)

tests/integration/test_cross_module_integration.py:32:1: SyntaxError: Expected a statement
   |
30 |     MockClaudeEnvironment,
31 |     PerformanceMonitor,
32 | )
   | ^
   |

tests/integration/test_cross_module_integration.py:32:2: SyntaxError: Expected a statement
   |
30 |     MockClaudeEnvironment,
31 |     PerformanceMonitor,
32 | )
   |  ^
33 |
34 |
35 | class TestFullSystemIntegration(AsyncIntegrationTestBase):
   |

tests/integration/test_dashboard_integration.py:13:30: SyntaxError: Expected one or more symbol names after import
   |
11 | from commands.setup import SetupCommand
12 | from commands.status import StatusCommand
13 | from .test_framework import (
   |                              ^
14 |
15 | # Copyright (c) 2024 Yesman Claude Project
16 | # Licensed under the MIT License
   |

tests/integration/test_dashboard_integration.py:27:1: SyntaxError: Unexpected indentation
   |
27 |     AsyncIntegrationTestBase,
   | ^^^^
28 |     CommandTestRunner,
29 |     MockClaudeEnvironment,
   |

tests/integration/test_dashboard_integration.py:27:1: E113 Unexpected indentation
   |
27 |     AsyncIntegrationTestBase,
   | ^^^^ E113
28 |     CommandTestRunner,
29 |     MockClaudeEnvironment,
   |

tests/integration/test_dashboard_integration.py:27:5: E303 Too many blank lines (4)
   |
27 |     AsyncIntegrationTestBase,
   |     ^^^^^^^^^^^^^^^^^^^^^^^^ E303
28 |     CommandTestRunner,
29 |     MockClaudeEnvironment,
   |
   = help: Remove extraneous blank line(s)

tests/integration/test_dashboard_integration.py:31:1: SyntaxError: Expected a statement
   |
29 |     MockClaudeEnvironment,
30 |     PerformanceMonitor,
31 | )
   | ^
   |

tests/integration/test_dashboard_integration.py:31:2: SyntaxError: Expected a statement
   |
29 |     MockClaudeEnvironment,
30 |     PerformanceMonitor,
31 | )
   |  ^
32 |
33 |
34 | class TestDashboardSystemIntegration(AsyncIntegrationTestBase):
   |

tests/integration/test_framework.py:128:64: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
126 |         return self.session_manager
127 |
128 |     def create_test_session(self, session_name: str, **kwargs: Any) -> dict[str, object]:
    |                                                                ^^^ ANN401
129 |         """Create a test session with specified configuration."""
130 |         config = {
    |

tests/integration/test_framework.py:293:58: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
    |
291 |         self.command_results = []
292 |
293 |     def run_command(self, command_class: type, **kwargs: Any) -> dict[str, object]:
    |                                                          ^^^ ANN401
294 |         """Run a command and capture results."""
295 |         command = command_class()
    |

tests/integration/test_framework.py:311:13: TRY300 Consider moving this statement to an `else` block
    |
309 |                 }
310 |             )
311 |             return result
    |             ^^^^^^^^^^^^^ TRY300
312 |         except Exception as e:
313 |             error_result = {
    |

tests/test_agent_monitor.py:7:51: SyntaxError: Expected one or more symbol names after import
   |
 5 | from unittest.mock import AsyncMock, Mock, patch
 6 | import pytest
 7 | from libs.dashboard.widgets.agent_monitor import (
   |                                                   ^
 8 | from libs.multi_agent.types import TaskStatus
 9 | from libs.dashboard.widgets.agent_monitor import create_agent_monitor
10 | from libs.dashboard.widgets.agent_monitor import create_agent_monitor
   |

tests/test_agent_monitor.py:20:1: SyntaxError: Unexpected indentation
   |
20 |     AgentMetrics,
   | ^^^^
21 |     AgentMonitor,
22 |     MonitorDisplayMode,
   |

tests/test_agent_monitor.py:20:1: E113 Unexpected indentation
   |
20 |     AgentMetrics,
   | ^^^^ E113
21 |     AgentMonitor,
22 |     MonitorDisplayMode,
   |

tests/test_agent_monitor.py:20:5: E303 Too many blank lines (3)
   |
20 |     AgentMetrics,
   |     ^^^^^^^^^^^^ E303
21 |     AgentMonitor,
22 |     MonitorDisplayMode,
   |
   = help: Remove extraneous blank line(s)

tests/test_agent_monitor.py:24:1: SyntaxError: Expected a statement
   |
22 |     MonitorDisplayMode,
23 |     TaskMetrics,
24 | )
   | ^
   |

tests/test_agent_monitor.py:24:2: SyntaxError: Expected a statement
   |
22 |     MonitorDisplayMode,
23 |     TaskMetrics,
24 | )
   |  ^
25 |
26 |
27 | class TestAgentMetrics:
   |

tests/test_agent_pool.py:18:1: PLR0904 Too many public methods (22 > 20)
    |
 18 | / class TestAgentPool:
 19 | |     """Test cases for AgentPool."""
 20 | |
 21 | |     @pytest.fixture
 22 | |     @staticmethod
 23 | |     def work_dir(tmp_path: Path) -> Path:
 24 | |         """Create temporary work directory.
 25 | |
 26 | |         Returns:
 27 | |         Path: Description of return value.
 28 | |         """
 29 | |         work_path = tmp_path / "agent_work"
 30 | |         work_path.mkdir()
 31 | |         return work_path
 32 | |
 33 | |     @pytest.fixture
 34 | |     @staticmethod
 35 | |     def agent_pool(work_dir: Path) -> AgentPool:
 36 | |         """Create AgentPool instance.
 37 | |
 38 | |         Returns:
 39 | |         AgentPool: Description of return value.
 40 | |         """
 41 | |         return AgentPool(max_agents=2, work_dir=str(work_dir))
 42 | |
 43 | |     @pytest.fixture
 44 | |     @staticmethod
 45 | |     def sample_task() -> Task:
 46 | |         """Create a sample task.
 47 | |
 48 | |         Returns:
 49 | |         Task: Description of return value.
 50 | |         """
 51 | |         return Task(
 52 | |             task_id="test-task-1",
 53 | |             title="Test Task",
 54 | |             description="A test task",
 55 | |             command=["echo", "Hello World"],
 56 | |             working_directory="/tmp",
 57 | |             timeout=30,
 58 | |         )
 59 | |
 60 | |     @staticmethod
 61 | |     def test_init(agent_pool: AgentPool, work_dir: Path) -> None:
 62 | |         """Test AgentPool initialization."""
 63 | |         assert agent_pool.max_agents == 2
 64 | |         assert agent_pool.work_dir == work_dir
 65 | |         assert agent_pool.agents == {}
 66 | |         assert agent_pool.tasks == {}
 67 | |         assert not agent_pool._running  # noqa: SLF001
 68 | |         assert work_dir.exists()
 69 | |
 70 | |     @staticmethod
 71 | |     def test_add_task(agent_pool: AgentPool, sample_task: Task) -> None:
 72 | |         """Test adding a task to the pool."""
 73 | |         agent_pool.add_task(sample_task)
 74 | |
 75 | |         assert sample_task.task_id in agent_pool.tasks
 76 | |         assert agent_pool.tasks[sample_task.task_id] == sample_task
 77 | |         assert agent_pool.task_queue.qsize() == 1
 78 | |
 79 | |     @staticmethod
 80 | |     def test_create_task(agent_pool: AgentPool) -> None:
 81 | |         """Test creating and adding a task."""
 82 | |         task = agent_pool.create_task(
 83 | |             title="New Task",
 84 | |             command=["ls", "-la"],
 85 | |             working_directory="/tmp",
 86 | |             description="List files",
 87 | |         )
 88 | |
 89 | |         assert task.title == "New Task"
 90 | |         assert task.command == ["ls", "-la"]
 91 | |         assert task.working_directory == "/tmp"
 92 | |         assert task.description == "List files"
 93 | |         assert task.task_id in agent_pool.tasks
 94 | |
 95 | |     @pytest.mark.asyncio
 96 | |     @staticmethod
 97 | |     async def test_create_agent(agent_pool: AgentPool) -> None:
 98 | |         """Test creating a new agent."""
 99 | |         agent = await agent_pool._create_agent()  # noqa: SLF001
100 | |
101 | |         assert agent.agent_id.startswith("agent-")
102 | |         assert agent.state == AgentState.IDLE
103 | |         assert agent.current_task is None
104 | |         assert agent.agent_id in agent_pool.agents
105 | |
106 | |     @pytest.mark.asyncio
107 | |     @staticmethod
108 | |     async def test_get_available_agent(agent_pool: AgentPool) -> None:
109 | |         """Test getting available agent."""
110 | |         # No agents initially
111 | |         agent = await agent_pool._get_available_agent()  # noqa: SLF001
112 | |         assert agent is not None  # Should create new agent
113 | |         assert len(agent_pool.agents) == 1
114 | |
115 | |         # Agent should be available
116 | |         same_agent = await agent_pool._get_available_agent()  # noqa: SLF001
117 | |         assert same_agent == agent
118 | |
119 | |         # Make agent busy
120 | |         agent.state = AgentState.WORKING
121 | |
122 | |         # Should create new agent
123 | |         new_agent = await agent_pool._get_available_agent()  # noqa: SLF001
124 | |         assert new_agent != agent
125 | |         assert len(agent_pool.agents) == 2
126 | |
127 | |         # Both agents busy, at max capacity
128 | |         new_agent.state = AgentState.WORKING
129 | |         no_agent = await agent_pool._get_available_agent()  # noqa: SLF001
130 | |         assert no_agent is None
131 | |
132 | |     @pytest.mark.asyncio
133 | |     @staticmethod
134 | |     async def test_assign_task_to_agent(agent_pool: AgentPool, sample_task: Task) -> None:
135 | |         """Test assigning a task to an agent."""
136 | |         agent = await agent_pool._create_agent()  # noqa: SLF001
137 | |
138 | |         await agent_pool._assign_task_to_agent(agent, sample_task)  # noqa: SLF001
139 | |
140 | |         # Check task state
141 | |         assert sample_task.status == TaskStatus.ASSIGNED
142 | |         assert sample_task.assigned_agent == agent.agent_id
143 | |
144 | |         # Check agent state
145 | |         assert agent.state == AgentState.WORKING
146 | |         assert agent.current_task == sample_task.task_id
147 | |
148 | |     @pytest.mark.asyncio
149 | |     @staticmethod
150 | |     async def test_execute_task_success(agent_pool: AgentPool) -> None:
151 | |         """Test successful task execution."""
152 | |         agent = await agent_pool._create_agent()  # noqa: SLF001
153 | |         task = Task(
154 | |             task_id="success-task",
155 | |             title="Success Task",
156 | |             description="Should succeed",
157 | |             command=["echo", "success"],
158 | |             working_directory="/tmp",
159 | |         )
160 | |
161 | |         # Mock callbacks
162 | |         started_callback = AsyncMock()
163 | |         completed_callback = AsyncMock()
164 | |         agent_pool.on_task_started(started_callback)
165 | |         agent_pool.on_task_completed(completed_callback)
166 | |
167 | |         await agent_pool._execute_task(agent, task)  # noqa: SLF001
168 | |
169 | |         # Check task completion
170 | |         assert task.status == TaskStatus.COMPLETED
171 | |         assert task.exit_code == 0
172 | |         assert "success" in task.output
173 | |         assert task.start_time is not None
174 | |         assert task.end_time is not None
175 | |
176 | |         # Check agent state
177 | |         assert agent.state == AgentState.IDLE
178 | |         assert agent.current_task is None
179 | |         assert agent.completed_tasks == 1
180 | |         assert agent.failed_tasks == 0
181 | |
182 | |         # Check callbacks
183 | |         started_callback.assert_called_once_with(task)
184 | |         completed_callback.assert_called_once_with(task)
185 | |
186 | |     @pytest.mark.asyncio
187 | |     @staticmethod
188 | |     async def test_execute_task_failure(agent_pool: AgentPool) -> None:
189 | |         """Test failed task execution."""
190 | |         agent = await agent_pool._create_agent()  # noqa: SLF001
191 | |         task = Task(
192 | |             task_id="fail-task",
193 | |             title="Fail Task",
194 | |             description="Should fail",
195 | |             command=["false"],  # Command that always fails
196 | |             working_directory="/tmp",
197 | |         )
198 | |
199 | |         # Mock callbacks
200 | |         failed_callback = AsyncMock()
201 | |         agent_pool.on_task_failed(failed_callback)
202 | |
203 | |         await agent_pool._execute_task(agent, task)  # noqa: SLF001
204 | |
205 | |         # Check task failure
206 | |         assert task.status == TaskStatus.FAILED
207 | |         assert task.exit_code != 0
208 | |
209 | |         # Check agent state
210 | |         assert agent.state == AgentState.IDLE
211 | |         assert agent.current_task is None
212 | |         assert agent.completed_tasks == 0
213 | |         assert agent.failed_tasks == 1
214 | |
215 | |         # Check callback
216 | |         failed_callback.assert_called_once_with(task)
217 | |
218 | |     @pytest.mark.asyncio
219 | |     @staticmethod
220 | |     async def test_execute_task_timeout(agent_pool: AgentPool) -> None:
221 | |         """Test task timeout handling."""
222 | |         agent = await agent_pool._create_agent()  # noqa: SLF001  # noqa: SLF001
223 | |         task = Task(
224 | |             task_id="timeout-task",
225 | |             title="Timeout Task",
226 | |             description="Should timeout",
227 | |             command=["sleep", "10"],  # Long running command
228 | |             working_directory="/tmp",
229 | |             timeout=1,  # 1 second timeout
230 | |         )
231 | |
232 | |         await agent_pool._execute_task(agent, task)  # noqa: SLF001
233 | |
234 | |         # Check task timeout
235 | |         assert task.status == TaskStatus.FAILED
236 | |         assert "timed out" in task.error
237 | |
238 | |         # Check agent state
239 | |         assert agent.state == AgentState.IDLE
240 | |         assert agent.failed_tasks == 1
241 | |
242 | |     @pytest.mark.asyncio
243 | |     @staticmethod
244 | |     async def test_terminate_agent(agent_pool: AgentPool) -> None:
245 | |         """Test agent termination."""
246 | |         agent = await agent_pool._create_agent()  # noqa: SLF001  # noqa: SLF001
247 | |
248 | |         # Mock process
249 | |         mock_process = MagicMock()
250 | |         mock_process.terminate = AsyncMock()
251 | |         mock_process.wait = AsyncMock()
252 | |         agent.process = mock_process
253 | |
254 | |         await agent_pool._terminate_agent(agent.agent_id)  # noqa: SLF001
255 | |
256 | |         assert agent.state == AgentState.TERMINATED
257 | |         assert agent.current_task is None
258 | |         assert agent.process is None
259 | |         mock_process.terminate.assert_called_once()
260 | |
261 | |     @staticmethod
262 | |     def test_get_agent_status(agent_pool: AgentPool) -> None:
263 | |         """Test getting agent status."""
264 | |         # Non-existent agent
265 | |         status = agent_pool.get_agent_status("non-existent")
266 | |         assert status is None
267 | |
268 | |         # Create agent
269 | |         agent = Agent(agent_id="test-agent")
270 | |         agent_pool.agents["test-agent"] = agent
271 | |
272 | |         status = agent_pool.get_agent_status("test-agent")
273 | |         assert status is not None
274 | |         assert status["agent_id"] == "test-agent"
275 | |         assert status["state"] == AgentState.IDLE.value
276 | |
277 | |     @staticmethod
278 | |     def test_get_task_status(agent_pool: AgentPool, sample_task: Task) -> None:
279 | |         """Test getting task status."""
280 | |         # Non-existent task
281 | |         status = agent_pool.get_task_status("non-existent")
282 | |         assert status is None
283 | |
284 | |         # Add task
285 | |         agent_pool.tasks[sample_task.task_id] = sample_task
286 | |
287 | |         status = agent_pool.get_task_status(sample_task.task_id)
288 | |         assert status is not None
289 | |         assert status["task_id"] == sample_task.task_id
290 | |         assert status["status"] == TaskStatus.PENDING.value
291 | |
292 | |     @staticmethod
293 | |     def test_list_agents(agent_pool: AgentPool) -> None:
294 | |         """Test listing agents."""
295 | |         # Empty list initially
296 | |         agents = agent_pool.list_agents()
297 | |         assert agents == []
298 | |
299 | |         # Add agents
300 | |         agent1 = Agent(agent_id="agent-1")
301 | |         agent2 = Agent(agent_id="agent-2")
302 | |         agent_pool.agents["agent-1"] = agent1
303 | |         agent_pool.agents["agent-2"] = agent2
304 | |
305 | |         agents = agent_pool.list_agents()
306 | |         assert len(agents) == 2
307 | |         assert any(a["agent_id"] == "agent-1" for a in agents)
308 | |         assert any(a["agent_id"] == "agent-2" for a in agents)
309 | |
310 | |     @staticmethod
311 | |     def test_list_tasks(agent_pool: AgentPool) -> None:
312 | |         """Test listing tasks."""
313 | |         # Empty list initially
314 | |         tasks = agent_pool.list_tasks()
315 | |         assert tasks == []
316 | |
317 | |         # Add tasks
318 | |         task1 = Task(
319 | |             task_id="task-1",
320 | |             title="Task 1",
321 | |             command=["echo"],
322 | |             working_directory="/tmp",
323 | |         )
324 | |         task2 = Task(
325 | |             task_id="task-2",
326 | |             title="Task 2",
327 | |             command=["echo"],
328 | |             working_directory="/tmp",
329 | |             status=TaskStatus.COMPLETED,
330 | |         )
331 | |         agent_pool.tasks["task-1"] = task1
332 | |         agent_pool.tasks["task-2"] = task2
333 | |
334 | |         # List all tasks
335 | |         all_tasks = agent_pool.list_tasks()
336 | |         assert len(all_tasks) == 2
337 | |
338 | |         # Filter by status
339 | |         pending_tasks = agent_pool.list_tasks(TaskStatus.PENDING)
340 | |         assert len(pending_tasks) == 1
341 | |         assert pending_tasks[0]["task_id"] == "task-1"
342 | |
343 | |         completed_tasks = agent_pool.list_tasks(TaskStatus.COMPLETED)
344 | |         assert len(completed_tasks) == 1
345 | |         assert completed_tasks[0]["task_id"] == "task-2"
346 | |
347 | |     @staticmethod
348 | |     def test_get_pool_statistics(agent_pool: AgentPool) -> None:
349 | |         """Test getting pool statistics."""
350 | |         stats = agent_pool.get_pool_statistics()
351 | |
352 | |         assert stats["max_agents"] == 2
353 | |         assert stats["active_agents"] == 0
354 | |         assert stats["working_agents"] == 0
355 | |         assert stats["total_tasks"] == 0
356 | |         assert stats["running"] is False
357 | |
358 | |         # Add some data
359 | |         agent = Agent(agent_id="test-agent", completed_tasks=5, failed_tasks=2)
360 | |         agent_pool.agents["test-agent"] = agent
361 | |
362 | |         task = Task(
363 | |             task_id="test-task",
364 | |             title="Test",
365 | |             command=["echo"],
366 | |             working_directory="/tmp",
367 | |         )
368 | |         agent_pool.tasks["test-task"] = task
369 | |
370 | |         stats = agent_pool.get_pool_statistics()
371 | |         assert stats["active_agents"] == 1
372 | |         assert stats["total_tasks"] == 1
373 | |         assert stats["completed_tasks"] == 5
374 | |         assert stats["failed_tasks"] == 2
375 | |
376 | |     @staticmethod
377 | |     def test_state_persistence(agent_pool: AgentPool, work_dir: Path) -> None:
378 | |         """Test saving and loading state."""
379 | |         # Add some data
380 | |         agent = Agent(agent_id="test-agent", completed_tasks=3)
381 | |         agent_pool.agents["test-agent"] = agent
382 | |
383 | |         task = Task(
384 | |             task_id="test-task",
385 | |             title="Test",
386 | |             command=["echo"],
387 | |             working_directory="/tmp",
388 | |         )
389 | |         agent_pool.tasks["test-task"] = task
390 | |         agent_pool.completed_tasks = ["completed-1", "completed-2"]
391 | |
392 | |         # Save state
393 | |         agent_pool._save_state()  # noqa: SLF001
394 | |
395 | |         # Create new pool and load
396 | |         new_pool = AgentPool(max_agents=2, work_dir=str(work_dir))
397 | |
398 | |         assert len(new_pool.agents) == 1
399 | |         assert "test-agent" in new_pool.agents
400 | |         assert new_pool.agents["test-agent"].completed_tasks == 3
401 | |
402 | |         assert len(new_pool.tasks) == 1
403 | |         assert "test-task" in new_pool.tasks
404 | |
405 | |         assert len(new_pool.completed_tasks) == 2
406 | |         assert "completed-1" in new_pool.completed_tasks
407 | |
408 | |     @staticmethod
409 | |     def test_task_serialization() -> None:
410 | |         """Test task to/from dict conversion."""
411 | |         task = Task(
412 | |             task_id="test",
413 | |             title="Test Task",
414 | |             command=["echo", "test"],
415 | |             working_directory="/tmp",
416 | |             status=TaskStatus.RUNNING,
417 | |             start_time=datetime.now(UTC),
418 | |             metadata={"key": "value"},
419 | |         )
420 | |
421 | |         # To dict
422 | |         data = task.to_dict()
423 | |         assert data["task_id"] == "test"
424 | |         assert data["status"] == "running"
425 | |         assert "start_time" in data
426 | |
427 | |         # From dict
428 | |         task2 = Task.from_dict(data)
429 | |         assert task2.task_id == task.task_id
430 | |         assert task2.status == task.status
431 | |         assert task2.metadata == task.metadata
432 | |
433 | |     @staticmethod
434 | |     def test_agent_serialization() -> None:
435 | |         """Test agent to/from dict conversion."""
436 | |         agent = Agent(
437 | |             agent_id="test-agent",
438 | |             state=AgentState.WORKING,
439 | |             completed_tasks=5,
440 | |             metadata={"key": "value"},
441 | |         )
442 | |
443 | |         # To dict
444 | |         data = agent.to_dict()
445 | |         assert data["agent_id"] == "test-agent"
446 | |         assert data["state"] == "working"
447 | |         assert data["completed_tasks"] == 5
448 | |         assert "process" not in data  # Should be excluded
449 | |
450 | |         # From dict
451 | |         agent2 = Agent.from_dict(data)
452 | |         assert agent2.agent_id == agent.agent_id
453 | |         assert agent2.state == agent.state
454 | |         assert agent2.completed_tasks == agent.completed_tasks
455 | |
456 | |     @staticmethod
457 | |     def test_event_callbacks(agent_pool: AgentPool) -> None:
458 | |         """Test event callback registration."""
459 | |         task_started_cb = AsyncMock()
460 | |         task_completed_cb = AsyncMock()
461 | |         task_failed_cb = AsyncMock()
462 | |         agent_error_cb = AsyncMock()
463 | |
464 | |         agent_pool.on_task_started(task_started_cb)
465 | |         agent_pool.on_task_completed(task_completed_cb)
466 | |         agent_pool.on_task_failed(task_failed_cb)
467 | |         agent_pool.on_agent_error(agent_error_cb)
468 | |
469 | |         assert task_started_cb in agent_pool.task_started_callbacks
470 | |         assert task_completed_cb in agent_pool.task_completed_callbacks
471 | |         assert task_failed_cb in agent_pool.task_failed_callbacks
472 | |         assert agent_error_cb in agent_pool.agent_error_callbacks
    | |_________________________________________________________________^ PLR0904
    |

tests/test_branch_info_protocol.py:6:52: SyntaxError: Expected one or more symbol names after import
  |
4 | from unittest.mock import AsyncMock, Mock
5 | import pytest
6 | from libs.multi_agent.branch_info_protocol import (
  |                                                    ^
7 | from libs.multi_agent.branch_manager import BranchManager
8 | from libs.multi_agent.collaboration_engine import CollaborationEngine, MessagePriority
  |

tests/test_branch_info_protocol.py:17:1: SyntaxError: Unexpected indentation
   |
17 |     BranchInfo,
   | ^^^^
18 |     BranchInfoProtocol,
19 |     BranchInfoType,
   |

tests/test_branch_info_protocol.py:17:1: E113 Unexpected indentation
   |
17 |     BranchInfo,
   | ^^^^ E113
18 |     BranchInfoProtocol,
19 |     BranchInfoType,
   |

tests/test_branch_info_protocol.py:17:5: E303 Too many blank lines (3)
   |
17 |     BranchInfo,
   |     ^^^^^^^^^^ E303
18 |     BranchInfoProtocol,
19 |     BranchInfoType,
   |
   = help: Remove extraneous blank line(s)

tests/test_branch_info_protocol.py:22:1: SyntaxError: Expected a statement
   |
20 |     BranchSyncEvent,
21 |     SyncStrategy,
22 | )
   | ^
   |

tests/test_branch_info_protocol.py:22:2: SyntaxError: Expected a statement
   |
20 |     BranchSyncEvent,
21 |     SyncStrategy,
22 | )
   |  ^
23 |
24 |
25 | class TestBranchInfo:
   |

tests/test_branch_manager.py:3:8: S404 `subprocess` module is possibly insecure
  |
1 | # Copyright notice.
2 |
3 | import subprocess
  |        ^^^^^^^^^^ S404
4 | from datetime import UTC, datetime
5 | from pathlib import Path
  |

tests/test_branch_manager.py:288:66: ARG004 Unused static method argument: `tmp_path`
    |
287 |     @staticmethod
288 |     def test_metadata_persistence(branch_manager: BranchManager, tmp_path: Path) -> None:  # noqa: ARG002  # noqa: ARG004
    |                                                                  ^^^^^^^^ ARG004
289 |         """Test saving and loading branch metadata."""
290 |         # Create test branch info
    |

tests/test_branch_testing.py:12:51: SyntaxError: Expected one or more symbol names after import
   |
10 | import pytest
11 | from libs.multi_agent.agent_pool import AgentPool
12 | from libs.multi_agent.branch_test_manager import (
   |                                                   ^
13 |
14 | # Copyright (c) 2024 Yesman Claude Project
15 | # Licensed under the MIT License
   |

tests/test_branch_testing.py:21:1: SyntaxError: Unexpected indentation
   |
21 |     BranchTestManager,
   | ^^^^
22 |     TestResult,
23 |     TestStatus,
   |

tests/test_branch_testing.py:21:1: E113 Unexpected indentation
   |
21 |     BranchTestManager,
   | ^^^^ E113
22 |     TestResult,
23 |     TestStatus,
   |

tests/test_branch_testing.py:21:5: E303 Too many blank lines (3)
   |
21 |     BranchTestManager,
   |     ^^^^^^^^^^^^^^^^^ E303
22 |     TestResult,
23 |     TestStatus,
   |
   = help: Remove extraneous blank line(s)

tests/test_branch_testing.py:26:1: SyntaxError: Expected a statement
   |
24 |     TestSuite,
25 |     TestType,
26 | )
   | ^
   |

tests/test_branch_testing.py:26:2: SyntaxError: Expected a statement
   |
24 |     TestSuite,
25 |     TestType,
26 | )
   |  ^
27 |
28 |
29 | class TestBranchTestManager:
   |

tests/test_code_review_engine.py:9:50: SyntaxError: Expected one or more symbol names after import
   |
 7 | import pytest
 8 | from libs.multi_agent.branch_manager import BranchManager
 9 | from libs.multi_agent.code_review_engine import (
   |                                                  ^
10 | from libs.multi_agent.collaboration_engine import CollaborationEngine
11 | from libs.multi_agent.semantic_analyzer import SemanticAnalyzer
12 | import os
   |

tests/test_code_review_engine.py:24:1: SyntaxError: Unexpected indentation
   |
24 |     CodeReview,
   | ^^^^
25 |     CodeReviewEngine,
26 |     QualityMetric,
   |

tests/test_code_review_engine.py:24:1: E113 Unexpected indentation
   |
24 |     CodeReview,
   | ^^^^ E113
25 |     CodeReviewEngine,
26 |     QualityMetric,
   |

tests/test_code_review_engine.py:24:5: E303 Too many blank lines (3)
   |
24 |     CodeReview,
   |     ^^^^^^^^^^ E303
25 |     CodeReviewEngine,
26 |     QualityMetric,
   |
   = help: Remove extraneous blank line(s)

tests/test_code_review_engine.py:32:1: SyntaxError: Expected a statement
   |
30 |     ReviewStatus,
31 |     ReviewType,
32 | )
   | ^
   |

tests/test_code_review_engine.py:32:2: SyntaxError: Expected a statement
   |
30 |     ReviewStatus,
31 |     ReviewType,
32 | )
   |  ^
33 |
34 |
35 | @pytest.fixture
   |

tests/test_collaboration_engine.py:9:52: SyntaxError: Expected one or more symbol names after import
   |
 7 | from libs.multi_agent.agent_pool import AgentPool
 8 | from libs.multi_agent.branch_manager import BranchManager
 9 | from libs.multi_agent.collaboration_engine import (
   |                                                    ^
10 | from libs.multi_agent.conflict_resolution import ConflictResolutionEngine
11 | from libs.multi_agent.semantic_analyzer import SemanticAnalyzer
12 | from libs.multi_agent.types import AgentState
   |

tests/test_collaboration_engine.py:21:1: SyntaxError: Unexpected indentation
   |
21 |     CollaborationEngine,
   | ^^^^
22 |     CollaborationMessage,
23 |     CollaborationMode,
   |

tests/test_collaboration_engine.py:21:1: E113 Unexpected indentation
   |
21 |     CollaborationEngine,
   | ^^^^ E113
22 |     CollaborationMessage,
23 |     CollaborationMode,
   |

tests/test_collaboration_engine.py:21:5: E303 Too many blank lines (3)
   |
21 |     CollaborationEngine,
   |     ^^^^^^^^^^^^^^^^^^^ E303
22 |     CollaborationMessage,
23 |     CollaborationMode,
   |
   = help: Remove extraneous blank line(s)

tests/test_collaboration_engine.py:28:1: SyntaxError: Expected a statement
   |
26 |     MessageType,
27 |     SharedKnowledge,
28 | )
   | ^
   |

tests/test_collaboration_engine.py:28:2: SyntaxError: Expected a statement
   |
26 |     MessageType,
27 |     SharedKnowledge,
28 | )
   |  ^
29 |
30 |
31 | class TestCollaborationMessage:
   |

tests/test_config_management.py:8:38: SyntaxError: Expected one or more symbol names after import
   |
 6 | import pytest
 7 | import yaml
 8 | from libs.core.config_loader import (
   |                                      ^
 9 | from libs.core.config_schema import YesmanConfigSchema
10 | from libs.yesman_config import YesmanConfig
   |

tests/test_config_management.py:19:1: SyntaxError: Unexpected indentation
   |
19 |     ConfigLoader,
   | ^^^^
20 |     DictSource,
21 |     EnvironmentSource,
   |

tests/test_config_management.py:19:1: E113 Unexpected indentation
   |
19 |     ConfigLoader,
   | ^^^^ E113
20 |     DictSource,
21 |     EnvironmentSource,
   |

tests/test_config_management.py:19:5: E303 Too many blank lines (3)
   |
19 |     ConfigLoader,
   |     ^^^^^^^^^^^^ E303
20 |     DictSource,
21 |     EnvironmentSource,
   |
   = help: Remove extraneous blank line(s)

tests/test_config_management.py:24:1: SyntaxError: Expected a statement
   |
22 |     YamlFileSource,
23 |     create_default_loader,
24 | )
   | ^
   |

tests/test_config_management.py:24:2: SyntaxError: Expected a statement
   |
22 |     YamlFileSource,
23 |     create_default_loader,
24 | )
   |  ^
25 |
26 |
27 | class TestConfigSchema:
   |

tests/test_conflict_prediction.py:8:51: SyntaxError: Expected one or more symbol names after import
   |
 6 | import pytest
 7 | from libs.multi_agent.branch_manager import BranchManager
 8 | from libs.multi_agent.conflict_prediction import (
   |                                                   ^
 9 | from libs.multi_agent.conflict_resolution import (
10 | import os
11 | import sys
   |

tests/test_conflict_prediction.py:9:51: SyntaxError: Expected one or more symbol names after import
   |
 7 | from libs.multi_agent.branch_manager import BranchManager
 8 | from libs.multi_agent.conflict_prediction import (
 9 | from libs.multi_agent.conflict_resolution import (
   |                                                   ^
10 | import os
11 | import sys
12 | from datetime import UTC, datetime
   |

tests/test_conflict_prediction.py:16:1: E116 Unexpected indentation (comment)
   |
14 | import os
15 | from sys import path
16 |         # Should detect potential conflict due to overlapping but different imports
   | ^^^^^^^^ E116
   |

tests/test_conflict_prediction.py:27:1: SyntaxError: Unexpected indentation
   |
27 |     ConflictPattern,
   | ^^^^
28 |     ConflictPredictor,
29 |     ConflictVector,
   |

tests/test_conflict_prediction.py:27:1: E113 Unexpected indentation
   |
27 |     ConflictPattern,
   | ^^^^ E113
28 |     ConflictPredictor,
29 |     ConflictVector,
   |

tests/test_conflict_prediction.py:27:5: E303 Too many blank lines (3)
   |
27 |     ConflictPattern,
   |     ^^^^^^^^^^^^^^^ E303
28 |     ConflictPredictor,
29 |     ConflictVector,
   |
   = help: Remove extraneous blank line(s)

tests/test_conflict_prediction.py:32:1: SyntaxError: Expected a statement
   |
30 |     PredictionConfidence,
31 |     PredictionResult,
32 | )
   | ^
33 |     ConflictResolutionEngine,
34 |     ConflictSeverity,
   |

tests/test_conflict_prediction.py:32:2: SyntaxError: Expected a statement
   |
30 |     PredictionConfidence,
31 |     PredictionResult,
32 | )
   |  ^
33 |     ConflictResolutionEngine,
34 |     ConflictSeverity,
35 |     ConflictType,
   |

tests/test_conflict_prediction.py:33:1: SyntaxError: Unexpected indentation
   |
31 |     PredictionResult,
32 | )
33 |     ConflictResolutionEngine,
   | ^^^^
34 |     ConflictSeverity,
35 |     ConflictType,
   |

tests/test_conflict_prediction.py:33:1: E113 Unexpected indentation
   |
31 |     PredictionResult,
32 | )
33 |     ConflictResolutionEngine,
   | ^^^^ E113
34 |     ConflictSeverity,
35 |     ConflictType,
   |

tests/test_conflict_prediction.py:36:1: SyntaxError: Expected a statement
   |
34 |     ConflictSeverity,
35 |     ConflictType,
36 | )
   | ^
   |

tests/test_conflict_prediction.py:36:2: SyntaxError: Expected a statement
   |
34 |     ConflictSeverity,
35 |     ConflictType,
36 | )
   |  ^
37 |
38 |
39 | class TestConflictVector:
   |

tests/test_conflict_prevention.py:10:52: SyntaxError: Expected one or more symbol names after import
   |
 8 | from libs.multi_agent.auto_resolver import AutoResolutionMode, AutoResolver
 9 | from libs.multi_agent.branch_manager import BranchManager
10 | from libs.multi_agent.collaboration_engine import (
   |                                                    ^
11 | from libs.multi_agent.conflict_prediction import (
12 | from libs.multi_agent.conflict_prevention import (
13 | from libs.multi_agent.conflict_resolution import ConflictSeverity, ConflictType
   |

tests/test_conflict_prevention.py:11:51: SyntaxError: Expected one or more symbol names after import
   |
 9 | from libs.multi_agent.branch_manager import BranchManager
10 | from libs.multi_agent.collaboration_engine import (
11 | from libs.multi_agent.conflict_prediction import (
   |                                                   ^
12 | from libs.multi_agent.conflict_prevention import (
13 | from libs.multi_agent.conflict_resolution import ConflictSeverity, ConflictType
   |

tests/test_conflict_prevention.py:12:51: SyntaxError: Expected one or more symbol names after import
   |
10 | from libs.multi_agent.collaboration_engine import (
11 | from libs.multi_agent.conflict_prediction import (
12 | from libs.multi_agent.conflict_prevention import (
   |                                                   ^
13 | from libs.multi_agent.conflict_resolution import ConflictSeverity, ConflictType
14 |
15 | # Copyright (c) 2024 Yesman Claude Project
   |

tests/test_conflict_prevention.py:22:1: SyntaxError: Unexpected indentation
   |
22 |     CollaborationEngine,
   | ^^^^
23 |     MessagePriority,
24 |     MessageType,
   |

tests/test_conflict_prevention.py:22:1: E113 Unexpected indentation
   |
22 |     CollaborationEngine,
   | ^^^^ E113
23 |     MessagePriority,
24 |     MessageType,
   |

tests/test_conflict_prevention.py:22:5: E303 Too many blank lines (3)
   |
22 |     CollaborationEngine,
   |     ^^^^^^^^^^^^^^^^^^^ E303
23 |     MessagePriority,
24 |     MessageType,
   |
   = help: Remove extraneous blank line(s)

tests/test_conflict_prevention.py:25:1: SyntaxError: Expected a statement
   |
23 |     MessagePriority,
24 |     MessageType,
25 | )
   | ^
26 |     ConflictPattern,
27 |     ConflictPredictor,
   |

tests/test_conflict_prevention.py:25:2: SyntaxError: Expected a statement
   |
23 |     MessagePriority,
24 |     MessageType,
25 | )
   |  ^
26 |     ConflictPattern,
27 |     ConflictPredictor,
28 |     PredictionConfidence,
   |

tests/test_conflict_prevention.py:26:1: SyntaxError: Unexpected indentation
   |
24 |     MessageType,
25 | )
26 |     ConflictPattern,
   | ^^^^
27 |     ConflictPredictor,
28 |     PredictionConfidence,
   |

tests/test_conflict_prevention.py:26:1: E113 Unexpected indentation
   |
24 |     MessageType,
25 | )
26 |     ConflictPattern,
   | ^^^^ E113
27 |     ConflictPredictor,
28 |     PredictionConfidence,
   |

tests/test_conflict_prevention.py:30:1: SyntaxError: Expected a statement
   |
28 |     PredictionConfidence,
29 |     PredictionResult,
30 | )
   | ^
31 |     ConflictPreventionSystem,
32 |     PreventionAction,
   |

tests/test_conflict_prevention.py:30:2: SyntaxError: Expected a statement
   |
28 |     PredictionConfidence,
29 |     PredictionResult,
30 | )
   |  ^
31 |     ConflictPreventionSystem,
32 |     PreventionAction,
33 |     PreventionMeasure,
   |

tests/test_conflict_prevention.py:31:1: SyntaxError: Unexpected indentation
   |
29 |     PredictionResult,
30 | )
31 |     ConflictPreventionSystem,
   | ^^^^
32 |     PreventionAction,
33 |     PreventionMeasure,
   |

tests/test_conflict_prevention.py:31:1: E113 Unexpected indentation
   |
29 |     PredictionResult,
30 | )
31 |     ConflictPreventionSystem,
   | ^^^^ E113
32 |     PreventionAction,
33 |     PreventionMeasure,
   |

tests/test_conflict_prevention.py:36:1: SyntaxError: Expected a statement
   |
34 |     PreventionResult,
35 |     PreventionStrategy,
36 | )
   | ^
   |

tests/test_conflict_prevention.py:36:2: SyntaxError: Expected a statement
   |
34 |     PreventionResult,
35 |     PreventionStrategy,
36 | )
   |  ^
37 |
38 |
39 | class TestPreventionMeasure:
   |

tests/test_conflict_resolution.py:8:51: SyntaxError: Expected one or more symbol names after import
  |
6 | import pytest
7 | from libs.multi_agent.branch_manager import BranchManager
8 | from libs.multi_agent.conflict_resolution import (
  |                                                   ^
9 |         # Test import conflict pattern
  |

tests/test_conflict_resolution.py:9:1: E116 Unexpected indentation (comment)
  |
7 | from libs.multi_agent.branch_manager import BranchManager
8 | from libs.multi_agent.conflict_resolution import (
9 |         # Test import conflict pattern
  | ^^^^^^^^ E116
  |

tests/test_conflict_resolution.py:20:1: SyntaxError: Unexpected indentation
   |
20 |     ConflictInfo,
   | ^^^^
21 |     ConflictResolutionEngine,
22 |     ConflictSeverity,
   |

tests/test_conflict_resolution.py:20:1: E113 Unexpected indentation
   |
20 |     ConflictInfo,
   | ^^^^ E113
21 |     ConflictResolutionEngine,
22 |     ConflictSeverity,
   |

tests/test_conflict_resolution.py:20:5: E303 Too many blank lines (3)
   |
20 |     ConflictInfo,
   |     ^^^^^^^^^^^^ E303
21 |     ConflictResolutionEngine,
22 |     ConflictSeverity,
   |
   = help: Remove extraneous blank line(s)

tests/test_conflict_resolution.py:26:1: SyntaxError: Expected a statement
   |
24 |     ResolutionResult,
25 |     ResolutionStrategy,
26 | )
   | ^
   |

tests/test_conflict_resolution.py:26:2: SyntaxError: Expected a statement
   |
24 |     ResolutionResult,
25 |     ResolutionStrategy,
26 | )
   |  ^
27 |
28 |
29 | class TestConflictInfo:
   |

tests/test_dashboard_launcher.py:5:8: S404 `subprocess` module is possibly insecure
  |
3 | import os
4 | import shutil
5 | import subprocess
  |        ^^^^^^^^^^ S404
6 | import sys
7 | import tempfile
  |

tests/test_dashboard_launcher.py:26:1: PLR0904 Too many public methods (44 > 20)
    |
 26 | / class TestDashboardLauncher:
 27 | |     """Test suite for DashboardLauncher functionality."""
 28 | |
 29 | |     @pytest.fixture
 30 | |     @staticmethod
 31 | |     def temp_project_root() -> Iterator[Path]:
 32 | |         """Create a temporary project directory structure.
 33 | |
 34 | |         Returns:
 35 | |         object: Description of return value.
 36 | |         """
 37 | |         with tempfile.TemporaryDirectory() as temp_dir:
 38 | |             project_root = Path(temp_dir)
 39 | |
 40 | |             # Create tauri-dashboard directory with package.json
 41 | |             tauri_dir = project_root / "tauri-dashboard"
 42 | |             tauri_dir.mkdir()
 43 | |             (tauri_dir / "package.json").write_text('{"name": "test-dashboard"}')
 44 | |
 45 | |             # Create api directory
 46 | |             api_dir = project_root / "api"
 47 | |             api_dir.mkdir()
 48 | |
 49 | |             yield project_root
 50 | |
 51 | |     @pytest.fixture
 52 | |     @staticmethod
 53 | |     def launcher(temp_project_root: Path) -> DashboardLauncher:
 54 | |         """Create DashboardLauncher instance with temp project root.
 55 | |
 56 | |         Returns:
 57 | |         DashboardLauncher: Description of return value.
 58 | |         """
 59 | |         return DashboardLauncher(project_root=temp_project_root)
 60 | |
 61 | |     @staticmethod
 62 | |     def test_init_with_project_root(temp_project_root: Path) -> None:
 63 | |         """Test launcher initialization with explicit project root."""
 64 | |         launcher = DashboardLauncher(project_root=temp_project_root)
 65 | |
 66 | |         assert launcher.project_root == temp_project_root
 67 | |         assert launcher.tauri_path == temp_project_root / "tauri-dashboard"
 68 | |         assert launcher.api_path == temp_project_root / "api"
 69 | |
 70 | |     @staticmethod
 71 | |     def test_init_without_project_root() -> None:
 72 | |         """Test launcher initialization with auto-detected project root."""
 73 | |         launcher = DashboardLauncher()
 74 | |
 75 | |         # Should auto-detect from current file location
 76 | |         expected_root = Path(__file__).resolve().parent.parent
 77 | |         assert launcher.project_root == expected_root
 78 | |
 79 | |     @staticmethod
 80 | |     def test_interface_configs_structure(launcher: DashboardLauncher) -> None:
 81 | |         """Test that interface configurations are properly structured."""
 82 | |         configs = launcher._interface_configs  # noqa: SLF001
 83 | |
 84 | |         # Check all expected interfaces exist
 85 | |         assert "tui" in configs
 86 | |         assert "web" in configs
 87 | |         assert "tauri" in configs
 88 | |
 89 | |         # Check InterfaceInfo structure
 90 | |         for config in configs.values():
 91 | |             assert isinstance(config, InterfaceInfo)
 92 | |             assert config.name
 93 | |             assert config.description
 94 | |             assert isinstance(config.requirements, list)
 95 | |             assert isinstance(config.available, bool)
 96 | |             assert isinstance(config.priority, int)
 97 | |
 98 | |     @patch("libs.dashboard.dashboard_launcher.platform.system")
 99 | |     @staticmethod
100 | |     def test_is_gui_available_macos(mock_platform: Mock, launcher: DashboardLauncher) -> None:
101 | |         """Test GUI detection on macOS."""
102 | |         mock_platform.return_value = "Darwin"
103 | |         assert launcher._is_gui_available() is True  # noqa: SLF001
104 | |
105 | |     @patch("libs.dashboard.dashboard_launcher.platform.system")
106 | |     @staticmethod
107 | |     def test_is_gui_available_windows(mock_platform: Mock, launcher: DashboardLauncher) -> None:
108 | |         """Test GUI detection on Windows."""
109 | |         mock_platform.return_value = "Windows"
110 | |         assert launcher._is_gui_available() is True  # noqa: SLF001
111 | |
112 | |     @patch("libs.dashboard.dashboard_launcher.platform.system")
113 | |     @patch.dict(os.environ, {"DISPLAY": ":0"})
114 | |     @staticmethod
115 | |     def test_is_gui_available_linux_with_display(mock_platform: Mock, launcher: DashboardLauncher) -> None:
116 | |         """Test GUI detection on Linux with DISPLAY."""
117 | |         mock_platform.return_value = "Linux"
118 | |         assert launcher._is_gui_available() is True  # noqa: SLF001
119 | |
120 | |     @patch("libs.dashboard.dashboard_launcher.platform.system")
121 | |     @patch.dict(os.environ, {"WAYLAND_DISPLAY": "wayland-0"})
122 | |     @staticmethod
123 | |     def test_is_gui_available_linux_with_wayland(mock_platform: Mock, launcher: DashboardLauncher) -> None:
124 | |         """Test GUI detection on Linux with Wayland."""
125 | |         mock_platform.return_value = "Linux"
126 | |         assert launcher._is_gui_available() is True  # noqa: SLF001
127 | |
128 | |     @patch("libs.dashboard.dashboard_launcher.platform.system")
129 | |     @patch.dict(os.environ, {}, clear=True)
130 | |     @staticmethod
131 | |     def test_is_gui_available_linux_without_display(mock_platform: Mock, launcher: DashboardLauncher) -> None:
132 | |         """Test GUI detection on Linux without display."""
133 | |         mock_platform.return_value = "Linux"
134 | |         assert launcher._is_gui_available() is False  # noqa: SLF001
135 | |
136 | |     @patch.dict(os.environ, {"SSH_CLIENT": "192.168.1.1 12345"})
137 | |     @staticmethod
138 | |     def test_is_ssh_session_with_ssh_client(launcher: DashboardLauncher) -> None:
139 | |         """Test SSH detection with SSH_CLIENT."""
140 | |         assert launcher._is_ssh_session() is True  # noqa: SLF001
141 | |
142 | |     @patch.dict(os.environ, {"SSH_TTY": "/dev/pts/0"})
143 | |     @staticmethod
144 | |     def test_is_ssh_session_with_ssh_tty(launcher: DashboardLauncher) -> None:
145 | |         """Test SSH detection with SSH_TTY."""
146 | |         assert launcher._is_ssh_session() is True  # noqa: SLF001
147 | |
148 | |     @patch.dict(os.environ, {}, clear=True)
149 | |     @staticmethod
150 | |     def test_is_ssh_session_without_ssh(launcher: DashboardLauncher) -> None:
151 | |         """Test SSH detection without SSH environment."""
152 | |         assert launcher._is_ssh_session() is False  # noqa: SLF001
153 | |
154 | |     @patch("sys.stdout.isatty")
155 | |     @patch.dict(os.environ, {"TERM": "xterm-256color"})
156 | |     @staticmethod
157 | |     def test_is_terminal_capable_with_tty(mock_isatty: Mock, launcher: DashboardLauncher) -> None:
158 | |         """Test terminal capability with TTY."""
159 | |         mock_isatty.return_value = True
160 | |         assert launcher._is_terminal_capable() is True  # noqa: SLF001
161 | |
162 | |     @patch("sys.stdout.isatty")
163 | |     @patch.dict(os.environ, {"TERM": "dumb"})
164 | |     @staticmethod
165 | |     def test_is_terminal_capable_with_dumb_term(mock_isatty: Mock, launcher: DashboardLauncher) -> None:
166 | |         """Test terminal capability with dumb terminal."""
167 | |         mock_isatty.return_value = True
168 | |         assert launcher._is_terminal_capable() is False  # noqa: SLF001
169 | |
170 | |     @patch("sys.stdout.isatty")
171 | |     @staticmethod
172 | |     def test_is_terminal_capable_without_tty(mock_isatty: Mock, launcher: DashboardLauncher) -> None:
173 | |         """Test terminal capability without TTY."""
174 | |         mock_isatty.return_value = False
175 | |         assert launcher._is_terminal_capable() is False  # noqa: SLF001
176 | |
177 | |     @patch("libs.dashboard.dashboard_launcher.shutil.which")
178 | |     @staticmethod
179 | |     def test_is_node_available_with_both(mock_which: Mock, launcher: DashboardLauncher) -> None:
180 | |         """Test Node.js availability with both node and npm."""
181 | |         mock_which.side_effect = lambda cmd: ("/usr/bin/" + cmd if cmd in {"node", "npm"} else None)
182 | |         assert launcher._is_node_available() is True  # noqa: SLF001
183 | |
184 | |     @patch("libs.dashboard.dashboard_launcher.shutil.which")
185 | |     @staticmethod
186 | |     def test_is_node_available_missing_node(mock_which: Mock, launcher: DashboardLauncher) -> None:
187 | |         """Test Node.js availability with missing node."""
188 | |         mock_which.side_effect = lambda cmd: "/usr/bin/npm" if cmd == "npm" else None
189 | |         assert launcher._is_node_available() is False  # noqa: SLF001
190 | |
191 | |     @patch("libs.dashboard.dashboard_launcher.shutil.which")
192 | |     @staticmethod
193 | |     def test_is_node_available_missing_npm(mock_which: Mock, launcher: DashboardLauncher) -> None:
194 | |         """Test Node.js availability with missing npm."""
195 | |         mock_which.side_effect = lambda cmd: "/usr/bin/node" if cmd == "node" else None
196 | |         assert launcher._is_node_available() is False  # noqa: SLF001
197 | |
198 | |     @staticmethod
199 | |     def test_is_tauri_available_with_complete_setup(launcher: DashboardLauncher) -> None:
200 | |         """Test Tauri availability with complete setup."""
201 | |         with patch.object(launcher, "_is_node_available", return_value=True):
202 | |             assert launcher._is_tauri_available() is True  # noqa: SLF001
203 | |
204 | |     @staticmethod
205 | |     def test_is_tauri_available_missing_directory(temp_project_root: Path) -> None:
206 | |         """Test Tauri availability with missing directory."""
207 | |         # Remove the tauri directory
208 | |         shutil.rmtree(temp_project_root / "tauri-dashboard")
209 | |
210 | |         launcher = DashboardLauncher(project_root=temp_project_root)
211 | |         assert launcher._is_tauri_available() is False  # noqa: SLF001
212 | |
213 | |     @staticmethod
214 | |     def test_is_tauri_available_missing_package_json(launcher: DashboardLauncher) -> None:
215 | |         """Test Tauri availability with missing package.json."""
216 | |         # Remove package.json
217 | |         (launcher.tauri_path / "package.json").unlink()
218 | |
219 | |         assert launcher._is_tauri_available() is False  # noqa: SLF001
220 | |
221 | |     @staticmethod
222 | |     def test_is_tauri_available_missing_node(launcher: DashboardLauncher) -> None:
223 | |         """Test Tauri availability with missing Node.js."""
224 | |         with patch.object(launcher, "_is_node_available", return_value=False):
225 | |             assert launcher._is_tauri_available() is False  # noqa: SLF001
226 | |
227 | |     @staticmethod
228 | |     def test_is_python_package_available_existing(launcher: DashboardLauncher) -> None:
229 | |         """Test Python package availability for existing package."""
230 | |         assert launcher._is_python_package_available("sys") is True  # noqa: SLF001
231 | |
232 | |     @staticmethod
233 | |     def test_is_python_package_available_missing(launcher: DashboardLauncher) -> None:
234 | |         """Test Python package availability for missing package."""
235 | |         assert launcher._is_python_package_available("nonexistent_package_12345") is False  # noqa: SLF001
236 | |
237 | |     @staticmethod
238 | |     def test_get_interface_info_valid(launcher: DashboardLauncher) -> None:
239 | |         """Test getting interface info for valid interface."""
240 | |         info = launcher.get_interface_info("tui")
241 | |
242 | |         assert isinstance(info, InterfaceInfo)
243 | |         assert info.name == "Terminal User Interface"
244 | |         assert "rich" in info.requirements
245 | |
246 | |     @staticmethod
247 | |     def test_get_interface_info_invalid(launcher: DashboardLauncher) -> None:
248 | |         """Test getting interface info for invalid interface."""
249 | |         with pytest.raises(ValueError, match="Unknown interface: invalid"):
250 | |             launcher.get_interface_info("invalid")
251 | |
252 | |     @staticmethod
253 | |     def test_get_available_interfaces(launcher: DashboardLauncher) -> None:
254 | |         """Test getting all available interfaces."""
255 | |         interfaces = launcher.get_available_interfaces()
256 | |
257 | |         assert len(interfaces) == 3
258 | |         assert "tui" in interfaces
259 | |         assert "web" in interfaces
260 | |         assert "tauri" in interfaces
261 | |
262 | |         for info in interfaces.values():
263 | |             assert isinstance(info, InterfaceInfo)
264 | |
265 | |     @patch.object(DashboardLauncher, "_is_ssh_session")
266 | |     @staticmethod
267 | |     def test_detect_best_interface_ssh(mock_ssh: Mock, launcher: DashboardLauncher) -> None:
268 | |         """Test interface detection in SSH environment."""
269 | |         mock_ssh.return_value = True
270 | |
271 | |         result = launcher.detect_best_interface()
272 | |         assert result == "tui"
273 | |
274 | |     @patch.object(DashboardLauncher, "_is_ssh_session")
275 | |     @patch.object(DashboardLauncher, "_is_gui_available")
276 | |     @staticmethod
277 | |     def test_detect_best_interface_gui_with_tauri(mock_gui: Mock, mock_ssh: Mock, launcher: DashboardLauncher) -> None:
278 | |         """Test interface detection with GUI and Tauri available."""
279 | |         mock_ssh.return_value = False
280 | |         mock_gui.return_value = True
281 | |
282 | |         # Mock tauri as available
283 | |         launcher._interface_configs["tauri"].available = True  # noqa: SLF001
284 | |
285 | |         result = launcher.detect_best_interface()
286 | |         assert result == "tauri"
287 | |
288 | |     @patch.object(DashboardLauncher, "_is_ssh_session")
289 | |     @patch.object(DashboardLauncher, "_is_gui_available")
290 | |     @patch.object(DashboardLauncher, "_is_terminal_capable")
291 | |     @staticmethod
292 | |     def test_detect_best_interface_gui_without_tauri_no_terminal(mock_term: Mock, mock_gui: Mock, mock_ssh: Mock, launcher: Dashboardâ€¦
293 | |         """Test interface detection with GUI but no Tauri and no terminal capability."""
294 | |         mock_ssh.return_value = False
295 | |         mock_gui.return_value = True
296 | |         mock_term.return_value = False
297 | |
298 | |         # Mock tauri as unavailable
299 | |         launcher._interface_configs["tauri"].available = False  # noqa: SLF001
300 | |
301 | |         result = launcher.detect_best_interface()
302 | |         assert result == "web"
303 | |
304 | |     @patch.object(DashboardLauncher, "_is_ssh_session")
305 | |     @patch.object(DashboardLauncher, "_is_terminal_capable")
306 | |     @staticmethod
307 | |     def test_detect_best_interface_terminal_capable(mock_term: Mock, mock_ssh: Mock, launcher: DashboardLauncher) -> None:
308 | |         """Test interface detection with terminal capability."""
309 | |         mock_ssh.return_value = False
310 | |         mock_term.return_value = True
311 | |
312 | |         result = launcher.detect_best_interface()
313 | |         assert result == "tui"
314 | |
315 | |     @patch.object(DashboardLauncher, "_is_ssh_session")
316 | |     @patch.object(DashboardLauncher, "_is_terminal_capable")
317 | |     @staticmethod
318 | |     def test_detect_best_interface_fallback_to_web(mock_term: Mock, mock_ssh: Mock, launcher: DashboardLauncher) -> None:
319 | |         """Test interface detection fallback to web."""
320 | |         mock_ssh.return_value = False
321 | |         mock_term.return_value = False
322 | |
323 | |         result = launcher.detect_best_interface()
324 | |         assert result == "web"
325 | |
326 | |     @staticmethod
327 | |     def test_check_system_requirements_structure(launcher: DashboardLauncher) -> None:
328 | |         """Test system requirements check structure."""
329 | |         reqs = launcher.check_system_requirements()
330 | |
331 | |         assert "tui" in reqs
332 | |         assert "web" in reqs
333 | |         assert "tauri" in reqs
334 | |
335 | |         for req_info in reqs.values():
336 | |             assert "available" in req_info
337 | |             assert "reason" in req_info
338 | |             assert "requirements" in req_info
339 | |             assert "missing" in req_info
340 | |             assert isinstance(req_info["requirements"], dict)
341 | |             assert isinstance(req_info["missing"], list)
342 | |
343 | |     @patch("subprocess.run")
344 | |     @staticmethod
345 | |     def test_check_requirement_node_success(mock_run: Mock, launcher: DashboardLauncher) -> None:
346 | |         """Test checking Node.js requirement successfully."""
347 | |         mock_run.return_value = MagicMock(stdout="v18.0.0\\n")
348 | |
349 | |         with patch(
350 | |             "libs.dashboard.dashboard_launcher.shutil.which",
351 | |             return_value="/usr/bin/node",
352 | |         ):
353 | |             status, details = launcher._check_requirement("node")  # noqa: SLF001
354 | |
355 | |             assert status is True
356 | |             assert "v18.0.0" in details
357 | |
358 | |     @patch("subprocess.run")
359 | |     @staticmethod
360 | |     def test_check_requirement_node_failure(mock_run: Mock, launcher: DashboardLauncher) -> None:
361 | |         """Test checking Node.js requirement with failure."""
362 | |         mock_run.side_effect = subprocess.CalledProcessError(1, ["node"])
363 | |
364 | |         with patch(
365 | |             "libs.dashboard.dashboard_launcher.shutil.which",
366 | |             return_value="/usr/bin/node",
367 | |         ):
368 | |             status, details = launcher._check_requirement("node")  # noqa: SLF001
369 | |
370 | |             assert status is False
371 | |             assert "not working" in details
372 | |
373 | |     @staticmethod
374 | |     def test_check_requirement_node_missing(launcher: DashboardLauncher) -> None:
375 | |         """Test checking Node.js requirement when missing."""
376 | |         with patch("libs.dashboard.dashboard_launcher.shutil.which", return_value=None):
377 | |             status, details = launcher._check_requirement("node")  # noqa: SLF001
378 | |
379 | |             assert status is False
380 | |             assert "not installed" in details
381 | |
382 | |     @staticmethod
383 | |     def test_check_requirement_python(launcher: DashboardLauncher) -> None:
384 | |         """Test checking Python requirement."""
385 | |         status, details = launcher._check_requirement("python")  # noqa: SLF001
386 | |
387 | |         assert status is True
388 | |         assert "Python" in details
389 | |         assert sys.version.split()[0] in details
390 | |
391 | |     @staticmethod
392 | |     def test_check_requirement_unknown(launcher: DashboardLauncher) -> None:
393 | |         """Test checking unknown requirement."""
394 | |         status, details = launcher._check_requirement("unknown_requirement")  # noqa: SLF001
395 | |
396 | |         assert status is False
397 | |         assert "Unknown requirement" in details
398 | |
399 | |     @patch("subprocess.run")
400 | |     @patch.object(DashboardLauncher, "_is_python_package_available")
401 | |     @staticmethod
402 | |     def test_install_dependencies_web_success(mock_available: Mock, mock_run: Mock, launcher: DashboardLauncher) -> None:
403 | |         """Test installing web dependencies successfully."""
404 | |         mock_available.side_effect = [False, False]  # fastapi and uvicorn not available
405 | |         mock_run.return_value = MagicMock()
406 | |
407 | |         launcher.install_dependencies("web")
408 | |
409 | |         # Should call pip install for both packages
410 | |         assert mock_run.call_count == 2
411 | |
412 | |     @staticmethod
413 | |     def test_install_dependencies_invalid_interface(launcher: DashboardLauncher) -> None:
414 | |         """Test installing dependencies for invalid interface."""
415 | |         result = launcher.install_dependencies("invalid")
416 | |         assert result is False
417 | |
418 | |     @patch("subprocess.run")
419 | |     @patch.object(DashboardLauncher, "_is_node_available")
420 | |     @patch("os.chdir")
421 | |     @staticmethod
422 | |     def test_install_dependencies_tauri_success(mock_chdir: Mock, mock_node: Mock, mock_run: Mock, launcher: DashboardLauncher) -> Noâ€¦
423 | |         """Test installing Tauri dependencies successfully."""
424 | |         mock_node.return_value = True
425 | |         mock_run.return_value = MagicMock()
426 | |
427 | |         launcher.install_dependencies("tauri")
428 | |
429 | |         # Should change directory and run npm install
430 | |         assert mock_chdir.call_count >= 1
431 | |         mock_run.assert_called_with(["npm", "install"], check=True)
432 | |
433 | |     @patch.object(DashboardLauncher, "_is_node_available")
434 | |     @staticmethod
435 | |     def test_install_dependencies_tauri_no_node(mock_node: Mock, launcher: DashboardLauncher) -> None:
436 | |         """Test installing Tauri dependencies without Node.js."""
437 | |         mock_node.return_value = False
438 | |
439 | |         result = launcher.install_dependencies("tauri")
440 | |         assert result is False
441 | |
442 | |     @staticmethod
443 | |     def test_install_dependencies_tauri_no_directory(temp_project_root: Path) -> None:
444 | |         """Test installing Tauri dependencies without Tauri directory."""
445 | |         # Remove tauri directory
446 | |         shutil.rmtree(temp_project_root / "tauri-dashboard")
447 | |
448 | |         launcher = DashboardLauncher(project_root=temp_project_root)
449 | |         result = launcher.install_dependencies("tauri")
450 | |         assert result is False
    | |______________________________^ PLR0904
    |

tests/test_dependency_propagation.py:13:54: SyntaxError: Expected one or more symbol names after import
   |
11 | from libs.multi_agent.branch_manager import BranchManager
12 | from libs.multi_agent.collaboration_engine import CollaborationEngine
13 | from libs.multi_agent.dependency_propagation import (
   |                                                      ^
14 | import pandas as pd
15 | from .utils import process_data, validate_input
16 | from src.main import DataProcessor
   |

tests/test_dependency_propagation.py:25:1: SyntaxError: Unexpected indentation
   |
25 |     ChangeImpact,
   | ^^^^
26 |     DependencyChange,
27 |     DependencyNode,
   |

tests/test_dependency_propagation.py:25:1: E113 Unexpected indentation
   |
25 |     ChangeImpact,
   | ^^^^ E113
26 |     DependencyChange,
27 |     DependencyNode,
   |

tests/test_dependency_propagation.py:25:5: E303 Too many blank lines (3)
   |
25 |     ChangeImpact,
   |     ^^^^^^^^^^^^ E303
26 |     DependencyChange,
27 |     DependencyNode,
   |
   = help: Remove extraneous blank line(s)

tests/test_dependency_propagation.py:32:1: SyntaxError: Expected a statement
   |
30 |     PropagationResult,
31 |     PropagationStrategy,
32 | )
   | ^
   |

tests/test_dependency_propagation.py:32:2: SyntaxError: Expected a statement
   |
30 |     PropagationResult,
31 |     PropagationStrategy,
32 | )
   |  ^
33 |
34 |
35 | class TestDependencyNode:
   |

tests/test_dynamic_redistribution.py:23:9: ANN205 Missing return type annotation for staticmethod `agent_pool`
   |
21 |     @pytest.fixture
22 |     @staticmethod
23 |     def agent_pool():
   |         ^^^^^^^^^^ ANN205
24 |         """Create agent pool for testing."""
25 |         return AgentPool(max_agents=3, work_dir="/tmp/test-agents")
   |
   = help: Add return type annotation

tests/test_dynamic_redistribution.py:29:9: ANN205 Missing return type annotation for staticmethod `scheduler`
   |
27 |     @pytest.fixture
28 |     @staticmethod
29 |     def scheduler():
   |         ^^^^^^^^^ ANN205
30 |         """Create scheduler for testing."""
31 |         return TaskScheduler()
   |
   = help: Add return type annotation

tests/test_error_handling.py:5:39: SyntaxError: Expected one or more symbol names after import
  |
3 | from fastapi import status
4 | from api.middleware.error_handler import create_error_response, error_to_status_code
5 | from libs.core.error_handling import (
  |                                       ^
6 | import json
7 |
8 | # Copyright (c) 2024 Yesman Claude Project
  |

tests/test_error_handling.py:14:1: SyntaxError: Unexpected indentation
   |
14 |     ConfigurationError,
   | ^^^^
15 |     ErrorCategory,
16 |     ErrorContext,
   |

tests/test_error_handling.py:14:1: E113 Unexpected indentation
   |
14 |     ConfigurationError,
   | ^^^^ E113
15 |     ErrorCategory,
16 |     ErrorContext,
   |

tests/test_error_handling.py:14:5: E303 Too many blank lines (2)
   |
14 |     ConfigurationError,
   |     ^^^^^^^^^^^^^^^^^^ E303
15 |     ErrorCategory,
16 |     ErrorContext,
   |
   = help: Remove extraneous blank line(s)

tests/test_error_handling.py:22:1: SyntaxError: Expected a statement
   |
20 |     ValidationError,
21 |     YesmanError,
22 | )
   | ^
   |

tests/test_error_handling.py:22:2: SyntaxError: Expected a statement
   |
20 |     ValidationError,
21 |     YesmanError,
22 | )
   |  ^
23 |
24 |
25 | class TestYesmanError:
   |

tests/test_metrics_verification.py:7:48: SyntaxError: Expected one or more symbol names after import
   |
 5 | import pytest
 6 | from libs.multi_agent.agent_pool import AgentPool
 7 | from libs.multi_agent.metrics_verifier import (
   |                                                ^
 8 |
 9 |
10 | # Copyright notice.
   |

tests/test_metrics_verification.py:18:1: SyntaxError: Unexpected indentation
   |
18 |     MetricsVerifier,
   | ^^^^
19 |     PerformanceMetrics,
20 |     SuccessCriteria,
   |

tests/test_metrics_verification.py:18:1: E113 Unexpected indentation
   |
18 |     MetricsVerifier,
   | ^^^^ E113
19 |     PerformanceMetrics,
20 |     SuccessCriteria,
   |

tests/test_metrics_verification.py:18:5: E303 Too many blank lines (3)
   |
18 |     MetricsVerifier,
   |     ^^^^^^^^^^^^^^^ E303
19 |     PerformanceMetrics,
20 |     SuccessCriteria,
   |
   = help: Remove extraneous blank line(s)

tests/test_metrics_verification.py:21:1: SyntaxError: Expected a statement
   |
19 |     PerformanceMetrics,
20 |     SuccessCriteria,
21 | )
   | ^
   |

tests/test_metrics_verification.py:21:2: SyntaxError: Expected a statement
   |
19 |     PerformanceMetrics,
20 |     SuccessCriteria,
21 | )
   |  ^
22 |
23 |
24 | class TestPerformanceMetrics:
   |

tests/test_multi_agent_cli.py:17:1: PLR0904 Too many public methods (21 > 20)
    |
 17 | / class TestMultiAgentCLI:
 18 | |     """Test cases for multi-agent CLI commands."""
 19 | |
 20 | |     @pytest.fixture
 21 | |     @staticmethod
 22 | |     def runner() -> CliRunner:
 23 | |         """Create CLI test runner."""
 24 | |         return CliRunner()
 25 | |
 26 | |     @pytest.fixture
 27 | |     @staticmethod
 28 | |     def mock_agent_pool() -> Mock:
 29 | |         """Create mock agent pool."""
 30 | |         pool = Mock()
 31 | |         pool._running = True
 32 | |         pool.start = AsyncMock()
 33 | |         pool.stop = AsyncMock()
 34 | |         pool._load_state = Mock()
 35 | |         pool.get_pool_statistics.return_value = {
 36 | |             "active_agents": 2,
 37 | |             "idle_agents": 1,
 38 | |             "completed_tasks": 10,
 39 | |             "failed_tasks": 2,
 40 | |             "queue_size": 3,
 41 | |             "average_execution_time": 45.5,
 42 | |         }
 43 | |         pool.list_agents.return_value = [
 44 | |             {
 45 | |                 "agent_id": "agent-1",
 46 | |                 "state": "working",
 47 | |                 "completed_tasks": 5,
 48 | |                 "failed_tasks": 1,
 49 | |             },
 50 | |             {
 51 | |                 "agent_id": "agent-2",
 52 | |                 "state": "idle",
 53 | |                 "completed_tasks": 3,
 54 | |                 "failed_tasks": 0,
 55 | |             },
 56 | |         ]
 57 | |         pool.list_tasks.return_value = [
 58 | |             {
 59 | |                 "task_id": "task-123",
 60 | |                 "title": "Test Task",
 61 | |                 "status": "running",
 62 | |                 "command": ["echo", "test"],
 63 | |                 "assigned_agent": "agent-1",
 64 | |             },
 65 | |         ]
 66 | |
 67 | |         # Mock task creation
 68 | |         mock_task = Mock()
 69 | |         mock_task.task_id = "new-task-456"
 70 | |         mock_task.title = "New Task"
 71 | |         mock_task.command = ["ls", "-la"]
 72 | |         mock_task.priority = 5
 73 | |         pool.create_task.return_value = mock_task
 74 | |
 75 | |         return pool
 76 | |
 77 | |     @staticmethod
 78 | |     def test_multi_agent_cli_help(runner: CliRunner) -> None:
 79 | |         """Test multi-agent CLI help."""
 80 | |         result = runner.invoke(multi_agent_cli, ["--help"])
 81 | |
 82 | |         assert result.exit_code == 0
 83 | |         assert "Multi-agent system" in result.output
 84 | |         assert "start" in result.output
 85 | |         assert "monitor" in result.output
 86 | |         assert "status" in result.output
 87 | |
 88 | |     @patch("commands.multi_agent.AgentPool")
 89 | |     @patch("commands.multi_agent.asyncio.run")
 90 | |     @staticmethod
 91 | |     def test_start_agents_basic(
 92 | |         mock_asyncio_run: Mock,
 93 | |         mock_agent_pool_class: Mock,
 94 | |         runner: CliRunner,
 95 | |         mock_agent_pool: Mock,
 96 | |     ) -> None:
 97 | |         """Test basic agent start command."""
 98 | |         mock_agent_pool_class.return_value = mock_agent_pool
 99 | |
100 | |         result = runner.invoke(multi_agent_cli, ["start"])
101 | |
102 | |         assert result.exit_code == 0
103 | |         assert "Starting multi-agent pool" in result.output
104 | |         mock_agent_pool_class.assert_called_once_with(max_agents=3, work_dir=None)
105 | |         mock_asyncio_run.assert_called_once()
106 | |
107 | |     @patch("commands.multi_agent.AgentPool")
108 | |     @patch("commands.multi_agent.asyncio.run")
109 | |     @staticmethod
110 | |     def test_start_agents_with_options(
111 | |         mock_asyncio_run: Mock,  # noqa: ARG002
112 | |         mock_agent_pool_class: Mock,
113 | |         runner: CliRunner,
114 | |         mock_agent_pool: Mock,
115 | |     ) -> None:
116 | |         """Test agent start with options."""
117 | |         mock_agent_pool_class.return_value = mock_agent_pool
118 | |
119 | |         result = runner.invoke(
120 | |             multi_agent_cli,
121 | |             ["start", "--max-agents", "5", "--work-dir", "/tmp/test"],
122 | |         )
123 | |
124 | |         assert result.exit_code == 0
125 | |         mock_agent_pool_class.assert_called_once_with(
126 | |             max_agents=5,
127 | |             work_dir="/tmp/test",
128 | |         )
129 | |
130 | |     @patch("commands.multi_agent.AgentPool")
131 | |     @patch("commands.multi_agent.run_agent_monitor")
132 | |     @patch("commands.multi_agent.asyncio.run")
133 | |     @staticmethod
134 | |     def test_start_agents_with_monitor(
135 | |         mock_asyncio_run: Mock,  # noqa: ARG002
136 | |         mock_run_monitor: Mock,
137 | |         mock_agent_pool_class: Mock,
138 | |         runner: CliRunner,
139 | |         mock_agent_pool: Mock,
140 | |     ) -> None:
141 | |         """Test agent start with monitoring."""
142 | |         mock_agent_pool_class.return_value = mock_agent_pool
143 | |         mock_run_monitor.return_value = AsyncMock()
144 | |
145 | |         result = runner.invoke(multi_agent_cli, ["start", "--monitor"])
146 | |
147 | |         assert result.exit_code == 0
148 | |         assert "Starting monitoring dashboard" in result.output
149 | |
150 | |     @patch("commands.multi_agent.AgentPool")
151 | |     @staticmethod
152 | |     def test_status_command(mock_agent_pool_class: Mock, runner: CliRunner, mock_agent_pool: Mock) -> None:
153 | |         """Test status command."""
154 | |         mock_agent_pool_class.return_value = mock_agent_pool
155 | |
156 | |         result = runner.invoke(multi_agent_cli, ["status"])
157 | |
158 | |         assert result.exit_code == 0
159 | |         assert "Multi-Agent Pool Status" in result.output
160 | |         assert "Total Agents: 2" in result.output
161 | |         assert "Active Agents: 2" in result.output
162 | |         assert "Completed Tasks: 10" in result.output
163 | |         assert "agent-1" in result.output
164 | |         assert "agent-2" in result.output
165 | |
166 | |     @patch("commands.multi_agent.AgentPool")
167 | |     @patch("commands.multi_agent.asyncio.run")
168 | |     @staticmethod
169 | |     def test_stop_command(
170 | |         mock_asyncio_run: Mock,
171 | |         mock_agent_pool_class: Mock,
172 | |         runner: CliRunner,
173 | |         mock_agent_pool: Mock,
174 | |     ) -> None:
175 | |         """Test stop command."""
176 | |         mock_agent_pool_class.return_value = mock_agent_pool
177 | |
178 | |         result = runner.invoke(multi_agent_cli, ["stop"])
179 | |
180 | |         assert result.exit_code == 0
181 | |         assert "Stopping multi-agent pool" in result.output
182 | |         mock_asyncio_run.assert_called_once()
183 | |
184 | |     @patch("commands.multi_agent.AgentPool")
185 | |     @patch("commands.multi_agent.asyncio.run")
186 | |     @staticmethod
187 | |     def test_monitor_command(
188 | |         mock_asyncio_run: Mock,
189 | |         mock_agent_pool_class: Mock,
190 | |         runner: CliRunner,
191 | |         mock_agent_pool: Mock,
192 | |     ) -> None:
193 | |         """Test monitor command."""
194 | |         mock_agent_pool_class.return_value = mock_agent_pool
195 | |
196 | |         result = runner.invoke(multi_agent_cli, ["monitor"])
197 | |
198 | |         assert result.exit_code == 0
199 | |         assert "Starting agent monitoring dashboard" in result.output
200 | |         mock_asyncio_run.assert_called_once()
201 | |
202 | |     @patch("commands.multi_agent.AgentPool")
203 | |     @patch("commands.multi_agent.asyncio.run")
204 | |     @staticmethod
205 | |     def test_monitor_with_options(
206 | |         mock_asyncio_run: Mock,
207 | |         mock_agent_pool_class: Mock,
208 | |         runner: CliRunner,
209 | |         mock_agent_pool: Mock,
210 | |     ) -> None:
211 | |         """Test monitor command with options."""
212 | |         mock_agent_pool_class.return_value = mock_agent_pool
213 | |
214 | |         result = runner.invoke(
215 | |             multi_agent_cli,
216 | |             ["monitor", "--duration", "10", "--refresh", "2.0"],
217 | |         )
218 | |
219 | |         assert result.exit_code == 0
220 | |         mock_asyncio_run.assert_called_once()
221 | |
222 | |     @patch("commands.multi_agent.AgentPool")
223 | |     @staticmethod
224 | |     def test_add_task_command(mock_agent_pool_class: Mock, runner: CliRunner, mock_agent_pool: Mock) -> None:
225 | |         """Test add-task command."""
226 | |         mock_agent_pool_class.return_value = mock_agent_pool
227 | |
228 | |         result = runner.invoke(
229 | |             multi_agent_cli,
230 | |             [
231 | |                 "add-task",
232 | |                 "Test Task",
233 | |                 "echo",
234 | |                 "hello",
235 | |                 "world",
236 | |                 "--priority",
237 | |                 "8",
238 | |                 "--complexity",
239 | |                 "3",
240 | |                 "--timeout",
241 | |                 "120",
242 | |             ],
243 | |         )
244 | |
245 | |         assert result.exit_code == 0
246 | |         assert "Task added: new-task-456" in result.output
247 | |         assert "Title: New Task" in result.output
248 | |         assert "Priority: 5" in result.output  # From mock task
249 | |
250 | |         # Verify create_task was called with correct parameters
251 | |         mock_agent_pool.create_task.assert_called_once()
252 | |         call_kwargs = mock_agent_pool.create_task.call_args[1]
253 | |         assert call_kwargs["title"] == "Test Task"
254 | |         assert call_kwargs["command"] == ["echo", "hello", "world"]
255 | |         assert call_kwargs["priority"] == 8
256 | |         assert call_kwargs["complexity"] == 3
257 | |         assert call_kwargs["timeout"] == 120
258 | |
259 | |     @patch("commands.multi_agent.AgentPool")
260 | |     @staticmethod
261 | |     def test_list_tasks_command(mock_agent_pool_class: Mock, runner: CliRunner, mock_agent_pool: Mock) -> None:
262 | |         """Test list-tasks command."""
263 | |         mock_agent_pool_class.return_value = mock_agent_pool
264 | |
265 | |         result = runner.invoke(multi_agent_cli, ["list-tasks"])
266 | |
267 | |         assert result.exit_code == 0
268 | |         assert "Tasks (1 found)" in result.output
269 | |         assert "task-123" in result.output
270 | |         assert "Test Task" in result.output
271 | |         assert "RUNNING" in result.output
272 | |         assert "Agent: agent-1" in result.output
273 | |
274 | |     @patch("commands.multi_agent.AgentPool")
275 | |     @staticmethod
276 | |     def test_list_tasks_with_status_filter(
277 | |         mock_agent_pool_class: Mock,
278 | |         runner: CliRunner,
279 | |         mock_agent_pool: Mock,
280 | |     ) -> None:
281 | |         """Test list-tasks with status filter."""
282 | |         mock_agent_pool_class.return_value = mock_agent_pool
283 | |
284 | |         # Mock filtered response
285 | |
286 | |         mock_agent_pool.list_tasks.return_value = []  # No running tasks
287 | |
288 | |         result = runner.invoke(multi_agent_cli, ["list-tasks", "--status", "running"])
289 | |
290 | |         assert result.exit_code == 0
291 | |         assert "No tasks found" in result.output
292 | |         mock_agent_pool.list_tasks.assert_called_once_with(TaskStatus.RUNNING)
293 | |
294 | |     @patch("commands.multi_agent.AgentPool")
295 | |     @staticmethod
296 | |     def test_list_tasks_invalid_status(
297 | |         mock_agent_pool_class: Mock,
298 | |         runner: CliRunner,
299 | |         mock_agent_pool: Mock,
300 | |     ) -> None:
301 | |         """Test list-tasks with invalid status."""
302 | |         mock_agent_pool_class.return_value = mock_agent_pool
303 | |
304 | |         result = runner.invoke(multi_agent_cli, ["list-tasks", "--status", "invalid"])
305 | |
306 | |         assert result.exit_code == 0
307 | |         assert "Invalid status: invalid" in result.output
308 | |
309 | |     @patch("commands.multi_agent.AgentPool")
310 | |     @staticmethod
311 | |     def test_status_command_exception(mock_agent_pool_class: Mock, runner: CliRunner) -> None:
312 | |         """Test status command with exception."""
313 | |         mock_agent_pool_class.side_effect = Exception("Test error")
314 | |
315 | |         result = runner.invoke(multi_agent_cli, ["status"])
316 | |
317 | |         assert result.exit_code == 0
318 | |         assert "Error getting status: Test error" in result.output
319 | |
320 | |     @patch("commands.multi_agent.AgentPool")
321 | |     @staticmethod
322 | |     def test_add_task_exception(mock_agent_pool_class: Mock, runner: CliRunner) -> None:
323 | |         """Test add-task command with exception."""
324 | |         mock_agent_pool_class.side_effect = Exception("Test error")
325 | |
326 | |         result = runner.invoke(multi_agent_cli, ["add-task", "Test", "echo", "test"])
327 | |
328 | |         assert result.exit_code == 0
329 | |         assert "Error adding task: Test error" in result.output
330 | |
331 | |     @patch("commands.multi_agent.AgentPool")
332 | |     @staticmethod
333 | |     def test_list_tasks_exception(mock_agent_pool_class: Mock, runner: CliRunner) -> None:
334 | |         """Test list-tasks command with exception."""
335 | |         mock_agent_pool_class.side_effect = Exception("Test error")
336 | |
337 | |         result = runner.invoke(multi_agent_cli, ["list-tasks"])
338 | |
339 | |         assert result.exit_code == 0
340 | |         assert "Error listing tasks: Test error" in result.output
341 | |
342 | |     @staticmethod
343 | |     def test_add_task_required_arguments(runner: CliRunner) -> None:
344 | |         """Test add-task command missing required arguments."""
345 | |         result = runner.invoke(multi_agent_cli, ["add-task", "Test Task"])
346 | |
347 | |         assert result.exit_code != 0
348 | |         assert "Missing argument" in result.output
349 | |
350 | |     @patch("commands.multi_agent.AgentPool")
351 | |     @staticmethod
352 | |     def test_add_task_with_description(
353 | |         mock_agent_pool_class: Mock,
354 | |         runner: CliRunner,
355 | |         mock_agent_pool: Mock,
356 | |     ) -> None:
357 | |         """Test add-task with custom description."""
358 | |         mock_agent_pool_class.return_value = mock_agent_pool
359 | |
360 | |         result = runner.invoke(
361 | |             multi_agent_cli,
362 | |             [
363 | |                 "add-task",
364 | |                 "Test Task",
365 | |                 "echo",
366 | |                 "hello",
367 | |                 "--description",
368 | |                 "Custom description",
369 | |             ],
370 | |         )
371 | |
372 | |         assert result.exit_code == 0
373 | |
374 | |         call_kwargs = mock_agent_pool.create_task.call_args[1]
375 | |         assert call_kwargs["description"] == "Custom description"
376 | |
377 | |     @patch("commands.multi_agent.AgentPool")
378 | |     @staticmethod
379 | |     def test_add_task_default_description(
380 | |         mock_agent_pool_class: Mock,
381 | |         runner: CliRunner,
382 | |         mock_agent_pool: Mock,
383 | |     ) -> None:
384 | |         """Test add-task with default description."""
385 | |         mock_agent_pool_class.return_value = mock_agent_pool
386 | |
387 | |         result = runner.invoke(
388 | |             multi_agent_cli,
389 | |             ["add-task", "Test Task", "echo", "hello"],
390 | |         )
391 | |
392 | |         assert result.exit_code == 0
393 | |
394 | |         call_kwargs = mock_agent_pool.create_task.call_args[1]
395 | |         assert call_kwargs["description"] == "Execute: echo hello"
396 | |
397 | |     @patch("commands.multi_agent.AgentPool")
398 | |     @staticmethod
399 | |     def test_monitor_no_active_pool(mock_agent_pool_class: Mock, runner: CliRunner) -> None:  # noqa: ARG002  # noqa: ARG004
400 | |         """Test monitor command when no active pool exists."""
401 | |         # Simulate no existing pool directory
402 | |         with patch("pathlib.Path.exists", return_value=False):
403 | |             result = runner.invoke(
404 | |                 multi_agent_cli,
405 | |                 ["monitor", "--work-dir", "/tmp/test"],
406 | |             )
407 | |
408 | |             assert result.exit_code == 0
409 | |             assert "No active agent pool found" in result.output
    | |________________________________________________________________^ PLR0904
    |

tests/test_multi_agent_cli.py:111:9: ARG004 Unused static method argument: `mock_asyncio_run`
    |
109 |     @staticmethod
110 |     def test_start_agents_with_options(
111 |         mock_asyncio_run: Mock,  # noqa: ARG002
    |         ^^^^^^^^^^^^^^^^ ARG004
112 |         mock_agent_pool_class: Mock,
113 |         runner: CliRunner,
    |

tests/test_multi_agent_cli.py:135:9: ARG004 Unused static method argument: `mock_asyncio_run`
    |
133 |     @staticmethod
134 |     def test_start_agents_with_monitor(
135 |         mock_asyncio_run: Mock,  # noqa: ARG002
    |         ^^^^^^^^^^^^^^^^ ARG004
136 |         mock_run_monitor: Mock,
137 |         mock_agent_pool_class: Mock,
    |

tests/test_multi_agent_cli.py:399:37: ARG004 Unused static method argument: `mock_agent_pool_class`
    |
397 |     @patch("commands.multi_agent.AgentPool")
398 |     @staticmethod
399 |     def test_monitor_no_active_pool(mock_agent_pool_class: Mock, runner: CliRunner) -> None:  # noqa: ARG002  # noqa: ARG004
    |                                     ^^^^^^^^^^^^^^^^^^^^^ ARG004
400 |         """Test monitor command when no active pool exists."""
401 |         # Simulate no existing pool directory
    |

tests/test_progress_indicators.py:8:44: SyntaxError: Expected one or more symbol names after import
   |
 6 | from typing import Never
 7 | import pytest
 8 | from libs.core.progress_indicators import (
   |                                            ^
 9 | import time
10 |
11 | # Copyright (c) 2024 Yesman Claude Project
   |

tests/test_progress_indicators.py:18:1: SyntaxError: Unexpected indentation
   |
18 |     ProgressManager,
   | ^^^^
19 |     ProgressStyle,
20 |     bar_progress,
   |

tests/test_progress_indicators.py:18:1: E113 Unexpected indentation
   |
18 |     ProgressManager,
   | ^^^^ E113
19 |     ProgressStyle,
20 |     bar_progress,
   |

tests/test_progress_indicators.py:18:5: E303 Too many blank lines (3)
   |
18 |     ProgressManager,
   |     ^^^^^^^^^^^^^^^ E303
19 |     ProgressStyle,
20 |     bar_progress,
   |
   = help: Remove extraneous blank line(s)

tests/test_progress_indicators.py:27:1: SyntaxError: Expected a statement
   |
25 |     with_processing_progress,
26 |     with_startup_progress,
27 | )
   | ^
   |

tests/test_progress_indicators.py:27:2: SyntaxError: Expected a statement
   |
25 |     with_processing_progress,
26 |     with_startup_progress,
27 | )
   |  ^
28 |
29 |
30 | class TestProgressIndicators:
   |

tests/test_recovery_system.py:11:47: SyntaxError: Expected one or more symbol names after import
   |
 9 | import pytest
10 | from libs.multi_agent.agent_pool import AgentPool
11 | from libs.multi_agent.recovery_engine import (
   |                                               ^
12 | from libs.multi_agent.types import AgentState, Task, TaskStatus
13 |
14 | # Copyright (c) 2024 Yesman Claude Project
   |

tests/test_recovery_system.py:21:1: SyntaxError: Unexpected indentation
   |
21 |     OperationSnapshot,
   | ^^^^
22 |     OperationType,
23 |     RecoveryAction,
   |

tests/test_recovery_system.py:21:1: E113 Unexpected indentation
   |
21 |     OperationSnapshot,
   | ^^^^ E113
22 |     OperationType,
23 |     RecoveryAction,
   |

tests/test_recovery_system.py:21:5: E303 Too many blank lines (3)
   |
21 |     OperationSnapshot,
   |     ^^^^^^^^^^^^^^^^^ E303
22 |     OperationType,
23 |     RecoveryAction,
   |
   = help: Remove extraneous blank line(s)

tests/test_recovery_system.py:25:1: SyntaxError: Expected a statement
   |
23 |     RecoveryAction,
24 |     RecoveryEngine,
25 | )
   | ^
   |

tests/test_recovery_system.py:25:2: SyntaxError: Expected a statement
   |
23 |     RecoveryAction,
24 |     RecoveryEngine,
25 | )
   |  ^
26 |
27 |
28 | class TestRecoveryEngine:
   |

tests/test_renderers.py:9:39: SyntaxError: Expected one or more symbol names after import
   |
 7 | import psutil
 8 | import pytest
 9 | from libs.dashboard.renderers import (
   |                                       ^
10 | from libs.dashboard.renderers.widget_models import (
11 | from libs.dashboard.renderers.optimizations import cached_render
   |

tests/test_renderers.py:10:53: SyntaxError: Expected one or more symbol names after import
   |
 8 | import pytest
 9 | from libs.dashboard.renderers import (
10 | from libs.dashboard.renderers.widget_models import (
   |                                                     ^
11 | from libs.dashboard.renderers.optimizations import cached_render
   |

tests/test_renderers.py:25:1: SyntaxError: Unexpected indentation
   |
25 |     BatchRenderer,
   | ^^^^
26 |     LazyRenderer,
27 |     RenderCache,
   |

tests/test_renderers.py:25:1: E113 Unexpected indentation
   |
25 |     BatchRenderer,
   | ^^^^ E113
26 |     LazyRenderer,
27 |     RenderCache,
   |

tests/test_renderers.py:25:5: E303 Too many blank lines (3)
   |
25 |     BatchRenderer,
   |     ^^^^^^^^^^^^^ E303
26 |     LazyRenderer,
27 |     RenderCache,
   |
   = help: Remove extraneous blank line(s)

tests/test_renderers.py:36:1: SyntaxError: Expected a statement
   |
34 |     render_all_formats,
35 |     render_widget,
36 | )
   | ^
37 |     ActivityData,
38 |     ActivityEntry,
   |

tests/test_renderers.py:36:2: SyntaxError: Expected a statement
   |
34 |     render_all_formats,
35 |     render_widget,
36 | )
   |  ^
37 |     ActivityData,
38 |     ActivityEntry,
39 |     ActivityType,
   |

tests/test_renderers.py:37:1: SyntaxError: Unexpected indentation
   |
35 |     render_widget,
36 | )
37 |     ActivityData,
   | ^^^^
38 |     ActivityEntry,
39 |     ActivityType,
   |

tests/test_renderers.py:37:1: E113 Unexpected indentation
   |
35 |     render_widget,
36 | )
37 |     ActivityData,
   | ^^^^ E113
38 |     ActivityEntry,
39 |     ActivityType,
   |

tests/test_renderers.py:52:1: SyntaxError: Expected a statement
   |
50 |     StatusIndicatorData,
51 |     WindowData,
52 | )
   | ^
   |

tests/test_renderers.py:52:2: SyntaxError: Expected a statement
   |
50 |     StatusIndicatorData,
51 |     WindowData,
52 | )
   |  ^
53 |
54 |
55 | class TestRendererSystemIntegration:
   |

tests/test_semantic_analyzer.py:10:49: SyntaxError: Expected one or more symbol names after import
   |
 8 | from libs.multi_agent.branch_manager import BranchManager
 9 | from libs.multi_agent.conflict_resolution import ConflictSeverity, ResolutionStrategy
10 | from libs.multi_agent.semantic_analyzer import (
   |                                                 ^
11 | import os
12 | import sys as system
13 | from pathlib import Path
   |

tests/test_semantic_analyzer.py:15:1: E116 Unexpected indentation (comment)
   |
13 | from pathlib import Path
14 | from typing import List, Dict as DictType, Any
15 |         # Check import os
   | ^^^^^^^^ E116
16 |         # Check import sys as system
17 |         # Check from pathlib import Path
   |

tests/test_semantic_analyzer.py:16:1: E116 Unexpected indentation (comment)
   |
14 | from typing import List, Dict as DictType, Any
15 |         # Check import os
16 |         # Check import sys as system
   | ^^^^^^^^ E116
17 |         # Check from pathlib import Path
18 |         # Check from typing import Dict as DictType
   |

tests/test_semantic_analyzer.py:17:1: E116 Unexpected indentation (comment)
   |
15 |         # Check import os
16 |         # Check import sys as system
17 |         # Check from pathlib import Path
   | ^^^^^^^^ E116
18 |         # Check from typing import Dict as DictType
19 | import os
   |

tests/test_semantic_analyzer.py:18:1: E116 Unexpected indentation (comment)
   |
16 |         # Check import sys as system
17 |         # Check from pathlib import Path
18 |         # Check from typing import Dict as DictType
   | ^^^^^^^^ E116
19 | import os
20 | from typing import List
   |

tests/test_semantic_analyzer.py:29:1: SyntaxError: Unexpected indentation
   |
29 |     ClassDefinition,
   | ^^^^
30 |     FunctionSignature,
31 |     ImportInfo,
   |

tests/test_semantic_analyzer.py:29:1: E113 Unexpected indentation
   |
29 |     ClassDefinition,
   | ^^^^ E113
30 |     FunctionSignature,
31 |     ImportInfo,
   |

tests/test_semantic_analyzer.py:29:5: E303 Too many blank lines (3)
   |
29 |     ClassDefinition,
   |     ^^^^^^^^^^^^^^^ E303
30 |     FunctionSignature,
31 |     ImportInfo,
   |
   = help: Remove extraneous blank line(s)

tests/test_semantic_analyzer.py:38:1: SyntaxError: Expected a statement
   |
36 |     SemanticVisitor,
37 |     SymbolVisibility,
38 | )
   | ^
   |

tests/test_semantic_analyzer.py:38:2: SyntaxError: Expected a statement
   |
36 |     SemanticVisitor,
37 |     SymbolVisibility,
38 | )
   |  ^
39 |
40 |
41 | class TestFunctionSignature:
   |

tests/test_semantic_merger.py:8:51: SyntaxError: Expected one or more symbol names after import
   |
 6 | import pytest
 7 | from libs.multi_agent.branch_manager import BranchManager
 8 | from libs.multi_agent.conflict_resolution import (
   |                                                   ^
 9 | from libs.multi_agent.semantic_analyzer import (
10 | from libs.multi_agent.semantic_merger import (
11 | import os
   |

tests/test_semantic_merger.py:9:49: SyntaxError: Expected one or more symbol names after import
   |
 7 | from libs.multi_agent.branch_manager import BranchManager
 8 | from libs.multi_agent.conflict_resolution import (
 9 | from libs.multi_agent.semantic_analyzer import (
   |                                                 ^
10 | from libs.multi_agent.semantic_merger import (
11 | import os
12 | import os
   |

tests/test_semantic_merger.py:10:47: SyntaxError: Expected one or more symbol names after import
   |
 8 | from libs.multi_agent.conflict_resolution import (
 9 | from libs.multi_agent.semantic_analyzer import (
10 | from libs.multi_agent.semantic_merger import (
   |                                               ^
11 | import os
12 | import os
13 | from typing import List, Any
   |

tests/test_semantic_merger.py:22:1: SyntaxError: Unexpected indentation
   |
22 |     ConflictResolutionEngine,
   | ^^^^
23 |     ConflictSeverity,
24 | )
   |

tests/test_semantic_merger.py:22:1: E113 Unexpected indentation
   |
22 |     ConflictResolutionEngine,
   | ^^^^ E113
23 |     ConflictSeverity,
24 | )
   |

tests/test_semantic_merger.py:22:5: E303 Too many blank lines (3)
   |
22 |     ConflictResolutionEngine,
   |     ^^^^^^^^^^^^^^^^^^^^^^^^ E303
23 |     ConflictSeverity,
24 | )
   |
   = help: Remove extraneous blank line(s)

tests/test_semantic_merger.py:24:1: SyntaxError: Expected a statement
   |
22 |     ConflictResolutionEngine,
23 |     ConflictSeverity,
24 | )
   | ^
25 |     FunctionSignature,
26 |     SemanticAnalyzer,
   |

tests/test_semantic_merger.py:24:2: SyntaxError: Expected a statement
   |
22 |     ConflictResolutionEngine,
23 |     ConflictSeverity,
24 | )
   |  ^
25 |     FunctionSignature,
26 |     SemanticAnalyzer,
27 |     SemanticConflict,
   |

tests/test_semantic_merger.py:25:1: SyntaxError: Unexpected indentation
   |
23 |     ConflictSeverity,
24 | )
25 |     FunctionSignature,
   | ^^^^
26 |     SemanticAnalyzer,
27 |     SemanticConflict,
   |

tests/test_semantic_merger.py:25:1: E113 Unexpected indentation
   |
23 |     ConflictSeverity,
24 | )
25 |     FunctionSignature,
   | ^^^^ E113
26 |     SemanticAnalyzer,
27 |     SemanticConflict,
   |

tests/test_semantic_merger.py:30:1: SyntaxError: Expected a statement
   |
28 |     SemanticConflictType,
29 |     SemanticContext,
30 | )
   | ^
31 |     ConflictResolutionRule,
32 |     MergeResolution,
   |

tests/test_semantic_merger.py:30:2: SyntaxError: Expected a statement
   |
28 |     SemanticConflictType,
29 |     SemanticContext,
30 | )
   |  ^
31 |     ConflictResolutionRule,
32 |     MergeResolution,
33 |     MergeResult,
   |

tests/test_semantic_merger.py:31:1: SyntaxError: Unexpected indentation
   |
29 |     SemanticContext,
30 | )
31 |     ConflictResolutionRule,
   | ^^^^
32 |     MergeResolution,
33 |     MergeResult,
   |

tests/test_semantic_merger.py:31:1: E113 Unexpected indentation
   |
29 |     SemanticContext,
30 | )
31 |     ConflictResolutionRule,
   | ^^^^ E113
32 |     MergeResolution,
33 |     MergeResult,
   |

tests/test_semantic_merger.py:36:1: SyntaxError: Expected a statement
   |
34 |     MergeStrategy,
35 |     SemanticMerger,
36 | )
   | ^
   |

tests/test_semantic_merger.py:36:2: SyntaxError: Expected a statement
   |
34 |     MergeStrategy,
35 |     SemanticMerger,
36 | )
   |  ^
37 |
38 |
39 | class TestMergeResult:
   |

tests/test_task_analyzer.py:11:1: E116 Unexpected indentation (comment)
   |
 9 | from libs.multi_agent.task_analyzer import TaskAnalyzer, TaskDefinition
10 |
11 |         # Check standard library import
   | ^^^^^^^^ E116
12 |         # Check from import
13 |         # Check local import
   |

tests/test_task_analyzer.py:12:1: E116 Unexpected indentation (comment)
   |
11 |         # Check standard library import
12 |         # Check from import
   | ^^^^^^^^ E116
13 |         # Check local import
14 |         # Should find module_a (imports module_b) and module_c (imported by module_b)
   |

tests/test_task_analyzer.py:13:1: E116 Unexpected indentation (comment)
   |
11 |         # Check standard library import
12 |         # Check from import
13 |         # Check local import
   | ^^^^^^^^ E116
14 |         # Should find module_a (imports module_b) and module_c (imported by module_b)
   |

tests/test_task_analyzer.py:14:1: E116 Unexpected indentation (comment)
   |
12 |         # Check from import
13 |         # Check local import
14 |         # Should find module_a (imports module_b) and module_c (imported by module_b)
   | ^^^^^^^^ E116
15 |
16 | # Copyright (c) 2024 Yesman Claude Project
   |

tests/test_task_scheduler.py:133:1: PLR0904 Too many public methods (21 > 20)
    |
133 | / class TestTaskScheduler:
134 | |     """Test cases for TaskScheduler."""
135 | |
136 | |     @pytest.fixture
137 | |     @staticmethod
138 | |     def scheduler() -> TaskScheduler:
139 | |         """Create TaskScheduler instance.
140 | |
141 | |         Returns:
142 | |         TaskScheduler: Description of return value.
143 | |         """
144 | |         return TaskScheduler()
145 | |
146 | |     @pytest.fixture
147 | |     @staticmethod
148 | |     def sample_agent() -> Agent:
149 | |         """Create sample agent.
150 | |
151 | |         Returns:
152 | |         Agent: Description of return value.
153 | |         """
154 | |         return Agent(agent_id="test-agent")
155 | |
156 | |     @pytest.fixture
157 | |     @staticmethod
158 | |     def sample_task() -> Task:
159 | |         """Create sample task.
160 | |
161 | |         Returns:
162 | |         Task: Description of return value.
163 | |         """
164 | |         return Task(
165 | |             task_id="test-task",
166 | |             title="Test Task",
167 | |             description="A test task",
168 | |             command=["echo", "test"],
169 | |             working_directory="/tmp",
170 | |             priority=7,
171 | |             complexity=5,
172 | |         )
173 | |
174 | |     @staticmethod
175 | |     def test_init(scheduler: TaskScheduler) -> None:
176 | |         """Test TaskScheduler initialization."""
177 | |         assert scheduler.agent_capabilities == {}
178 | |         assert scheduler.priority_queue == []
179 | |         assert scheduler.task_history == {}
180 | |         assert "total_scheduled" in scheduler.scheduling_metrics
181 | |         assert scheduler.priority_weight == 0.4
182 | |         assert scheduler.learning_rate == 0.1
183 | |
184 | |     @staticmethod
185 | |     def test_register_agent(scheduler: TaskScheduler, sample_agent: Agent) -> None:
186 | |         """Test agent registration."""
187 | |         scheduler.register_agent(sample_agent)
188 | |
189 | |         assert sample_agent.agent_id in scheduler.agent_capabilities
190 | |         assert sample_agent.agent_id in scheduler.task_history
191 | |         assert scheduler.task_history[sample_agent.agent_id] == []
192 | |
193 | |         capability = scheduler.agent_capabilities[sample_agent.agent_id]
194 | |         assert capability.agent_id == sample_agent.agent_id
195 | |
196 | |     @staticmethod
197 | |     def test_register_agent_with_capability(scheduler: TaskScheduler, sample_agent: Agent) -> None:
198 | |         """Test agent registration with custom capability."""
199 | |         custom_capability = AgentCapability(
200 | |             agent_id=sample_agent.agent_id,
201 | |             processing_power=2.0,
202 | |             specializations=["python"],
203 | |         )
204 | |
205 | |         scheduler.register_agent(sample_agent, custom_capability)
206 | |
207 | |         capability = scheduler.agent_capabilities[sample_agent.agent_id]
208 | |         assert capability.processing_power == 2.0
209 | |         assert capability.specializations == ["python"]
210 | |
211 | |     @staticmethod
212 | |     def test_add_task(scheduler: TaskScheduler, sample_task: Task) -> None:
213 | |         """Test adding task to priority queue."""
214 | |         scheduler.add_task(sample_task)
215 | |
216 | |         assert len(scheduler.priority_queue) == 1
217 | |         priority_task = scheduler.priority_queue[0]
218 | |         assert priority_task.task == sample_task
219 | |         assert priority_task.priority_score > 0
220 | |
221 | |     @staticmethod
222 | |     def test_calculate_priority_score(scheduler: TaskScheduler) -> None:
223 | |         """Test priority score calculation."""
224 | |         task = Task(
225 | |             task_id="test",
226 | |             title="Test",
227 | |             command=["echo"],
228 | |             working_directory="/tmp",
229 | |             priority=8,  # High priority
230 | |             complexity=6,  # Medium-high complexity
231 | |             metadata={"blocks_tasks": 3},  # Blocks other tasks
232 | |         )
233 | |
234 | |         score = scheduler._calculate_priority_score(task)  # noqa: SLF001
235 | |         assert score > 0.0
236 | |         assert score <= 1.0  # Should be normalized
237 | |
238 | |     @staticmethod
239 | |     def test_get_next_task_for_agent(scheduler: TaskScheduler, sample_agent: Agent, sample_task: Task) -> None:
240 | |         """Test getting next task for agent."""
241 | |         scheduler.register_agent(sample_agent)
242 | |         scheduler.add_task(sample_task)
243 | |
244 | |         task = scheduler.get_next_task_for_agent(sample_agent)
245 | |
246 | |         assert task == sample_task
247 | |         assert len(scheduler.priority_queue) == 0  # Task should be removed
248 | |         assert scheduler.scheduling_metrics["total_scheduled"] == 1
249 | |
250 | |     @staticmethod
251 | |     def test_get_next_task_for_unregistered_agent(
252 | |         scheduler: TaskScheduler,
253 | |         sample_agent: Agent,
254 | |         sample_task: Task,
255 | |     ) -> None:
256 | |         """Test getting task for unregistered agent."""
257 | |         scheduler.add_task(sample_task)
258 | |
259 | |         task = scheduler.get_next_task_for_agent(sample_agent)
260 | |
261 | |         assert task == sample_task
262 | |         assert sample_agent.agent_id in scheduler.agent_capabilities  # Should auto-register
263 | |
264 | |     @staticmethod
265 | |     def test_get_next_task_empty_queue(scheduler: TaskScheduler, sample_agent: Agent) -> None:
266 | |         """Test getting task from empty queue."""
267 | |         scheduler.register_agent(sample_agent)
268 | |
269 | |         task = scheduler.get_next_task_for_agent(sample_agent)
270 | |         assert task is None
271 | |
272 | |     @staticmethod
273 | |     def test_get_optimal_task_assignment(scheduler: TaskScheduler) -> None:
274 | |         """Test optimal task assignment for multiple agents."""
275 | |         # Create agents
276 | |         agent1 = Agent(agent_id="agent-1")
277 | |         agent2 = Agent(agent_id="agent-2")
278 | |         agents = [agent1, agent2]
279 | |
280 | |         # Register agents with different capabilities
281 | |         scheduler.register_agent(
282 | |             agent1,
283 | |             AgentCapability(
284 | |                 agent_id="agent-1",
285 | |                 processing_power=1.5,
286 | |                 specializations=["python"],
287 | |             ),
288 | |         )
289 | |         scheduler.register_agent(
290 | |             agent2,
291 | |             AgentCapability(
292 | |                 agent_id="agent-2",
293 | |                 processing_power=1.0,
294 | |                 specializations=["javascript"],
295 | |             ),
296 | |         )
297 | |
298 | |         # Add tasks
299 | |         task1 = Task(
300 | |             task_id="task-1",
301 | |             title="Python Task",
302 | |             command=["python", "script.py"],
303 | |             working_directory="/tmp",
304 | |             priority=8,
305 | |             metadata={"tags": ["python"]},
306 | |         )
307 | |         task2 = Task(
308 | |             task_id="task-2",
309 | |             title="JS Task",
310 | |             command=["node", "script.js"],
311 | |             working_directory="/tmp",
312 | |             priority=6,
313 | |             metadata={"tags": ["javascript"]},
314 | |         )
315 | |
316 | |         scheduler.add_task(task1)
317 | |         scheduler.add_task(task2)
318 | |
319 | |         assignments = scheduler.get_optimal_task_assignment(agents)
320 | |
321 | |         assert len(assignments) == 2
322 | |
323 | |         # Check that assignments make sense (agent1 gets python task, agent2 gets js task)
324 | |         {agent.agent_id: task for agent, task in assignments}
325 | |         assert "task-1" in [task.task_id for agent, task in assignments]
326 | |         assert "task-2" in [task.task_id for agent, task in assignments]
327 | |
328 | |     @staticmethod
329 | |     def test_update_agent_performance(scheduler: TaskScheduler, sample_agent: Agent, sample_task: Task) -> None:
330 | |         """Test updating agent performance metrics."""
331 | |         scheduler.register_agent(sample_agent)
332 | |
333 | |         # Simulate successful task completion
334 | |         scheduler.update_agent_performance(
335 | |             sample_agent.agent_id,
336 | |             sample_task,
337 | |             success=True,
338 | |             execution_time=120.0,
339 | |         )
340 | |
341 | |         capability = scheduler.agent_capabilities[sample_agent.agent_id]
342 | |         assert capability.success_rate > 0.9  # Should improve
343 | |         assert capability.average_execution_time == 120.0
344 | |         assert len(scheduler.task_history[sample_agent.agent_id]) == 1
345 | |
346 | |     @staticmethod
347 | |     def test_update_agent_performance_failure(
348 | |         scheduler: TaskScheduler,
349 | |         sample_agent: Agent,
350 | |         sample_task: Task,
351 | |     ) -> None:
352 | |         """Test updating agent performance on failure.
353 | |
354 | |         Returns:
355 | |         None: Description of return value.
356 | |         """
357 | |         scheduler.register_agent(sample_agent)
358 | |
359 | |         # Simulate failed task
360 | |         scheduler.update_agent_performance(
361 | |             sample_agent.agent_id,
362 | |             sample_task,
363 | |             success=False,
364 | |             execution_time=60.0,
365 | |         )
366 | |
367 | |         capability = scheduler.agent_capabilities[sample_agent.agent_id]
368 | |         assert capability.success_rate < 1.0  # Should decrease
369 | |
370 | |     @staticmethod
371 | |     def test_estimate_task_time(scheduler: TaskScheduler) -> None:
372 | |         """Test task time estimation."""
373 | |         capability = AgentCapability(
374 | |             agent_id="test-agent",
375 | |             processing_power=2.0,
376 | |             specializations=["python"],
377 | |         )
378 | |
379 | |         task = Task(
380 | |             task_id="test",
381 | |             title="Test",
382 | |             command=["python", "test.py"],
383 | |             working_directory="/tmp",
384 | |             complexity=5,
385 | |             metadata={"tags": ["python"]},
386 | |         )
387 | |
388 | |         estimated_time = scheduler._estimate_task_time(task, capability)  # noqa: SLF001
389 | |         assert estimated_time > 0
390 | |
391 | |         # Should be faster due to high processing power and specialization
392 | |         base_time = scheduler._estimate_base_task_time(task)  # noqa: SLF001
393 | |         assert estimated_time < base_time
394 | |
395 | |     @staticmethod
396 | |     def test_estimate_base_task_time(scheduler: TaskScheduler) -> None:
397 | |         """Test base task time estimation."""
398 | |         # Test task
399 | |         test_task = Task(
400 | |             task_id="test",
401 | |             title="Test",
402 | |             command=["pytest", "tests/"],
403 | |             working_directory="/tmp",
404 | |             complexity=5,
405 | |         )
406 | |
407 | |         test_time = scheduler._estimate_base_task_time(test_task)  # noqa: SLF001
408 | |
409 | |         # Build task
410 | |         build_task = Task(
411 | |             task_id="build",
412 | |             title="Build",
413 | |             command=["make", "build"],
414 | |             working_directory="/tmp",
415 | |             complexity=5,
416 | |         )
417 | |
418 | |         build_time = scheduler._estimate_base_task_time(build_task)  # noqa: SLF001
419 | |
420 | |         # Lint task
421 | |         lint_task = Task(
422 | |             task_id="lint",
423 | |             title="Lint",
424 | |             command=["ruff", "check"],
425 | |             working_directory="/tmp",
426 | |             complexity=5,
427 | |         )
428 | |
429 | |         lint_time = scheduler._estimate_base_task_time(lint_task)  # noqa: SLF001
430 | |
431 | |         # Test tasks should take longer than lint tasks
432 | |         assert test_time > lint_time
433 | |         assert build_time > lint_time
434 | |
435 | |     @staticmethod
436 | |     def test_update_agent_load(scheduler: TaskScheduler, sample_agent: Agent) -> None:
437 | |         """Test updating agent load."""
438 | |         scheduler.register_agent(sample_agent)
439 | |
440 | |         scheduler.update_agent_load(sample_agent.agent_id, 0.7)
441 | |
442 | |         capability = scheduler.agent_capabilities[sample_agent.agent_id]
443 | |         assert capability.current_load == 0.7
444 | |
445 | |         # Test bounds
446 | |         scheduler.update_agent_load(sample_agent.agent_id, 1.5)  # Over limit
447 | |         assert capability.current_load == 1.0  # Should be capped
448 | |
449 | |         scheduler.update_agent_load(sample_agent.agent_id, -0.5)  # Under limit
450 | |         assert capability.current_load == 0.0  # Should be floored
451 | |
452 | |     @staticmethod
453 | |     def test_get_scheduling_metrics(scheduler: TaskScheduler) -> None:
454 | |         """Test getting scheduling metrics."""
455 | |         metrics = scheduler.get_scheduling_metrics()
456 | |
457 | |         assert "total_scheduled" in metrics
458 | |         assert "load_balancing_score" in metrics
459 | |         assert "efficiency_score" in metrics
460 | |         assert "queue_size" in metrics
461 | |
462 | |         # Add some agents and test again
463 | |         agent1 = Agent(agent_id="agent-1")
464 | |         agent2 = Agent(agent_id="agent-2")
465 | |
466 | |         scheduler.register_agent(agent1)
467 | |         scheduler.register_agent(agent2)
468 | |
469 | |         scheduler.update_agent_load("agent-1", 0.3)
470 | |         scheduler.update_agent_load("agent-2", 0.7)
471 | |
472 | |         metrics = scheduler.get_scheduling_metrics()
473 | |         assert metrics["active_agents"] == 1  # Only agent-2 has load > 0
474 | |         assert 0.0 <= metrics["load_balancing_score"] <= 1.0
475 | |         assert 0.0 <= metrics["efficiency_score"] <= 2.0
476 | |
477 | |     @staticmethod
478 | |     def test_rebalance_tasks(scheduler: TaskScheduler) -> None:
479 | |         """Test task rebalancing."""
480 | |         # Create agents with different loads
481 | |         agent1 = Agent(agent_id="agent-1")
482 | |         agent2 = Agent(agent_id="agent-2")
483 | |         agent3 = Agent(agent_id="agent-3")
484 | |
485 | |         scheduler.register_agent(agent1)
486 | |         scheduler.register_agent(agent2)
487 | |         scheduler.register_agent(agent3)
488 | |
489 | |         # Set loads: agent1 overloaded, agent2 underloaded, agent3 normal
490 | |         scheduler.update_agent_load("agent-1", 0.9)  # Overloaded
491 | |         scheduler.update_agent_load("agent-2", 0.2)  # Underloaded
492 | |         scheduler.update_agent_load("agent-3", 0.5)  # Normal
493 | |
494 | |         rebalancing_actions = scheduler.rebalance_tasks()
495 | |
496 | |         assert len(rebalancing_actions) > 0
497 | |         assert any("agent-1" in action for action in rebalancing_actions)
498 | |         assert any("agent-2" in action for action in rebalancing_actions)
499 | |
500 | |     @staticmethod
501 | |     def test_task_dependency_check(scheduler: TaskScheduler) -> None:
502 | |         """Test task dependency checking."""
503 | |         # Task with no dependencies
504 | |         task_no_deps = Task(
505 | |             task_id="no-deps",
506 | |             title="No Dependencies",
507 | |             command=["echo"],
508 | |             working_directory="/tmp",
509 | |         )
510 | |
511 | |         assert scheduler._are_dependencies_met(task_no_deps) is True  # noqa: SLF001
512 | |
513 | |         # Task with dependencies (should fail for now)
514 | |         task_with_deps = Task(
515 | |             task_id="with-deps",
516 | |             title="With Dependencies",
517 | |             command=["echo"],
518 | |             working_directory="/tmp",
519 | |             dependencies=["other-task"],
520 | |         )
521 | |
522 | |         assert scheduler._are_dependencies_met(task_with_deps) is False  # noqa: SLF001
523 | |
524 | |     @staticmethod
525 | |     def test_task_history_limit(scheduler: TaskScheduler, sample_agent: Agent) -> None:
526 | |         """Test task history size limiting."""
527 | |         scheduler.register_agent(sample_agent)
528 | |
529 | |         # Add more than 100 tasks to history
530 | |         for i in range(105):
531 | |             task = Task(
532 | |                 task_id=f"task-{i}",
533 | |                 title=f"Task {i}",
534 | |                 command=["echo"],
535 | |                 working_directory="/tmp",
536 | |             )
537 | |             scheduler.update_agent_performance(
538 | |                 sample_agent.agent_id,
539 | |                 task,
540 | |                 success=True,
541 | |                 execution_time=60.0,
542 | |             )
543 | |
544 | |         # History should be limited to 100
545 | |         history = scheduler.task_history[sample_agent.agent_id]
546 | |         assert len(history) == 100
547 | |         assert history[0].task_id == "task-5"  # Should have removed first 5
548 | |         assert history[-1].task_id == "task-104"
    | |________________________________________________^ PLR0904
    |

tests/test_work_environment.py:4:8: S404 `subprocess` module is possibly insecure
  |
3 | import os
4 | import subprocess
  |        ^^^^^^^^^^ S404
5 | from pathlib import Path
6 | from unittest.mock import MagicMock, patch
  |

tests/unit/core/test_base_batch_processor.py:31:34: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `**kwargs`
   |
29 |     """Test implementation of batch processor."""
30 |
31 |     def __init__(self, **kwargs: Any) -> None:
   |                                  ^^^ ANN401
32 |         super().__init__(**kwargs)
33 |         self.processed_batches: list[TestBatch] = []
   |

tests/unit/core/test_error_handling.py:49:26: PLC1901 `result == ""` can be simplified to `not result` as an empty string is falsey
   |
47 |         # Should handle None gracefully
48 |         result = collector.process_content(None)
49 |         assert result == "" or result is None
   |                          ^^ PLC1901
50 |
51 |     @staticmethod
   |

tests/unit/dashboard/renderers/test_base_renderer.py:5:53: SyntaxError: Expected one or more symbol names after import
  |
3 | from datetime import datetime
4 | import pytest
5 | from libs.dashboard.renderers.base_renderer import (
  |                                                     ^
6 | from libs.dashboard.renderers.registry import RendererRegistry
7 |
8 | # Licensed under the MIT License
  |

tests/unit/dashboard/renderers/test_base_renderer.py:15:1: SyntaxError: Unexpected indentation
   |
15 |     BaseRenderer,
   | ^^^^
16 |     RenderFormat,
17 |     ThemeColor,
   |

tests/unit/dashboard/renderers/test_base_renderer.py:15:1: E113 Unexpected indentation
   |
15 |     BaseRenderer,
   | ^^^^ E113
16 |     RenderFormat,
17 |     ThemeColor,
   |

tests/unit/dashboard/renderers/test_base_renderer.py:15:5: E303 Too many blank lines (4)
   |
15 |     BaseRenderer,
   |     ^^^^^^^^^^^^ E303
16 |     RenderFormat,
17 |     ThemeColor,
   |
   = help: Remove extraneous blank line(s)

tests/unit/dashboard/renderers/test_base_renderer.py:19:1: SyntaxError: Expected a statement
   |
17 |     ThemeColor,
18 |     WidgetType,
19 | )
   | ^
   |

tests/unit/dashboard/renderers/test_base_renderer.py:19:2: SyntaxError: Expected a statement
   |
17 |     ThemeColor,
18 |     WidgetType,
19 | )
   |  ^
20 |
21 |
22 | class TestRenderer(BaseRenderer):
   |

tests/unit/dashboard/renderers/test_base_renderer.py:26:42: SyntaxError: Expected ')', found '['
   |
25 |     @staticmethod
26 |     def render_widget(data: options, dict[str] | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
   |                                          ^
27 |         return f"<widget type='{widget_type.value}' data='{data}' />"
   |

tests/unit/dashboard/renderers/test_base_renderer.py:26:54: E251 Unexpected spaces around keyword / parameter equals
   |
25 |     @staticmethod
26 |     def render_widget(data: options, dict[str] | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
   |                                                      ^ E251
27 |         return f"<widget type='{widget_type.value}' data='{data}' />"
   |
   = help: Remove whitespace

tests/unit/dashboard/renderers/test_base_renderer.py:26:56: E251 Unexpected spaces around keyword / parameter equals
   |
25 |     @staticmethod
26 |     def render_widget(data: options, dict[str] | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
   |                                                        ^ E251
27 |         return f"<widget type='{widget_type.value}' data='{data}' />"
   |
   = help: Remove whitespace

tests/unit/dashboard/renderers/test_base_renderer.py:26:61: SyntaxError: Expected newline, found ')'
   |
25 |     @staticmethod
26 |     def render_widget(data: options, dict[str] | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
   |                                                             ^
27 |         return f"<widget type='{widget_type.value}' data='{data}' />"
   |

tests/unit/dashboard/renderers/test_base_renderer.py:26:63: SyntaxError: Expected a statement
   |
25 |     @staticmethod
26 |     def render_widget(data: options, dict[str] | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
   |                                                               ^^
27 |         return f"<widget type='{widget_type.value}' data='{data}' />"
   |

tests/unit/dashboard/renderers/test_base_renderer.py:26:102: SyntaxError: Expected an expression
   |
25 |     @staticmethod
26 |     def render_widget(data: options, dict[str] | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
   |                                                                                                      ^
27 |         return f"<widget type='{widget_type.value}' data='{data}' />"
28 |
29 |     def render_layout(self, widgets: list[dict[str]], layout_config: dict[str] | None = None) -> str:  # noqa: ARG002
   |

tests/unit/dashboard/renderers/test_base_renderer.py:27:1: SyntaxError: Unexpected indentation
   |
25 |     @staticmethod
26 |     def render_widget(data: options, dict[str] | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
27 |         return f"<widget type='{widget_type.value}' data='{data}' />"
   | ^^^^^^^^
28 |
29 |     def render_layout(self, widgets: list[dict[str]], layout_config: dict[str] | None = None) -> str:  # noqa: ARG002
   |

tests/unit/dashboard/renderers/test_base_renderer.py:41:57: SyntaxError: Expected ')', found '['
   |
40 |     @staticmethod
41 |     def render_container(content: container_config, dict[str] | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
   |                                                         ^
42 |         return f"<container>{content}</container>"
   |

tests/unit/dashboard/renderers/test_base_renderer.py:41:69: E251 Unexpected spaces around keyword / parameter equals
   |
40 |     @staticmethod
41 |     def render_container(content: container_config, dict[str] | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
   |                                                                     ^ E251
42 |         return f"<container>{content}</container>"
   |
   = help: Remove whitespace

tests/unit/dashboard/renderers/test_base_renderer.py:41:71: E251 Unexpected spaces around keyword / parameter equals
   |
40 |     @staticmethod
41 |     def render_container(content: container_config, dict[str] | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
   |                                                                       ^ E251
42 |         return f"<container>{content}</container>"
   |
   = help: Remove whitespace

tests/unit/dashboard/renderers/test_base_renderer.py:41:76: SyntaxError: Expected newline, found ')'
   |
40 |     @staticmethod
41 |     def render_container(content: container_config, dict[str] | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
   |                                                                            ^
42 |         return f"<container>{content}</container>"
   |

tests/unit/dashboard/renderers/test_base_renderer.py:41:78: SyntaxError: Expected a statement
   |
40 |     @staticmethod
41 |     def render_container(content: container_config, dict[str] | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
   |                                                                              ^^
42 |         return f"<container>{content}</container>"
   |

tests/unit/dashboard/renderers/test_base_renderer.py:41:117: SyntaxError: Expected an expression
   |
40 |     @staticmethod
41 |     def render_container(content: container_config, dict[str] | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
   |                                                                                                                     ^
42 |         return f"<container>{content}</container>"
   |

tests/unit/dashboard/renderers/test_base_renderer.py:42:1: SyntaxError: Unexpected indentation
   |
40 |     @staticmethod
41 |     def render_container(content: container_config, dict[str] | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
42 |         return f"<container>{content}</container>"
   | ^^^^^^^^
   |

tests/unit/dashboard/renderers/test_base_renderer.py:45:1: SyntaxError: Expected a statement
   |
45 | class TestBaseRenderer:
   | ^
46 |     """Test cases for BaseRenderer abstract class."""
   |

tests/unit/dashboard/renderers/test_optimizations.py:42:60: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `data`
   |
40 |         self.last_params = None
41 |
42 |     def render_widget(self, widget_type: WidgetType, data: Any, options: dict | None = None) -> str:
   |                                                            ^^^ ANN401
43 |         self.render_count += 1
44 |         self.last_params = (widget_type, data, options)
   |

tests/unit/dashboard/renderers/test_optimizations.py:52:38: ARG004 Unused static method argument: `layout_config`
   |
51 |     @staticmethod
52 |     def render_layout(widgets: list, layout_config: dict | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
   |                                      ^^^^^^^^^^^^^ ARG004
53 |         return f"mock-layout-{len(widgets)}"
   |

tests/unit/dashboard/renderers/test_optimizations.py:56:26: ARG004 Unused static method argument: `content`
   |
55 |     @staticmethod
56 |     def render_container(content: str, container_config: dict | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
   |                          ^^^^^^^ ARG004
57 |         return "mock-container"
   |

tests/unit/dashboard/renderers/test_optimizations.py:56:40: ARG004 Unused static method argument: `container_config`
   |
55 |     @staticmethod
56 |     def render_container(content: str, container_config: dict | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
   |                                        ^^^^^^^^^^^^^^^^ ARG004
57 |         return "mock-container"
   |

tests/unit/dashboard/renderers/test_optimizations.py:60:26: ARG004 Unused static method argument: `feature`
   |
59 |     @staticmethod
60 |     def supports_feature(feature: str) -> bool:  # noqa: ARG002  # noqa: ARG004
   |                          ^^^^^^^ ARG004
61 |         return True
   |

tests/unit/dashboard/renderers/test_optimizations.py:256:25: ARG001 Unused function argument: `widget_type`
    |
255 |         @cached_render(self.cache)
256 |         def mock_render(widget_type: WidgetType, data: Any, options: dict | None = None) -> str:
    |                         ^^^^^^^^^^^ ARG001
257 |             self.render_count += 1
258 |             return f"render-{self.render_count}"
    |

tests/unit/dashboard/renderers/test_optimizations.py:256:50: ARG001 Unused function argument: `data`
    |
255 |         @cached_render(self.cache)
256 |         def mock_render(widget_type: WidgetType, data: Any, options: dict | None = None) -> str:
    |                                                  ^^^^ ARG001
257 |             self.render_count += 1
258 |             return f"render-{self.render_count}"
    |

tests/unit/dashboard/renderers/test_optimizations.py:256:56: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `data`
    |
255 |         @cached_render(self.cache)
256 |         def mock_render(widget_type: WidgetType, data: Any, options: dict | None = None) -> str:
    |                                                        ^^^ ANN401
257 |             self.render_count += 1
258 |             return f"render-{self.render_count}"
    |

tests/unit/dashboard/renderers/test_optimizations.py:256:61: ARG001 Unused function argument: `options`
    |
255 |         @cached_render(self.cache)
256 |         def mock_render(widget_type: WidgetType, data: Any, options: dict | None = None) -> str:
    |                                                             ^^^^^^^ ARG001
257 |             self.render_count += 1
258 |             return f"render-{self.render_count}"
    |

tests/unit/dashboard/renderers/test_optimizations.py:278:32: ARG001 Unused function argument: `self`
    |
277 |         @cached_layout(self.cache)
278 |         def mock_render_layout(self: MockRenderer, widgets: list, layout_config: dict | None = None) -> str:
    |                                ^^^^ ARG001
279 |             return f"layout-{len(widgets)}-{id(layout_config)}"
    |

tests/unit/dashboard/renderers/test_renderer_factory.py:47:60: ANN401 Dynamically typed expressions (typing.Any) are disallowed in `data`
   |
45 |         self.render_count = 0
46 |
47 |     def render_widget(self, widget_type: WidgetType, data: Any, options: dict[str, Any] | None = None) -> str:  # noqa: ARG002
   |                                                            ^^^ ANN401
48 |         self.render_count += 1
49 |         if self.should_fail:
   |

tests/unit/dashboard/renderers/test_renderer_factory.py:51:19: TRY002 Create your own exception
   |
49 |         if self.should_fail:
50 |             msg = "Mock render failure"
51 |             raise Exception(msg)
   |                   ^^^^^^^^^^^^^^ TRY002
52 |         return f"mock-{widget_type.value}-{self.render_count}"
   |

tests/unit/dashboard/renderers/test_renderer_factory.py:55:49: ARG004 Unused static method argument: `layout_config`
   |
54 |     @staticmethod
55 |     def render_layout(widgets: list[dict[str]], layout_config: dict[str] | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
   |                                                 ^^^^^^^^^^^^^ ARG004
56 |         return f"mock-layout-{len(widgets)}"
   |

tests/unit/dashboard/renderers/test_renderer_factory.py:59:40: ARG004 Unused static method argument: `container_config`
   |
58 |     @staticmethod
59 |     def render_container(content: str, container_config: dict[str, Any] | None = None) -> str:  # noqa: ARG002  # noqa: ARG004
   |                                        ^^^^^^^^^^^^^^^^ ARG004
60 |         return f"mock-container-{type(content).__name__}"
   |

tests/unit/dashboard/renderers/test_renderer_factory.py:509:17: TRY300 Consider moving this statement to an `else` block
    |
507 |                     RenderFormat.TUI,
508 |                 )
509 |                 return i, result
    |                 ^^^^^^^^^^^^^^^^ TRY300
510 |             except Exception as e:
511 |                 return i, str(e)
    |

tests/unit/dashboard/renderers/test_tauri_renderer.py:31:1: PLR0904 Too many public methods (21 > 20)
    |
 31 | / class TestTauriRenderer:
 32 | |     """Test cases for Tauri renderer."""
 33 | |
 34 | |     def setup_method(self) -> None:
 35 | |         """Setup test renderer."""
 36 | |         self.renderer = TauriRenderer()
 37 | |
 38 | |     def test_renderer_initialization(self) -> None:
 39 | |         """Test Tauri renderer initialization."""
 40 | |         assert self.renderer.format_type == RenderFormat.TAURI
 41 | |         assert self.renderer.widget_id_counter == 0
 42 | |         assert "success" in self.renderer._style_classes  # noqa: SLF001
 43 | |         assert isinstance(self.renderer._style_classes["success"], dict)  # noqa: SLF001
 44 | |
 45 | |     def test_widget_id_generation(self) -> None:
 46 | |         """Test unique widget ID generation."""
 47 | |         id1 = self.renderer._generate_widget_id()  # noqa: SLF001
 48 | |         id2 = self.renderer._generate_widget_id()  # noqa: SLF001
 49 | |
 50 | |         assert id1 != id2
 51 | |         assert id1.startswith("tauri-widget-1-")
 52 | |         assert id2.startswith("tauri-widget-2-")
 53 | |         assert self.renderer.widget_id_counter == 2
 54 | |
 55 | |     def test_default_style(self) -> None:
 56 | |         """Test default style generation."""
 57 | |         style = self.renderer._get_default_style()  # noqa: SLF001
 58 | |
 59 | |         assert isinstance(style, dict)
 60 | |         assert style["background"] == "surface"
 61 | |         assert style["border"] is True
 62 | |         assert style["shadow"] is True
 63 | |         assert style["rounded"] is True
 64 | |
 65 | |     def test_supports_feature(self) -> None:
 66 | |         """Test feature support checking."""
 67 | |         assert self.renderer.supports_feature("json") is True
 68 | |         assert self.renderer.supports_feature("charts") is True
 69 | |         assert self.renderer.supports_feature("interactive") is True
 70 | |         assert self.renderer.supports_feature("actions") is True
 71 | |         assert self.renderer.supports_feature("animations") is True
 72 | |         assert self.renderer.supports_feature("serializable") is True
 73 | |         assert self.renderer.supports_feature("nonexistent") is False
 74 | |
 75 | |     def test_render_session_browser(self) -> None:
 76 | |         """Test session browser rendering."""
 77 | |         now = datetime.now(UTC)
 78 | |         window = WindowData(id="1", name="test-window", active=True, panes=2)
 79 | |
 80 | |         session = SessionData(
 81 | |             name="test-session",
 82 | |             id="session-123",
 83 | |             status=SessionStatus.ACTIVE,
 84 | |             created_at=now,
 85 | |             windows=[window],
 86 | |             panes=2,
 87 | |             claude_active=True,
 88 | |         )
 89 | |
 90 | |         result = self.renderer.render_widget(
 91 | |             WidgetType.SESSION_BROWSER,
 92 | |             [session],
 93 | |             {"view_mode": "table"},
 94 | |         )
 95 | |
 96 | |         assert isinstance(result, dict)
 97 | |         assert result["type"] == "widget"
 98 | |         assert result["widget_type"] == "session_browser"
 99 | |         assert "id" in result
100 | |         assert "data" in result
101 | |         assert "metadata" in result
102 | |         assert "style" in result
103 | |         assert "actions" in result
104 | |
105 | |         # Check data structure
106 | |         assert "sessions" in result["data"]
107 | |         assert len(result["data"]["sessions"]) == 1
108 | |         assert result["data"]["sessions"][0]["name"] == "test-session"
109 | |         assert result["data"]["view_mode"] == "table"
110 | |         assert result["data"]["total_count"] == 1
111 | |         assert result["data"]["active_count"] == 1
112 | |         assert result["data"]["claude_count"] == 1
113 | |
114 | |         # Check actions
115 | |         assert len(result["actions"]) >= 3
116 | |         action_ids = [action["id"] for action in result["actions"]]
117 | |         assert "refresh" in action_ids
118 | |         assert "new_session" in action_ids
119 | |
120 | |     def test_render_health_meter(self) -> None:
121 | |         """Test health meter rendering."""
122 | |         now = datetime.now(UTC)
123 | |         category = HealthCategoryData(
124 | |             category="build",
125 | |             score=85,
126 | |             level=HealthLevel.GOOD,
127 | |             message="Build successful",
128 | |         )
129 | |
130 | |         health = HealthData(
131 | |             overall_score=85,
132 | |             overall_level=HealthLevel.GOOD,
133 | |             categories=[category],
134 | |             last_updated=now,
135 | |         )
136 | |
137 | |         result = self.renderer.render_widget(
138 | |             WidgetType.HEALTH_METER,
139 | |             health,
140 | |             {},
141 | |         )
142 | |
143 | |         assert isinstance(result, dict)
144 | |         assert result["widget_type"] == "health_meter"
145 | |         assert "data" in result
146 | |         assert "chart_data" in result
147 | |
148 | |         # Check data structure
149 | |         assert result["data"]["overall_score"] == 85
150 | |         # The enum might return different format, just check it's a string representation
151 | |         assert isinstance(result["data"]["overall_level"], str)
152 | |         assert "overall_emoji" in result["data"]
153 | |         assert len(result["data"]["categories"]) == 1
154 | |         assert result["data"]["categories"][0]["category"] == "build"
155 | |         assert result["data"]["categories"][0]["score"] == 85
156 | |
157 | |         # Check chart data
158 | |         assert result["chart_data"]["type"] == "radar"
159 | |         assert "labels" in result["chart_data"]
160 | |         assert "datasets" in result["chart_data"]
161 | |         assert len(result["chart_data"]["labels"]) == 1
162 | |         assert result["chart_data"]["labels"][0] == "Build"
163 | |
164 | |     def test_render_activity_heatmap(self) -> None:
165 | |         """Test activity heatmap rendering."""
166 | |         now = datetime.now(UTC)
167 | |         entry = ActivityEntry(
168 | |             timestamp=now,
169 | |             activity_type=ActivityType.FILE_CREATED,
170 | |             description="Created new file",
171 | |         )
172 | |
173 | |         activity = ActivityData(
174 | |             entries=[entry],
175 | |             total_activities=10,
176 | |             active_days=5,
177 | |             activity_rate=75.0,
178 | |             current_streak=3,
179 | |             longest_streak=7,
180 | |             avg_per_day=2.0,
181 | |         )
182 | |
183 | |         result = self.renderer.render_widget(
184 | |             WidgetType.ACTIVITY_HEATMAP,
185 | |             activity,
186 | |             {},
187 | |         )
188 | |
189 | |         assert isinstance(result, dict)
190 | |         assert result["widget_type"] == "activity_heatmap"
191 | |         assert "data" in result
192 | |
193 | |         # Check data structure
194 | |         assert result["data"]["total_activities"] == 10
195 | |         assert result["data"]["active_days"] == 5
196 | |         assert result["data"]["activity_rate"] == 75.0
197 | |         assert "heatmap_data" in result["data"]
198 | |         assert "activity_levels" in result["data"]
199 | |
200 | |         # Check heatmap data structure
201 | |         assert isinstance(result["data"]["heatmap_data"], list)
202 | |         if result["data"]["heatmap_data"]:
203 | |             heatmap_entry = result["data"]["heatmap_data"][0]
204 | |             assert "date" in heatmap_entry
205 | |             assert "value" in heatmap_entry
206 | |             assert "level" in heatmap_entry
207 | |
208 | |         # Check activity levels
209 | |         assert len(result["data"]["activity_levels"]) == 5
210 | |         for level in result["data"]["activity_levels"]:
211 | |             assert "level" in level
212 | |             assert "color" in level
213 | |             assert "label" in level
214 | |
215 | |     def test_render_progress_tracker(self) -> None:
216 | |         """Test progress tracker rendering."""
217 | |         now = datetime.now(UTC)
218 | |
219 | |         progress = ProgressData(
220 | |             phase=ProgressPhase.IMPLEMENTING,
221 | |             phase_progress=75.0,
222 | |             overall_progress=50.0,
223 | |             files_created=5,
224 | |             files_modified=10,
225 | |             commands_executed=20,
226 | |             commands_succeeded=18,
227 | |             commands_failed=2,
228 | |             start_time=now,
229 | |             todos_identified=10,
230 | |             todos_completed=7,
231 | |         )
232 | |
233 | |         result = self.renderer.render_widget(
234 | |             WidgetType.PROGRESS_TRACKER,
235 | |             progress,
236 | |             {},
237 | |         )
238 | |
239 | |         assert isinstance(result, dict)
240 | |         assert result["widget_type"] == "progress_tracker"
241 | |         assert "data" in result
242 | |         assert "chart_data" in result
243 | |
244 | |         # Check data structure
245 | |         assert result["data"]["phase"] == "implementing"
246 | |         assert "phase_emoji" in result["data"]
247 | |         assert result["data"]["phase_progress"] == 75.0
248 | |         assert result["data"]["overall_progress"] == 50.0
249 | |         assert result["data"]["files_created"] == 5
250 | |         assert result["data"]["commands_executed"] == 20
251 | |         assert result["data"]["success_rate"] == 90.0  # 18/20 * 100
252 | |         assert result["data"]["completion_rate"] == 70.0  # 7/10 * 100
253 | |
254 | |         # Check chart data (doughnut chart)
255 | |         assert result["chart_data"]["type"] == "doughnut"
256 | |         assert result["chart_data"]["labels"] == ["Completed", "Remaining"]
257 | |         assert result["chart_data"]["datasets"][0]["data"] == [50.0, 50.0]
258 | |
259 | |     def test_render_log_viewer(self) -> None:
260 | |         """Test log viewer rendering."""
261 | |         logs = [
262 | |             {
263 | |                 "timestamp": "2023-01-01T10:00:00",
264 | |                 "level": "INFO",
265 | |                 "message": "Application started",
266 | |                 "source": "main",
267 | |             },
268 | |             {
269 | |                 "timestamp": "2023-01-01T10:01:00",
270 | |                 "level": "WARNING",
271 | |                 "message": "Configuration missing",
272 | |             },
273 | |             {
274 | |                 "timestamp": "2023-01-01T10:02:00",
275 | |                 "level": "ERROR",
276 | |                 "message": "Connection failed",
277 | |             },
278 | |         ]
279 | |
280 | |         result = self.renderer.render_widget(
281 | |             WidgetType.LOG_VIEWER,
282 | |             {"logs": logs},
283 | |             {"max_lines": 5},
284 | |         )
285 | |
286 | |         assert isinstance(result, dict)
287 | |         assert result["widget_type"] == "log_viewer"
288 | |         assert "data" in result
289 | |         assert "actions" in result
290 | |
291 | |         # Check data structure
292 | |         assert len(result["data"]["logs"]) == 3
293 | |         assert result["data"]["total_count"] == 3
294 | |         assert result["data"]["displayed_count"] == 3
295 | |         assert result["data"]["max_lines"] == 5
296 | |         assert "level_counts" in result["data"]
297 | |
298 | |         # Check processed log structure
299 | |         log_entry = result["data"]["logs"][0]
300 | |         assert "timestamp" in log_entry
301 | |         assert "level" in log_entry
302 | |         assert "message" in log_entry
303 | |         assert "color" in log_entry
304 | |
305 | |         # Check level counts
306 | |         level_counts = result["data"]["level_counts"]
307 | |         assert level_counts["INFO"] == 1
308 | |         assert level_counts["WARNING"] == 1
309 | |         assert level_counts["ERROR"] == 1
310 | |
311 | |         # Check actions
312 | |         action_ids = [action["id"] for action in result["actions"]]
313 | |         assert "refresh" in action_ids
314 | |         assert "clear" in action_ids
315 | |         assert "filter" in action_ids
316 | |
317 | |     def test_render_metric_card(self) -> None:
318 | |         """Test metric card rendering."""
319 | |         metric = MetricCardData(
320 | |             title="CPU Usage",
321 | |             value=75.5,
322 | |             suffix="%",
323 | |             trend=5.2,
324 | |             color="warning",
325 | |             icon="ğŸ–¥ï¸",
326 | |             comparison="vs last hour",
327 | |         )
328 | |
329 | |         result = self.renderer.render_widget(
330 | |             WidgetType.METRIC_CARD,
331 | |             metric,
332 | |             {},
333 | |         )
334 | |
335 | |         assert isinstance(result, dict)
336 | |         assert result["widget_type"] == "metric_card"
337 | |         assert "data" in result
338 | |         assert "chart_data" in result
339 | |
340 | |         # Check data structure
341 | |         assert result["data"]["title"] == "CPU Usage"
342 | |         assert result["data"]["value"] == 75.5
343 | |         assert "75.50%" in result["data"]["formatted_value"]
344 | |         assert result["data"]["suffix"] == "%"
345 | |         assert result["data"]["trend"] == 5.2
346 | |         assert result["data"]["trend_direction"] == "up"
347 | |         assert result["data"]["color"] == "warning"
348 | |         assert result["data"]["icon"] == "ğŸ–¥ï¸"
349 | |
350 | |         # Check chart data (sparkline)
351 | |         assert result["chart_data"]["type"] == "line"
352 | |         assert result["chart_data"]["labels"] == ["Previous", "Current"]
353 | |         assert result["chart_data"]["datasets"][0]["data"] == [0, 5.2]
354 | |
355 | |         # Check style update based on color
356 | |         assert "warning" in str(result["style"])
357 | |
358 | |     def test_render_status_indicator(self) -> None:
359 | |         """Test status indicator rendering."""
360 | |         status = StatusIndicatorData(
361 | |             status="running",
362 | |             label="Service Active",
363 | |             color="success",
364 | |             icon="âœ…",
365 | |             pulse=True,
366 | |         )
367 | |
368 | |         result = self.renderer.render_widget(
369 | |             WidgetType.STATUS_INDICATOR,
370 | |             status,
371 | |             {},
372 | |         )
373 | |
374 | |         assert isinstance(result, dict)
375 | |         assert result["widget_type"] == "status_indicator"
376 | |         assert "data" in result
377 | |         assert "animation" in result
378 | |
379 | |         # Check data structure
380 | |         assert result["data"]["status"] == "running"
381 | |         assert result["data"]["label"] == "Service Active"
382 | |         assert result["data"]["color"] == "success"
383 | |         assert result["data"]["icon"] == "âœ…"
384 | |         assert result["data"]["pulse"] is True
385 | |         assert "timestamp" in result["data"]
386 | |
387 | |         # Check animation config
388 | |         assert result["animation"]["type"] == "pulse"
389 | |         assert result["animation"]["duration"] == 2000
390 | |         assert result["animation"]["infinite"] is True
391 | |
392 | |     def test_render_chart(self) -> None:
393 | |         """Test chart rendering."""
394 | |         now = datetime.now(UTC)
395 | |         points = [
396 | |             ChartDataPoint(x=now, y=10),
397 | |             ChartDataPoint(x="2023-01-02", y=20),
398 | |             ChartDataPoint(x=3, y=30),
399 | |         ]
400 | |
401 | |         chart = ChartData(
402 | |             title="Test Chart",
403 | |             chart_type="line",
404 | |             data_points=points,
405 | |             x_label="Time",
406 | |             y_label="Value",
407 | |             colors=["#3b82f6"],
408 | |         )
409 | |
410 | |         result = self.renderer.render_widget(
411 | |             WidgetType.CHART,
412 | |             chart,
413 | |             {},
414 | |         )
415 | |
416 | |         assert isinstance(result, dict)
417 | |         assert result["widget_type"] == "chart"
418 | |         assert "data" in result
419 | |         assert "chart_data" in result
420 | |
421 | |         # Check data structure
422 | |         assert result["data"]["title"] == "Test Chart"
423 | |         assert result["data"]["chart_type"] == "line"
424 | |         assert result["data"]["x_label"] == "Time"
425 | |         assert result["data"]["y_label"] == "Value"
426 | |         assert result["data"]["data_points"] == 3
427 | |
428 | |         # Check chart data (Chart.js format)
429 | |         assert result["chart_data"]["type"] == "line"
430 | |         assert len(result["chart_data"]["labels"]) == 3
431 | |         assert len(result["chart_data"]["datasets"][0]["data"]) == 3
432 | |         assert result["chart_data"]["datasets"][0]["data"] == [10, 20, 30]
433 | |         assert result["chart_data"]["datasets"][0]["backgroundColor"] == "#3b82f6"
434 | |
435 | |     def test_render_table(self) -> None:
436 | |         """Test table rendering."""
437 | |         table_data = {
438 | |             "headers": ["Name", "Status", "Count"],
439 | |             "rows": [
440 | |                 ["Session 1", "Active", "5"],
441 | |                 ["Session 2", "Idle", "3"],
442 | |                 {"Name": "Session 3", "Status": "Error", "Count": "0"},
443 | |             ],
444 | |         }
445 | |
446 | |         result = self.renderer.render_widget(
447 | |             WidgetType.TABLE,
448 | |             table_data,
449 | |             {},
450 | |         )
451 | |
452 | |         assert isinstance(result, dict)
453 | |         assert result["widget_type"] == "table"
454 | |         assert "data" in result
455 | |         assert "actions" in result
456 | |
457 | |         # Check data structure
458 | |         assert result["data"]["headers"] == ["Name", "Status", "Count"]
459 | |         assert len(result["data"]["rows"]) == 3
460 | |         assert result["data"]["total_rows"] == 3
461 | |         assert result["data"]["total_columns"] == 3
462 | |
463 | |         # Check processed rows
464 | |         assert result["data"]["rows"][0] == ["Session 1", "Active", "5"]
465 | |         assert result["data"]["rows"][2] == [
466 | |             "Session 3",
467 | |             "Error",
468 | |             "0",
469 | |         ]  # Dict converted to list
470 | |
471 | |         # Check actions
472 | |         action_ids = [action["id"] for action in result["actions"]]
473 | |         assert "sort" in action_ids
474 | |         assert "filter" in action_ids
475 | |         assert "export" in action_ids
476 | |
477 | |     def test_render_generic_widget(self) -> None:
478 | |         """Test generic widget rendering."""
479 | |         result = self.renderer.render_widget(
480 | |             WidgetType.METRIC_CARD,
481 | |             {"unknown": "data"},
482 | |             {},
483 | |         )
484 | |
485 | |         # Should not error but will process as metric card with data conversion
486 | |         assert isinstance(result, dict)
487 | |         assert result["widget_type"] == "metric_card"
488 | |         assert "error" in result["data"]
489 | |
490 | |     def test_render_layout(self) -> None:
491 | |         """Test layout rendering."""
492 | |         widgets = [
493 | |             {
494 | |                 "type": WidgetType.METRIC_CARD,
495 | |                 "data": MetricCardData(title="Test 1", value=10),
496 | |             },
497 | |             {
498 | |                 "type": WidgetType.METRIC_CARD,
499 | |                 "data": MetricCardData(title="Test 2", value=20),
500 | |             },
501 | |         ]
502 | |
503 | |         result = self.renderer.render_layout(widgets, {"type": "grid", "columns": 2})
504 | |
505 | |         assert isinstance(result, dict)
506 | |         assert result["type"] == "layout"
507 | |         assert result["layout_type"] == "grid"
508 | |         assert "id" in result
509 | |         assert "widgets" in result
510 | |         assert "config" in result
511 | |         assert "metadata" in result
512 | |
513 | |         # Check widgets
514 | |         assert len(result["widgets"]) == 2
515 | |         assert result["widgets"][0]["widget_type"] == "metric_card"
516 | |         assert result["widgets"][1]["widget_type"] == "metric_card"
517 | |
518 | |         # Check metadata
519 | |         assert result["metadata"]["widget_count"] == 2
520 | |
521 | |     def test_render_container(self) -> None:
522 | |         """Test container rendering."""
523 | |         content = {"type": "widget", "widget_type": "metric_card", "data": {}}
524 | |
525 | |         result = self.renderer.render_container(
526 | |             content,
527 | |             {
528 | |                 "title": "Test Container",
529 | |                 "border": True,
530 | |                 "background": "primary",
531 | |                 "shadow": True,
532 | |             },
533 | |         )
534 | |
535 | |         assert isinstance(result, dict)
536 | |         assert result["type"] == "container"
537 | |         assert "id" in result
538 | |         assert "content" in result
539 | |         assert "config" in result
540 | |         assert "style" in result
541 | |         assert "metadata" in result
542 | |
543 | |         # Check config
544 | |         assert result["config"]["title"] == "Test Container"
545 | |         assert result["config"]["border"] is True
546 | |         assert result["config"]["background"] == "primary"
547 | |         assert result["config"]["shadow"] is True
548 | |
549 | |         # Check content
550 | |         assert result["content"] == content
551 | |
552 | |     def test_json_serialization(self) -> None:
553 | |         """Test that rendered output is JSON serializable."""
554 | |         now = datetime.now(UTC)
555 | |         session = SessionData(
556 | |             name="test-session",
557 | |             id="session-123",
558 | |             status=SessionStatus.ACTIVE,
559 | |             created_at=now,
560 | |             windows=[],
561 | |         )
562 | |
563 | |         result = self.renderer.render_widget(
564 | |             WidgetType.SESSION_BROWSER,
565 | |             [session],
566 | |             {"view_mode": "table"},
567 | |         )
568 | |
569 | |         # Should serialize to JSON without errors
570 | |         try:
571 | |             json_str = json.dumps(result, default=str)
572 | |             parsed = json.loads(json_str)
573 | |             assert isinstance(parsed, dict)
574 | |             assert parsed["widget_type"] == "session_browser"
575 | |         except (TypeError, ValueError) as e:
576 | |             pytest.fail(f"JSON serialization failed: {e}")
577 | |
578 | |     def test_helper_methods(self) -> None:
579 | |         """Test helper methods."""
580 | |         # Test health color mapping
581 | |         assert self.renderer._get_health_color(HealthLevel.EXCELLENT) == "#10b981"  # noqa: SLF001
582 | |         assert self.renderer._get_health_color(HealthLevel.CRITICAL) == "#ef4444"  # noqa: SLF001
583 | |
584 | |         # Test phase emoji mapping
585 | |         assert self.renderer._get_phase_emoji(ProgressPhase.IMPLEMENTING) == "âš™ï¸"  # noqa: SLF001
586 | |         assert self.renderer._get_phase_emoji(ProgressPhase.COMPLETED) == "ğŸ‰"  # noqa: SLF001
587 | |
588 | |         # Test log level color mapping
589 | |         assert self.renderer._get_log_level_color("ERROR") == "#ef4444"  # noqa: SLF001
590 | |         assert self.renderer._get_log_level_color("INFO") == "#3b82f6"  # noqa: SLF001
591 | |
592 | |         # Test log level counting
593 | |         logs = [
594 | |             {"level": "ERROR"},
595 | |             {"level": "ERROR"},
596 | |             {"level": "INFO"},
597 | |             {"level": "WARNING"},
598 | |         ]
599 | |         counts = self.renderer._count_log_levels(logs)  # noqa: SLF001
600 | |         assert counts["ERROR"] == 2
601 | |         assert counts["INFO"] == 1
602 | |         assert counts["WARNING"] == 1
603 | |         assert counts["DEBUG"] == 0
604 | |
605 | |     def test_error_handling(self) -> None:
606 | |         """Test error handling with invalid data."""
607 | |         # Test with invalid health data
608 | |         result = self.renderer.render_widget(
609 | |             WidgetType.HEALTH_METER,
610 | |             "invalid_data",
611 | |             {},
612 | |         )
613 | |
614 | |         assert result["data"]["error"] == "Invalid health data"
615 | |
616 | |         # Test with invalid progress data
617 | |         result = self.renderer.render_widget(
618 | |             WidgetType.PROGRESS_TRACKER,
619 | |             None,
620 | |             {},
621 | |         )
622 | |
623 | |         assert result["data"]["error"] == "Invalid progress data"
624 | |
625 | |     def test_trend_calculations(self) -> None:
626 | |         """Test trend direction calculations."""
627 | |         # Positive trend
628 | |         metric_up = MetricCardData(title="Test", value=100, trend=5.0)
629 | |         result_up = self.renderer.render_widget(WidgetType.METRIC_CARD, metric_up, {})
630 | |         assert result_up["data"]["trend_direction"] == "up"
631 | |
632 | |         # Negative trend
633 | |         metric_down = MetricCardData(title="Test", value=100, trend=-3.0)
634 | |         result_down = self.renderer.render_widget(WidgetType.METRIC_CARD, metric_down, {})
635 | |         assert result_down["data"]["trend_direction"] == "down"
636 | |
637 | |         # Neutral trend
638 | |         metric_neutral = MetricCardData(title="Test", value=100, trend=0)
639 | |         result_neutral = self.renderer.render_widget(WidgetType.METRIC_CARD, metric_neutral, {})
640 | |         assert result_neutral["data"]["trend_direction"] == "neutral"
641 | |
642 | |         # No trend
643 | |         metric_none = MetricCardData(title="Test", value=100, trend=None)
644 | |         result_none = self.renderer.render_widget(WidgetType.METRIC_CARD, metric_none, {})
645 | |         assert result_none["data"]["trend_direction"] == "neutral"
    | |__________________________________________________________________^ PLR0904
    |

tests/unit/dashboard/renderers/test_tui_renderer.py:31:1: PLR0904 Too many public methods (28 > 20)
    |
 31 | / class TestTUIRenderer:
 32 | |     """Test cases for TUI renderer."""
 33 | |
 34 | |     def setup_method(self) -> None:
 35 | |         """Setup test renderer."""
 36 | |         # Use StringIO to capture output for testing
 37 | |         self.string_io = StringIO()
 38 | |         self.console = Console(file=self.string_io, width=80, height=24, force_terminal=True)
 39 | |         self.renderer = TUIRenderer(console=self.console)
 40 | |
 41 | |     def test_renderer_initialization(self) -> None:
 42 | |         """Test TUI renderer initialization."""
 43 | |         assert self.renderer.format_type == RenderFormat.TUI
 44 | |         assert self.renderer.console is not None
 45 | |         assert self.renderer.supports_color
 46 | |         assert self.renderer.terminal_width == 80
 47 | |         assert self.renderer.terminal_height == 24
 48 | |
 49 | |     def test_color_mapping(self) -> None:
 50 | |         """Test color mapping functionality."""
 51 | |         assert self.renderer._color_map["success"] == "green"  # noqa: SLF001
 52 | |         assert self.renderer._color_map["warning"] == "yellow"  # noqa: SLF001
 53 | |         assert self.renderer._color_map["error"] == "red"  # noqa: SLF001
 54 | |         assert self.renderer._color_map["info"] == "blue"  # noqa: SLF001
 55 | |
 56 | |     def test_get_status_color(self) -> None:
 57 | |         """Test status color retrieval."""
 58 | |         color = self.renderer._get_status_color("active")  # noqa: SLF001
 59 | |         assert color in {"green", "blue", "cyan", "white"}  # Valid color options
 60 | |
 61 | |         color = self.renderer._get_status_color("error")  # noqa: SLF001
 62 | |         assert color in {"red", "yellow", "white"}
 63 | |
 64 | |     def test_get_health_color(self) -> None:
 65 | |         """Test health level color mapping."""
 66 | |         assert self.renderer._get_health_color(HealthLevel.EXCELLENT) == "green"  # noqa: SLF001
 67 | |         assert self.renderer._get_health_color(HealthLevel.GOOD) == "blue"  # noqa: SLF001
 68 | |         assert self.renderer._get_health_color(HealthLevel.WARNING) == "yellow"  # noqa: SLF001
 69 | |         assert self.renderer._get_health_color(HealthLevel.CRITICAL) == "red"  # noqa: SLF001
 70 | |         assert self.renderer._get_health_color(HealthLevel.UNKNOWN) == "dim"  # noqa: SLF001
 71 | |
 72 | |     def test_supports_feature(self) -> None:
 73 | |         """Test feature support checking."""
 74 | |         assert self.renderer.supports_feature("color") is True
 75 | |         assert self.renderer.supports_feature("unicode") is True
 76 | |         assert self.renderer.supports_feature("progress") is True
 77 | |         assert self.renderer.supports_feature("tables") is True
 78 | |         assert self.renderer.supports_feature("trees") is True
 79 | |         assert self.renderer.supports_feature("panels") is True
 80 | |         assert self.renderer.supports_feature("nonexistent") is False
 81 | |
 82 | |     def test_render_session_browser_table(self) -> None:
 83 | |         """Test session browser table view rendering."""
 84 | |         now = datetime.now(UTC)
 85 | |         window = WindowData(id="1", name="test-window", active=True, panes=2)
 86 | |
 87 | |         session = SessionData(
 88 | |             name="test-session",
 89 | |             id="session-123",
 90 | |             status=SessionStatus.ACTIVE,
 91 | |             created_at=now,
 92 | |             windows=[window],
 93 | |             panes=2,
 94 | |             claude_active=True,
 95 | |             last_activity=now,
 96 | |         )
 97 | |
 98 | |         result = self.renderer.render_widget(
 99 | |             WidgetType.SESSION_BROWSER,
100 | |             [session],
101 | |             {"view_mode": "table"},
102 | |         )
103 | |
104 | |         assert isinstance(result, str)
105 | |         assert "test-session" in result
106 | |         assert "ACTIVE" in result
107 | |         assert "ğŸ¤–" in result  # Claude active indicator
108 | |
109 | |     def test_render_session_browser_tree(self) -> None:
110 | |         """Test session browser tree view rendering."""
111 | |         now = datetime.now(UTC)
112 | |         window = WindowData(id="1", name="test-window", active=True, panes=2)
113 | |
114 | |         session = SessionData(
115 | |             name="test-session",
116 | |             id="session-123",
117 | |             status=SessionStatus.ACTIVE,
118 | |             created_at=now,
119 | |             windows=[window],
120 | |             panes=2,
121 | |             claude_active=True,
122 | |         )
123 | |
124 | |         result = self.renderer.render_widget(
125 | |             WidgetType.SESSION_BROWSER,
126 | |             [session],
127 | |             {"view_mode": "tree"},
128 | |         )
129 | |
130 | |         assert isinstance(result, str)
131 | |         assert "Sessions" in result
132 | |         assert "test-session" in result
133 | |         assert "ğŸŸ¢" in result  # Active status emoji
134 | |
135 | |     def test_render_session_browser_cards(self) -> None:
136 | |         """Test session browser cards view rendering."""
137 | |         now = datetime.now(UTC)
138 | |         window = WindowData(id="1", name="test-window", active=True, panes=2)
139 | |
140 | |         session = SessionData(
141 | |             name="test-session",
142 | |             id="session-123",
143 | |             status=SessionStatus.ACTIVE,
144 | |             created_at=now,
145 | |             windows=[window],
146 | |             panes=2,
147 | |             claude_active=True,
148 | |         )
149 | |
150 | |         result = self.renderer.render_widget(
151 | |             WidgetType.SESSION_BROWSER,
152 | |             [session],
153 | |             {"view_mode": "cards"},
154 | |         )
155 | |
156 | |         assert isinstance(result, str)
157 | |         assert "test-session" in result
158 | |         assert "Status:" in result
159 | |         assert "Windows:" in result
160 | |         assert "Claude:" in result
161 | |
162 | |     def test_render_health_meter(self) -> None:
163 | |         """Test health meter rendering."""
164 | |         now = datetime.now(UTC)
165 | |         category = HealthCategoryData(
166 | |             category="build",
167 | |             score=85,
168 | |             level=HealthLevel.GOOD,
169 | |             message="Build successful",
170 | |         )
171 | |
172 | |         health = HealthData(
173 | |             overall_score=85,
174 | |             overall_level=HealthLevel.GOOD,
175 | |             categories=[category],
176 | |             last_updated=now,
177 | |         )
178 | |
179 | |         result = self.renderer.render_widget(
180 | |             WidgetType.HEALTH_METER,
181 | |             health,
182 | |             {},
183 | |         )
184 | |
185 | |         assert isinstance(result, str)
186 | |         assert "Project Health" in result
187 | |         assert "85%" in result
188 | |         assert "Build:" in result
189 | |
190 | |     def test_render_activity_heatmap(self) -> None:
191 | |         """Test activity heatmap rendering."""
192 | |         now = datetime.now(UTC)
193 | |         entry = ActivityEntry(
194 | |             timestamp=now,
195 | |             activity_type=ActivityType.FILE_CREATED,
196 | |             description="Created new file",
197 | |         )
198 | |
199 | |         activity = ActivityData(
200 | |             entries=[entry],
201 | |             total_activities=10,
202 | |             active_days=5,
203 | |             activity_rate=75.0,
204 | |             current_streak=3,
205 | |             longest_streak=7,
206 | |             avg_per_day=2.0,
207 | |         )
208 | |
209 | |         result = self.renderer.render_widget(
210 | |             WidgetType.ACTIVITY_HEATMAP,
211 | |             activity,
212 | |             {},
213 | |         )
214 | |
215 | |         assert isinstance(result, str)
216 | |         assert "Activity Overview" in result
217 | |         assert "Total Activities: 10" in result
218 | |         assert "Active Days: 5" in result
219 | |         assert "75.0%" in result
220 | |         assert "ğŸ”¥" in result  # Heat indicator
221 | |
222 | |     def test_render_progress_tracker(self) -> None:
223 | |         """Test progress tracker rendering."""
224 | |         now = datetime.now(UTC)
225 | |
226 | |         progress = ProgressData(
227 | |             phase=ProgressPhase.IMPLEMENTING,
228 | |             phase_progress=75.0,
229 | |             overall_progress=50.0,
230 | |             files_created=5,
231 | |             files_modified=10,
232 | |             commands_executed=20,
233 | |             commands_succeeded=18,
234 | |             commands_failed=2,
235 | |             start_time=now,
236 | |             active_duration=3600.0,
237 | |             todos_identified=10,
238 | |             todos_completed=7,
239 | |         )
240 | |
241 | |         result = self.renderer.render_widget(
242 | |             WidgetType.PROGRESS_TRACKER,
243 | |             progress,
244 | |             {},
245 | |         )
246 | |
247 | |         assert isinstance(result, str)
248 | |         assert "Progress Tracker" in result
249 | |         assert "âš™ï¸" in result  # Implementing phase emoji
250 | |         assert "Overall Progress" in result
251 | |         assert "Files Created: 5" in result
252 | |         assert "Commands: 20" in result
253 | |
254 | |     def test_render_log_viewer(self) -> None:
255 | |         """Test log viewer rendering."""
256 | |         logs = [
257 | |             {
258 | |                 "timestamp": "2023-01-01T10:00:00",
259 | |                 "level": "INFO",
260 | |                 "message": "Application started",
261 | |             },
262 | |             {
263 | |                 "timestamp": "2023-01-01T10:01:00",
264 | |                 "level": "WARNING",
265 | |                 "message": "Configuration missing",
266 | |             },
267 | |             {
268 | |                 "timestamp": "2023-01-01T10:02:00",
269 | |                 "level": "ERROR",
270 | |                 "message": "Connection failed",
271 | |             },
272 | |         ]
273 | |
274 | |         result = self.renderer.render_widget(
275 | |             WidgetType.LOG_VIEWER,
276 | |             {"logs": logs},
277 | |             {"max_lines": 5},
278 | |         )
279 | |
280 | |         assert isinstance(result, str)
281 | |         assert "Recent Logs" in result
282 | |         assert "Application started" in result
283 | |         assert "Configuration missing" in result
284 | |         assert "Connection failed" in result
285 | |         assert "INFO:" in result
286 | |         assert "WARNING:" in result
287 | |         assert "ERROR:" in result
288 | |
289 | |     def test_render_metric_card(self) -> None:
290 | |         """Test metric card rendering."""
291 | |         metric = MetricCardData(
292 | |             title="CPU Usage",
293 | |             value=75.5,
294 | |             suffix="%",
295 | |             trend=5.2,
296 | |             color="warning",
297 | |             icon="ğŸ–¥ï¸",
298 | |             comparison="vs last hour",
299 | |         )
300 | |
301 | |         result = self.renderer.render_widget(
302 | |             WidgetType.METRIC_CARD,
303 | |             metric,
304 | |             {},
305 | |         )
306 | |
307 | |         assert isinstance(result, str)
308 | |         assert "CPU Usage" in result
309 | |         assert "75.50%" in result  # format_number adds precision
310 | |         assert "ğŸ–¥ï¸" in result
311 | |         assert "â†—ï¸ +5.2" in result
312 | |         assert "vs last hour" in result
313 | |
314 | |     def test_render_status_indicator(self) -> None:
315 | |         """Test status indicator rendering."""
316 | |         status = StatusIndicatorData(
317 | |             status="running",
318 | |             label="Service Active",
319 | |             color="success",
320 | |             icon="âœ…",
321 | |             pulse=True,
322 | |         )
323 | |
324 | |         result = self.renderer.render_widget(
325 | |             WidgetType.STATUS_INDICATOR,
326 | |             status,
327 | |             {},
328 | |         )
329 | |
330 | |         assert isinstance(result, str)
331 | |         assert "RUNNING" in result
332 | |         assert "Service Active" in result
333 | |         assert "âœ…" in result
334 | |         assert "âŸ³" in result  # Pulse indicator
335 | |
336 | |     def test_render_chart(self) -> None:
337 | |         """Test chart rendering."""
338 | |         now = datetime.now(UTC)
339 | |         points = [
340 | |             ChartDataPoint(x=now, y=10),
341 | |             ChartDataPoint(x="2023-01-02", y=20),
342 | |             ChartDataPoint(x=3, y=30),
343 | |         ]
344 | |
345 | |         chart = ChartData(
346 | |             title="Test Chart",
347 | |             chart_type="line",
348 | |             data_points=points,
349 | |             x_label="Time",
350 | |             y_label="Value",
351 | |         )
352 | |
353 | |         result = self.renderer.render_widget(
354 | |             WidgetType.CHART,
355 | |             chart,
356 | |             {},
357 | |         )
358 | |
359 | |         assert isinstance(result, str)
360 | |         assert "Test Chart" in result
361 | |         assert "Max: 30, Min: 10" in result
362 | |         assert "â–ˆ" in result  # Bar character
363 | |         assert "â–‘" in result  # Empty bar character
364 | |
365 | |     def test_render_table(self) -> None:
366 | |         """Test generic table rendering."""
367 | |         table_data = {
368 | |             "headers": ["Name", "Status", "Count"],
369 | |             "rows": [
370 | |                 ["Session 1", "Active", "5"],
371 | |                 ["Session 2", "Idle", "3"],
372 | |                 {"Name": "Session 3", "Status": "Error", "Count": "0"},
373 | |             ],
374 | |         }
375 | |
376 | |         result = self.renderer.render_widget(
377 | |             WidgetType.TABLE,
378 | |             table_data,
379 | |             {},
380 | |         )
381 | |
382 | |         assert isinstance(result, str)
383 | |         assert "Name" in result
384 | |         assert "Status" in result
385 | |         assert "Count" in result
386 | |         assert "Session 1" in result
387 | |         assert "Active" in result
388 | |
389 | |     def test_render_generic_widget(self) -> None:
390 | |         """Test generic widget fallback rendering."""
391 | |         result = self.renderer._render_generic_widget(  # noqa: SLF001
392 | |             WidgetType.METRIC_CARD,
393 | |             {"test": "data"},
394 | |             {},
395 | |         )
396 | |
397 | |         assert isinstance(result, str)
398 | |         assert "Widget Type: metric_card" in result
399 | |         assert "Data:" in result
400 | |
401 | |     def test_render_layout_vertical(self) -> None:
402 | |         """Test vertical layout rendering."""
403 | |         widgets = [
404 | |             {
405 | |                 "type": WidgetType.METRIC_CARD,
406 | |                 "data": MetricCardData(title="Test 1", value=10),
407 | |             },
408 | |             {
409 | |                 "type": WidgetType.METRIC_CARD,
410 | |                 "data": MetricCardData(title="Test 2", value=20),
411 | |             },
412 | |         ]
413 | |
414 | |         result = self.renderer.render_layout(widgets, {"type": "vertical", "spacing": 2})
415 | |
416 | |         assert isinstance(result, str)
417 | |         assert "Test 1" in result
418 | |         assert "Test 2" in result
419 | |
420 | |     def test_render_layout_horizontal(self) -> None:
421 | |         """Test horizontal layout rendering."""
422 | |         widgets = [
423 | |             {
424 | |                 "type": WidgetType.METRIC_CARD,
425 | |                 "data": MetricCardData(title="Test 1", value=10),
426 | |             },
427 | |             {
428 | |                 "type": WidgetType.METRIC_CARD,
429 | |                 "data": MetricCardData(title="Test 2", value=20),
430 | |             },
431 | |         ]
432 | |
433 | |         result = self.renderer.render_layout(widgets, {"type": "horizontal"})
434 | |
435 | |         assert isinstance(result, str)
436 | |         assert "Test 1" in result
437 | |         assert "Test 2" in result
438 | |
439 | |     def test_render_layout_grid(self) -> None:
440 | |         """Test grid layout rendering."""
441 | |         widgets = [
442 | |             {
443 | |                 "type": WidgetType.METRIC_CARD,
444 | |                 "data": MetricCardData(title="Test 1", value=10),
445 | |             },
446 | |             {
447 | |                 "type": WidgetType.METRIC_CARD,
448 | |                 "data": MetricCardData(title="Test 2", value=20),
449 | |             },
450 | |             {
451 | |                 "type": WidgetType.METRIC_CARD,
452 | |                 "data": MetricCardData(title="Test 3", value=30),
453 | |             },
454 | |             {
455 | |                 "type": WidgetType.METRIC_CARD,
456 | |                 "data": MetricCardData(title="Test 4", value=40),
457 | |             },
458 | |         ]
459 | |
460 | |         result = self.renderer.render_layout(widgets, {"type": "grid", "columns": 2})
461 | |
462 | |         assert isinstance(result, str)
463 | |         assert "Test 1" in result
464 | |         assert "Test 4" in result
465 | |
466 | |     def test_render_container_with_border(self) -> None:
467 | |         """Test container rendering with border."""
468 | |         content = "Test content"
469 | |
470 | |         result = self.renderer.render_container(
471 | |             content,
472 | |             {
473 | |                 "title": "Test Container",
474 | |                 "border": True,
475 | |                 "style": "blue",
476 | |                 "rounded": True,
477 | |                 "padding": (1, 2),
478 | |             },
479 | |         )
480 | |
481 | |         assert isinstance(result, str)
482 | |         assert "Test content" in result
483 | |
484 | |     def test_render_container_without_border(self) -> None:
485 | |         """Test container rendering without border."""
486 | |         content = "Test content"
487 | |
488 | |         result = self.renderer.render_container(
489 | |             content,
490 | |             {"border": False},
491 | |         )
492 | |
493 | |         assert result == content
494 | |
495 | |     def test_error_handling_invalid_data(self) -> None:
496 | |         """Test error handling with invalid data."""
497 | |         # Test with invalid health data
498 | |         result = self.renderer.render_widget(
499 | |             WidgetType.HEALTH_METER,
500 | |             "invalid_data",
501 | |             {},
502 | |         )
503 | |
504 | |         assert result == "Invalid health data"
505 | |
506 | |         # Test with invalid session data
507 | |         result = self.renderer.render_widget(
508 | |             WidgetType.SESSION_BROWSER,
509 | |             None,
510 | |             {},
511 | |         )
512 | |
513 | |         assert isinstance(result, str)
514 | |
515 | |     def test_empty_session_list(self) -> None:
516 | |         """Test rendering empty session list."""
517 | |         result = self.renderer.render_widget(
518 | |             WidgetType.SESSION_BROWSER,
519 | |             [],
520 | |             {"view_mode": "cards"},
521 | |         )
522 | |
523 | |         assert result == "No sessions found"
524 | |
525 | |     def test_activity_heatmap_heat_levels(self) -> None:
526 | |         """Test different activity heat levels."""
527 | |         # High activity
528 | |         high_activity = ActivityData(
529 | |             entries=[],
530 | |             total_activities=100,
531 | |             active_days=10,
532 | |             activity_rate=85.0,
533 | |             current_streak=5,
534 | |         )
535 | |
536 | |         result = self.renderer._render_activity_heatmap(high_activity, {})  # noqa: SLF001
537 | |         assert "ğŸ”¥ğŸ”¥ğŸ”¥" in result
538 | |
539 | |         # Medium activity
540 | |         medium_activity = ActivityData(
541 | |             entries=[],
542 | |             total_activities=50,
543 | |             active_days=5,
544 | |             activity_rate=65.0,
545 | |             current_streak=3,
546 | |         )
547 | |
548 | |         result = self.renderer._render_activity_heatmap(medium_activity, {})  # noqa: SLF001
549 | |         assert "ğŸ”¥ğŸ”¥" in result
550 | |
551 | |         # Low activity
552 | |         low_activity = ActivityData(
553 | |             entries=[],
554 | |             total_activities=10,
555 | |             active_days=2,
556 | |             activity_rate=45.0,
557 | |             current_streak=1,
558 | |         )
559 | |
560 | |         result = self.renderer._render_activity_heatmap(low_activity, {})  # noqa: SLF001
561 | |         assert "ğŸ”¥" in result
562 | |
563 | |         # Very low activity
564 | |         very_low_activity = ActivityData(
565 | |             entries=[],
566 | |             total_activities=5,
567 | |             active_days=1,
568 | |             activity_rate=25.0,
569 | |             current_streak=0,
570 | |         )
571 | |
572 | |         result = self.renderer._render_activity_heatmap(very_low_activity, {})  # noqa: SLF001
573 | |         assert "â„ï¸" in result
574 | |
575 | |     def test_progress_phase_emojis(self) -> None:
576 | |         """Test different progress phase emojis."""
577 | |         phases_and_emojis = [
578 | |             (ProgressPhase.STARTING, "ğŸš€"),
579 | |             (ProgressPhase.ANALYZING, "ğŸ”"),
580 | |             (ProgressPhase.IMPLEMENTING, "âš™ï¸"),
581 | |             (ProgressPhase.TESTING, "ğŸ§ª"),
582 | |             (ProgressPhase.COMPLETING, "âœ…"),
583 | |             (ProgressPhase.COMPLETED, "ğŸ‰"),
584 | |             (ProgressPhase.IDLE, "ğŸ’¤"),
585 | |             (ProgressPhase.ERROR, "âŒ"),
586 | |         ]
587 | |
588 | |         for phase, emoji in phases_and_emojis:
589 | |             progress = ProgressData(
590 | |                 phase=phase,
591 | |                 phase_progress=50.0,
592 | |                 overall_progress=25.0,
593 | |             )
594 | |
595 | |             result = self.renderer._render_progress_tracker(progress, {})  # noqa: SLF001
596 | |             assert emoji in result
597 | |
598 | |     def test_metric_card_trend_indicators(self) -> None:
599 | |         """Test metric card trend indicators."""
600 | |         # Positive trend
601 | |         positive_metric = MetricCardData(
602 | |             title="Test",
603 | |             value=100,
604 | |             trend=5.2,
605 | |         )
606 | |         result = self.renderer._render_metric_card(positive_metric, {})  # noqa: SLF001
607 | |         assert "â†—ï¸ +5.2" in result
608 | |
609 | |         # Negative trend
610 | |         negative_metric = MetricCardData(
611 | |             title="Test",
612 | |             value=100,
613 | |             trend=-3.1,
614 | |         )
615 | |         result = self.renderer._render_metric_card(negative_metric, {})  # noqa: SLF001
616 | |         assert "â†˜ï¸ -3.1" in result
617 | |
618 | |         # No trend
619 | |         no_trend_metric = MetricCardData(
620 | |             title="Test",
621 | |             value=100,
622 | |             trend=0,
623 | |         )
624 | |         result = self.renderer._render_metric_card(no_trend_metric, {})  # noqa: SLF001
625 | |         assert "â¡ï¸ 0" in result
626 | |
627 | |         # No trend data
628 | |         no_trend_data_metric = MetricCardData(
629 | |             title="Test",
630 | |             value=100,
631 | |             trend=None,
632 | |         )
633 | |         result = self.renderer._render_metric_card(no_trend_data_metric, {})  # noqa: SLF001
634 | |         assert "â†—ï¸" not in result
635 | |         assert "â†˜ï¸" not in result
636 | |         assert "â¡ï¸" not in result
    | |________________________________^ PLR0904
    |

tests/unit/dashboard/renderers/test_web_renderer.py:32:1: PLR0904 Too many public methods (29 > 20)
    |
 32 | / class TestWebRenderer:
 33 | |     """Test cases for Web renderer."""
 34 | |
 35 | |     def setup_method(self) -> None:
 36 | |         """Setup test renderer."""
 37 | |         self.renderer = WebRenderer()
 38 | |
 39 | |     def test_renderer_initialization(self) -> None:
 40 | |         """Test Web renderer initialization."""
 41 | |         assert self.renderer.format_type == RenderFormat.WEB
 42 | |         assert self.renderer.component_id_counter == 0
 43 | |         assert "success" in self.renderer._color_classes  # noqa: SLF001
 44 | |         assert "active" in self.renderer._status_classes  # noqa: SLF001
 45 | |
 46 | |     def test_component_id_generation(self) -> None:
 47 | |         """Test unique component ID generation."""
 48 | |         id1 = self.renderer._generate_component_id()  # noqa: SLF001
 49 | |         id2 = self.renderer._generate_component_id()  # noqa: SLF001
 50 | |
 51 | |         assert id1 != id2
 52 | |         assert id1.startswith("widget-1-")
 53 | |         assert id2.startswith("widget-2-")
 54 | |         assert self.renderer.component_id_counter == 2
 55 | |
 56 | |     def test_health_color_class_mapping(self) -> None:
 57 | |         """Test health level to CSS class mapping."""
 58 | |         assert self.renderer._get_health_color_class(HealthLevel.EXCELLENT) == "green-600"  # noqa: SLF001
 59 | |         assert self.renderer._get_health_color_class(HealthLevel.GOOD) == "blue-600"  # noqa: SLF001
 60 | |         assert self.renderer._get_health_color_class(HealthLevel.WARNING) == "yellow-600"  # noqa: SLF001
 61 | |         assert self.renderer._get_health_color_class(HealthLevel.CRITICAL) == "red-600"  # noqa: SLF001
 62 | |         assert self.renderer._get_health_color_class(HealthLevel.UNKNOWN) == "gray-600"  # noqa: SLF001
 63 | |
 64 | |     def test_supports_feature(self) -> None:
 65 | |         """Test feature support checking."""
 66 | |         assert self.renderer.supports_feature("html") is True
 67 | |         assert self.renderer.supports_feature("css") is True
 68 | |         assert self.renderer.supports_feature("javascript") is True
 69 | |         assert self.renderer.supports_feature("interactive") is True
 70 | |         assert self.renderer.supports_feature("responsive") is True
 71 | |         assert self.renderer.supports_feature("themes") is True
 72 | |         assert self.renderer.supports_feature("animations") is True
 73 | |         assert self.renderer.supports_feature("nonexistent") is False
 74 | |
 75 | |     def test_embed_widget_data(self) -> None:
 76 | |         """Test JavaScript data embedding."""
 77 | |         component_id = "test-widget-123"
 78 | |         test_data = {"type": "test", "value": 42, "name": "test widget"}
 79 | |
 80 | |         result = self.renderer._embed_widget_data(component_id, test_data)  # noqa: SLF001
 81 | |
 82 | |         assert isinstance(result, str)
 83 | |         assert component_id in result
 84 | |         assert "window.widgetData" in result
 85 | |         assert '"type": "test"' in result
 86 | |         assert '"value": 42' in result
 87 | |         assert "<script>" in result
 88 | |         assert "</script>" in result
 89 | |
 90 | |     def test_render_session_browser_table(self) -> None:
 91 | |         """Test session browser table view rendering."""
 92 | |         now = datetime.now(UTC)
 93 | |         window = WindowData(id="1", name="test-window", active=True, panes=2)
 94 | |
 95 | |         session = SessionData(
 96 | |             name="test-session",
 97 | |             id="session-123",
 98 | |             status=SessionStatus.ACTIVE,
 99 | |             created_at=now,
100 | |             windows=[window],
101 | |             panes=2,
102 | |             claude_active=True,
103 | |             last_activity=now,
104 | |         )
105 | |
106 | |         result = self.renderer.render_widget(
107 | |             WidgetType.SESSION_BROWSER,
108 | |             [session],
109 | |             {"view_mode": "table"},
110 | |         )
111 | |
112 | |         assert isinstance(result, str)
113 | |         assert "session-browser-table" in result
114 | |         assert "test-session" in result
115 | |         assert "ACTIVE" in result
116 | |         assert "ğŸ¤– Active" in result
117 | |         assert "table" in result
118 | |         assert "thead" in result
119 | |         assert "tbody" in result
120 | |         assert "window.widgetData" in result
121 | |
122 | |     def test_render_session_browser_cards(self) -> None:
123 | |         """Test session browser cards view rendering."""
124 | |         now = datetime.now(UTC)
125 | |         window = WindowData(id="1", name="test-window", active=True, panes=2)
126 | |
127 | |         session = SessionData(
128 | |             name="test-session",
129 | |             id="session-123",
130 | |             status=SessionStatus.ACTIVE,
131 | |             created_at=now,
132 | |             windows=[window],
133 | |             panes=2,
134 | |             claude_active=True,
135 | |         )
136 | |
137 | |         result = self.renderer.render_widget(
138 | |             WidgetType.SESSION_BROWSER,
139 | |             [session],
140 | |             {"view_mode": "cards"},
141 | |         )
142 | |
143 | |         assert isinstance(result, str)
144 | |         assert "session-browser-cards" in result
145 | |         assert "test-session" in result
146 | |         assert "grid" in result
147 | |         assert "ğŸ“‹" in result
148 | |         assert "window.widgetData" in result
149 | |
150 | |     def test_render_session_browser_list(self) -> None:
151 | |         """Test session browser list view rendering."""
152 | |         now = datetime.now(UTC)
153 | |         window = WindowData(id="1", name="test-window", active=True, panes=2)
154 | |
155 | |         session = SessionData(
156 | |             name="test-session",
157 | |             id="session-123",
158 | |             status=SessionStatus.ACTIVE,
159 | |             created_at=now,
160 | |             windows=[window],
161 | |             panes=2,
162 | |             claude_active=True,
163 | |         )
164 | |
165 | |         result = self.renderer.render_widget(
166 | |             WidgetType.SESSION_BROWSER,
167 | |             [session],
168 | |             {"view_mode": "list"},
169 | |         )
170 | |
171 | |         assert isinstance(result, str)
172 | |         assert "session-browser-list" in result
173 | |         assert "test-session" in result
174 | |         assert "ğŸ“‹" in result
175 | |         assert "<ul" in result
176 | |         assert "<li" in result
177 | |         assert "window.widgetData" in result
178 | |
179 | |     def test_render_health_meter(self) -> None:
180 | |         """Test health meter rendering."""
181 | |         now = datetime.now(UTC)
182 | |         category = HealthCategoryData(
183 | |             category="build",
184 | |             score=85,
185 | |             level=HealthLevel.GOOD,
186 | |             message="Build successful",
187 | |         )
188 | |
189 | |         health = HealthData(
190 | |             overall_score=85,
191 | |             overall_level=HealthLevel.GOOD,
192 | |             categories=[category],
193 | |             last_updated=now,
194 | |         )
195 | |
196 | |         result = self.renderer.render_widget(
197 | |             WidgetType.HEALTH_METER,
198 | |             health,
199 | |             {},
200 | |         )
201 | |
202 | |         assert isinstance(result, str)
203 | |         assert "health-meter" in result
204 | |         assert "Project Health" in result
205 | |         assert "85%" in result
206 | |         assert "Build" in result
207 | |         assert "progress" in result or "bg-blue-600" in result
208 | |         assert "window.widgetData" in result
209 | |
210 | |     def test_render_activity_heatmap(self) -> None:
211 | |         """Test activity heatmap rendering."""
212 | |         now = datetime.now(UTC)
213 | |         entry = ActivityEntry(
214 | |             timestamp=now,
215 | |             activity_type=ActivityType.FILE_CREATED,
216 | |             description="Created new file",
217 | |         )
218 | |
219 | |         activity = ActivityData(
220 | |             entries=[entry],
221 | |             total_activities=10,
222 | |             active_days=5,
223 | |             activity_rate=75.0,
224 | |             current_streak=3,
225 | |             longest_streak=7,
226 | |             avg_per_day=2.0,
227 | |         )
228 | |
229 | |         result = self.renderer.render_widget(
230 | |             WidgetType.ACTIVITY_HEATMAP,
231 | |             activity,
232 | |             {},
233 | |         )
234 | |
235 | |         assert isinstance(result, str)
236 | |         assert "activity-heatmap" in result
237 | |         assert "Activity Overview" in result
238 | |         assert "Total Activities:" in result
239 | |         assert ">10<" in result
240 | |         assert "Active Days:" in result
241 | |         assert ">5<" in result
242 | |         assert "75.0%" in result
243 | |         assert "ğŸ”¥" in result or "ğŸ”¥ğŸ”¥" in result
244 | |         assert "window.widgetData" in result
245 | |
246 | |     def test_render_progress_tracker(self) -> None:
247 | |         """Test progress tracker rendering."""
248 | |         now = datetime.now(UTC)
249 | |
250 | |         progress = ProgressData(
251 | |             phase=ProgressPhase.IMPLEMENTING,
252 | |             phase_progress=75.0,
253 | |             overall_progress=50.0,
254 | |             files_created=5,
255 | |             files_modified=10,
256 | |             commands_executed=20,
257 | |             commands_succeeded=18,
258 | |             commands_failed=2,
259 | |             start_time=now,
260 | |             active_duration=3600.0,
261 | |             todos_identified=10,
262 | |             todos_completed=7,
263 | |         )
264 | |
265 | |         result = self.renderer.render_widget(
266 | |             WidgetType.PROGRESS_TRACKER,
267 | |             progress,
268 | |             {},
269 | |         )
270 | |
271 | |         assert isinstance(result, str)
272 | |         assert "progress-tracker" in result
273 | |         assert "Progress Tracker" in result
274 | |         assert "âš™ï¸" in result
275 | |         assert "50%" in result  # overall progress
276 | |         assert "75%" in result  # phase progress
277 | |         assert "Files Created:" in result
278 | |         assert ">5<" in result
279 | |         assert "Commands:" in result
280 | |         assert ">20<" in result
281 | |         assert "window.widgetData" in result
282 | |
283 | |     def test_render_log_viewer(self) -> None:
284 | |         """Test log viewer rendering."""
285 | |         logs = [
286 | |             {
287 | |                 "timestamp": "2023-01-01T10:00:00",
288 | |                 "level": "INFO",
289 | |                 "message": "Application started",
290 | |             },
291 | |             {
292 | |                 "timestamp": "2023-01-01T10:01:00",
293 | |                 "level": "WARNING",
294 | |                 "message": "Configuration missing",
295 | |             },
296 | |             {
297 | |                 "timestamp": "2023-01-01T10:02:00",
298 | |                 "level": "ERROR",
299 | |                 "message": "Connection failed",
300 | |             },
301 | |         ]
302 | |
303 | |         result = self.renderer.render_widget(
304 | |             WidgetType.LOG_VIEWER,
305 | |             {"logs": logs},
306 | |             {"max_lines": 5},
307 | |         )
308 | |
309 | |         assert isinstance(result, str)
310 | |         assert "log-viewer" in result
311 | |         assert "Recent Logs" in result
312 | |         assert "Application started" in result
313 | |         assert "Configuration missing" in result
314 | |         assert "Connection failed" in result
315 | |         assert "INFO" in result
316 | |         assert "WARNING" in result
317 | |         assert "ERROR" in result
318 | |         assert "window.widgetData" in result
319 | |
320 | |     def test_render_metric_card(self) -> None:
321 | |         """Test metric card rendering."""
322 | |         metric = MetricCardData(
323 | |             title="CPU Usage",
324 | |             value=75.5,
325 | |             suffix="%",
326 | |             trend=5.2,
327 | |             color="warning",
328 | |             icon="ğŸ–¥ï¸",
329 | |             comparison="vs last hour",
330 | |         )
331 | |
332 | |         result = self.renderer.render_widget(
333 | |             WidgetType.METRIC_CARD,
334 | |             metric,
335 | |             {},
336 | |         )
337 | |
338 | |         assert isinstance(result, str)
339 | |         assert "metric-card" in result
340 | |         assert "CPU Usage" in result
341 | |         assert "75.50%" in result  # formatted number
342 | |         assert "ğŸ–¥ï¸" in result
343 | |         assert "â†—ï¸ +5.2" in result
344 | |         assert "vs last hour" in result
345 | |         assert "window.widgetData" in result
346 | |
347 | |     def test_render_status_indicator(self) -> None:
348 | |         """Test status indicator rendering."""
349 | |         status = StatusIndicatorData(
350 | |             status="running",
351 | |             label="Service Active",
352 | |             color="success",
353 | |             icon="âœ…",
354 | |             pulse=True,
355 | |         )
356 | |
357 | |         result = self.renderer.render_widget(
358 | |             WidgetType.STATUS_INDICATOR,
359 | |             status,
360 | |             {},
361 | |         )
362 | |
363 | |         assert isinstance(result, str)
364 | |         assert "status-indicator" in result
365 | |         assert "running" in result  # Status is lowercase in web renderer
366 | |         assert "Service Active" in result
367 | |         assert "âœ…" in result
368 | |         assert "animate-pulse" in result
369 | |         assert "window.widgetData" in result
370 | |
371 | |     def test_render_chart(self) -> None:
372 | |         """Test chart rendering."""
373 | |         now = datetime.now(UTC)
374 | |         points = [
375 | |             ChartDataPoint(x=now, y=10),
376 | |             ChartDataPoint(x="2023-01-02", y=20),
377 | |             ChartDataPoint(x=3, y=30),
378 | |         ]
379 | |
380 | |         chart = ChartData(
381 | |             title="Test Chart",
382 | |             chart_type="line",
383 | |             data_points=points,
384 | |             x_label="Time",
385 | |             y_label="Value",
386 | |         )
387 | |
388 | |         result = self.renderer.render_widget(
389 | |             WidgetType.CHART,
390 | |             chart,
391 | |             {},
392 | |         )
393 | |
394 | |         assert isinstance(result, str)
395 | |         assert "chart-widget" in result
396 | |         assert "Test Chart" in result
397 | |         assert "chart-container" in result
398 | |         assert "Chart data loading..." in result
399 | |         assert "window.widgetData" in result
400 | |
401 | |     def test_render_table(self) -> None:
402 | |         """Test generic table rendering."""
403 | |         table_data = {
404 | |             "headers": ["Name", "Status", "Count"],
405 | |             "rows": [
406 | |                 ["Session 1", "Active", "5"],
407 | |                 ["Session 2", "Idle", "3"],
408 | |                 {"Name": "Session 3", "Status": "Error", "Count": "0"},
409 | |             ],
410 | |         }
411 | |
412 | |         result = self.renderer.render_widget(
413 | |             WidgetType.TABLE,
414 | |             table_data,
415 | |             {},
416 | |         )
417 | |
418 | |         assert isinstance(result, str)
419 | |         assert "table-widget" in result
420 | |         assert "Name" in result
421 | |         assert "Status" in result
422 | |         assert "Count" in result
423 | |         assert "Session 1" in result
424 | |         assert "Active" in result
425 | |         assert "<table" in result
426 | |         assert "<thead" in result
427 | |         assert "<tbody" in result
428 | |         assert "window.widgetData" in result
429 | |
430 | |     def test_render_generic_widget(self) -> None:
431 | |         """Test generic widget fallback rendering."""
432 | |         result = self.renderer._render_generic_widget(  # noqa: SLF001
433 | |             WidgetType.METRIC_CARD,
434 | |             {"test": "data"},
435 | |             {},
436 | |             "test-component-123",
437 | |         )
438 | |
439 | |         assert isinstance(result, str)
440 | |         assert "generic-widget" in result
441 | |         assert "Metric Card" in result
442 | |         assert "Widget renderer not implemented" in result
443 | |         assert "test-component-123" in result
444 | |
445 | |     def test_render_error_widget(self) -> None:
446 | |         """Test error widget rendering."""
447 | |         result = self.renderer._render_error_widget(  # noqa: SLF001
448 | |             "Test error message",
449 | |             "error-component-123",
450 | |         )
451 | |
452 | |         assert isinstance(result, str)
453 | |         assert "error-widget" in result
454 | |         assert "Rendering Error" in result
455 | |         assert "Test error message" in result
456 | |         assert "error-component-123" in result
457 | |         assert "âŒ" in result
458 | |
459 | |     def test_render_layout_vertical(self) -> None:
460 | |         """Test vertical layout rendering."""
461 | |         widgets = [
462 | |             {
463 | |                 "type": WidgetType.METRIC_CARD,
464 | |                 "data": MetricCardData(title="Test 1", value=10),
465 | |             },
466 | |             {
467 | |                 "type": WidgetType.METRIC_CARD,
468 | |                 "data": MetricCardData(title="Test 2", value=20),
469 | |             },
470 | |         ]
471 | |
472 | |         result = self.renderer.render_layout(widgets, {"type": "vertical", "spacing": "space-y-6"})
473 | |
474 | |         assert isinstance(result, str)
475 | |         assert "dashboard-layout-vertical" in result
476 | |         assert "space-y-6" in result
477 | |         assert "Test 1" in result
478 | |         assert "Test 2" in result
479 | |
480 | |     def test_render_layout_flex(self) -> None:
481 | |         """Test flex layout rendering."""
482 | |         widgets = [
483 | |             {
484 | |                 "type": WidgetType.METRIC_CARD,
485 | |                 "data": MetricCardData(title="Test 1", value=10),
486 | |             },
487 | |             {
488 | |                 "type": WidgetType.METRIC_CARD,
489 | |                 "data": MetricCardData(title="Test 2", value=20),
490 | |             },
491 | |         ]
492 | |
493 | |         result = self.renderer.render_layout(widgets, {"type": "flex", "direction": "row", "gap": "gap-6"})
494 | |
495 | |         assert isinstance(result, str)
496 | |         assert "dashboard-layout-flex" in result
497 | |         assert "flex-row" in result
498 | |         assert "gap-6" in result
499 | |         assert "flex-1" in result
500 | |         assert "Test 1" in result
501 | |         assert "Test 2" in result
502 | |
503 | |     def test_render_layout_grid(self) -> None:
504 | |         """Test grid layout rendering."""
505 | |         widgets = [
506 | |             {
507 | |                 "type": WidgetType.METRIC_CARD,
508 | |                 "data": MetricCardData(title="Test 1", value=10),
509 | |             },
510 | |             {
511 | |                 "type": WidgetType.METRIC_CARD,
512 | |                 "data": MetricCardData(title="Test 2", value=20),
513 | |             },
514 | |             {
515 | |                 "type": WidgetType.METRIC_CARD,
516 | |                 "data": MetricCardData(title="Test 3", value=30),
517 | |             },
518 | |             {
519 | |                 "type": WidgetType.METRIC_CARD,
520 | |                 "data": MetricCardData(title="Test 4", value=40),
521 | |             },
522 | |         ]
523 | |
524 | |         result = self.renderer.render_layout(widgets, {"type": "grid", "columns": 2, "gap": "gap-4"})
525 | |
526 | |         assert isinstance(result, str)
527 | |         assert "dashboard-layout-grid" in result
528 | |         assert "grid-cols-1 md:grid-cols-2" in result
529 | |         assert "gap-4" in result
530 | |         assert "Test 1" in result
531 | |         assert "Test 4" in result
532 | |
533 | |     def test_render_container_with_border(self) -> None:
534 | |         """Test container rendering with border."""
535 | |         content = "<p>Test content</p>"
536 | |
537 | |         result = self.renderer.render_container(
538 | |             content,
539 | |             {
540 | |                 "title": "Test Container",
541 | |                 "border": True,
542 | |                 "style": "info",
543 | |                 "padding": "p-6",
544 | |             },
545 | |         )
546 | |
547 | |         assert isinstance(result, str)
548 | |         assert "dashboard-container" in result
549 | |         assert "Test Container" in result
550 | |         assert "Test content" in result
551 | |         assert "border" in result
552 | |         assert "rounded-lg" in result
553 | |         assert "p-6" in result
554 | |
555 | |     def test_render_container_without_border(self) -> None:
556 | |         """Test container rendering without border."""
557 | |         content = "<p>Test content</p>"
558 | |
559 | |         result = self.renderer.render_container(
560 | |             content,
561 | |             {"border": False},
562 | |         )
563 | |
564 | |         assert isinstance(result, str)
565 | |         assert "Test content" in result
566 | |         assert "border" not in result
567 | |
568 | |     def test_html_escaping(self) -> None:
569 | |         """Test HTML escaping for security."""
570 | |         malicious_data = MetricCardData(
571 | |             title="<script>alert('xss')</script>",
572 | |             value=100,
573 | |             comparison="<img src=x onerror=alert('xss')>",
574 | |         )
575 | |
576 | |         result = self.renderer.render_widget(
577 | |             WidgetType.METRIC_CARD,
578 | |             malicious_data,
579 | |             {},
580 | |         )
581 | |
582 | |         # Should be escaped in the content area (not in the embedded script)
583 | |         assert "&lt;script&gt;" in result
584 | |         assert "&lt;img" in result
585 | |         # Check that malicious content is properly escaped in HTML content
586 | |         # The title should be escaped in the dt element
587 | |         assert "&lt;script&gt;alert(&#x27;xss&#x27;)&lt;/script&gt;" in result
588 | |         # The comparison should be escaped
589 | |         assert "&lt;img src=x onerror=alert(&#x27;xss&#x27;)&gt;" in result
590 | |
591 | |     def test_error_handling_invalid_data(self) -> None:
592 | |         """Test error handling with invalid data."""
593 | |         # Test with invalid health data
594 | |         result = self.renderer.render_widget(
595 | |             WidgetType.HEALTH_METER,
596 | |             "invalid_data",
597 | |             {},
598 | |         )
599 | |
600 | |         assert "error-widget" in result
601 | |         assert "Invalid health data" in result
602 | |
603 | |         # Test with invalid session data produces valid HTML
604 | |         result = self.renderer.render_widget(
605 | |             WidgetType.SESSION_BROWSER,
606 | |             None,
607 | |             {},
608 | |         )
609 | |
610 | |         assert isinstance(result, str)
611 | |         assert "session-browser" in result
612 | |
613 | |     def test_activity_heatmap_heat_levels(self) -> None:
614 | |         """Test different activity heat levels."""
615 | |         # High activity
616 | |         high_activity = ActivityData(
617 | |             entries=[],
618 | |             total_activities=100,
619 | |             active_days=10,
620 | |             activity_rate=85.0,
621 | |             current_streak=5,
622 | |         )
623 | |
624 | |         result = self.renderer._render_activity_heatmap(high_activity, {}, "test-1")  # noqa: SLF001
625 | |         assert "ğŸ”¥ğŸ”¥ğŸ”¥" in result
626 | |         assert "text-red-600" in result
627 | |
628 | |         # Medium activity
629 | |         medium_activity = ActivityData(
630 | |             entries=[],
631 | |             total_activities=50,
632 | |             active_days=5,
633 | |             activity_rate=65.0,
634 | |             current_streak=3,
635 | |         )
636 | |
637 | |         result = self.renderer._render_activity_heatmap(medium_activity, {}, "test-2")  # noqa: SLF001
638 | |         assert "ğŸ”¥ğŸ”¥" in result
639 | |         assert "text-orange-600" in result
640 | |
641 | |         # Low activity
642 | |         low_activity = ActivityData(
643 | |             entries=[],
644 | |             total_activities=10,
645 | |             active_days=2,
646 | |             activity_rate=45.0,
647 | |             current_streak=1,
648 | |         )
649 | |
650 | |         result = self.renderer._render_activity_heatmap(low_activity, {}, "test-3")  # noqa: SLF001
651 | |         assert "ğŸ”¥" in result
652 | |         assert "text-yellow-600" in result
653 | |
654 | |         # Very low activity
655 | |         very_low_activity = ActivityData(
656 | |             entries=[],
657 | |             total_activities=5,
658 | |             active_days=1,
659 | |             activity_rate=25.0,
660 | |             current_streak=0,
661 | |         )
662 | |
663 | |         result = self.renderer._render_activity_heatmap(very_low_activity, {}, "test-4")  # noqa: SLF001
664 | |         assert "â„ï¸" in result
665 | |         assert "text-blue-600" in result
666 | |
667 | |     def test_metric_card_trend_indicators(self) -> None:
668 | |         """Test metric card trend indicators."""
669 | |         # Positive trend
670 | |         positive_metric = MetricCardData(
671 | |             title="Test",
672 | |             value=100,
673 | |             trend=5.2,
674 | |         )
675 | |         result = self.renderer._render_metric_card(positive_metric, {}, "test-1")  # noqa: SLF001
676 | |         assert "â†—ï¸ +5.2" in result
677 | |         assert "text-green-600" in result
678 | |
679 | |         # Negative trend
680 | |         negative_metric = MetricCardData(
681 | |             title="Test",
682 | |             value=100,
683 | |             trend=-3.1,
684 | |         )
685 | |         result = self.renderer._render_metric_card(negative_metric, {}, "test-2")  # noqa: SLF001
686 | |         assert "â†˜ï¸ -3.1" in result
687 | |         assert "text-red-600" in result
688 | |
689 | |         # No trend
690 | |         no_trend_metric = MetricCardData(
691 | |             title="Test",
692 | |             value=100,
693 | |             trend=0,
694 | |         )
695 | |         result = self.renderer._render_metric_card(no_trend_metric, {}, "test-3")  # noqa: SLF001
696 | |         assert "â¡ï¸ 0" in result
697 | |         assert "text-gray-600" in result
698 | |
699 | |         # No trend data
700 | |         no_trend_data_metric = MetricCardData(
701 | |             title="Test",
702 | |             value=100,
703 | |             trend=None,
704 | |         )
705 | |         result = self.renderer._render_metric_card(no_trend_data_metric, {}, "test-4")  # noqa: SLF001
706 | |         assert "â†—ï¸" not in result
707 | |         assert "â†˜ï¸" not in result
708 | |         assert "â¡ï¸" not in result
709 | |
710 | |     def test_javascript_data_validity(self) -> None:
711 | |         """Test that embedded JavaScript data is valid JSON."""
712 | |         session = SessionData(
713 | |             name="test-session",
714 | |             id="session-123",
715 | |             status=SessionStatus.ACTIVE,
716 | |             created_at=datetime.now(UTC),
717 | |             windows=[],
718 | |         )
719 | |
720 | |         result = self.renderer.render_widget(
721 | |             WidgetType.SESSION_BROWSER,
722 | |             [session],
723 | |             {"view_mode": "table"},
724 | |         )
725 | |
726 | |         # Check that JavaScript data is embedded properly
727 | |         assert "window.widgetData" in result
728 | |         assert '"type": "session_browser"' in result
729 | |         assert '"view_mode": "table"' in result
730 | |
731 | |         # Extract and validate JSON structure
732 | |
733 | |         # Find the JSON data using regex
734 | |         json_match = re.search(r"window\.widgetData\[\'[^\']+\'\] = ({.*?});", result, re.DOTALL)
735 | |
736 | |         if json_match:
737 | |             json_str = json_match.group(1)
738 | |             try:
739 | |                 parsed_data = json.loads(json_str)
740 | |                 assert isinstance(parsed_data, dict)
741 | |                 assert "type" in parsed_data
742 | |                 assert "sessions" in parsed_data
743 | |             except json.JSONDecodeError:
744 | |                 pytest.fail("Generated JavaScript data is not valid JSON")
745 | |         else:
746 | |             pytest.fail("Could not find JavaScript data in output")
    | |___________________________________________________________________^ PLR0904
    |

Found 716 errors.
make: *** [Makefile.lint.mk:73: lint-fix] Error 1
